{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Assignment 7: Energy Efficiency Dataset üè¢‚ö°\n",
                "\n",
                "## üìã Objectives\n",
                "- Perform comprehensive **EDA** on building energy data\n",
                "- Build **custom transformers** and preprocessing pipelines\n",
                "- Train and compare **regression models** (Linear Regression, Random Forest, Gradient Boosting)\n",
                "- Predict **Heating Load** from building features"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part A: Data Loading & Exploratory Analysis"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Q1 (5 marks)\n",
                "Download the dataset, read the file and show the first 5 rows."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd  # Data manipulation / –†–∞–±–æ—Ç–∞ —Å –¥–∞–Ω–Ω—ã–º–∏\n",
                "import numpy as np  # Numerical operations / –ß–∏—Å–ª–æ–≤—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏\n",
                "import matplotlib.pyplot as plt  # Plotting / –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è\n",
                "import seaborn as sns  # Statistical plots / –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –≥—Ä–∞—Ñ–∏–∫–∏\n",
                "\n",
                "# Configure plot style / –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å—Ç–∏–ª—è –≥—Ä–∞—Ñ–∏–∫–æ–≤\n",
                "sns.set_style('whitegrid')\n",
                "plt.rcParams['figure.figsize'] = (10, 6)\n",
                "\n",
                "# Read the Excel file / –ß—Ç–µ–Ω–∏–µ Excel —Ñ–∞–π–ª–∞\n",
                "df = pd.read_excel('ENB2012_data.xlsx')\n",
                "\n",
                "# Rename columns to meaningful names / –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–æ–∫ –Ω–∞ –ø–æ–Ω—è—Ç–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è\n",
                "columns = ['Relative_Compactness', 'Surface_Area', 'Wall_Area', 'Roof_Area',\n",
                "           'Overall_Height', 'Orientation', 'Glazing_Area', 'Glazing_Area_Distribution',\n",
                "           'Heating_Load', 'Cooling_Load']\n",
                "df.columns = columns\n",
                "\n",
                "# Display first 5 rows / –ü–æ–∫–∞–∑–∞—Ç—å –ø–µ—Ä–≤—ã–µ 5 —Å—Ç—Ä–æ–∫\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Q2 (5 marks)\n",
                "Provide a brief summary: number of rows, missing values, and basic statistics."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Dataset information / –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–∞—Ç–∞—Å–µ—Ç–µ\n",
                "print(\"=== Dataset Info ===\")\n",
                "print(f\"Number of rows: {len(df)}\")\n",
                "print(f\"Number of columns: {len(df.columns)}\")\n",
                "print(f\"\\nData types:\\n{df.dtypes}\")\n",
                "print(f\"\\n=== Missing Values ===\")\n",
                "print(df.isnull().sum())\n",
                "\n",
                "# Basic statistics / –û—Å–Ω–æ–≤–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\n",
                "print(\"\\n=== Basic Statistics ===\")\n",
                "df.describe()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Q3 (10 marks)\n",
                "Display the distribution of all features. Explain any notable patterns."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot distributions for all features / –ü–æ—Å—Ç—Ä–æ–∏—Ç—å —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–ª—è –≤—Å–µ—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
                "fig, axes = plt.subplots(4, 3, figsize=(15, 12))\n",
                "axes = axes.ravel()\n",
                "\n",
                "for idx, col in enumerate(df.columns):\n",
                "    axes[idx].hist(df[col].dropna(), bins=30, edgecolor='black', alpha=0.7)\n",
                "    axes[idx].set_title(f'Distribution of {col}')\n",
                "    axes[idx].set_xlabel(col)\n",
                "    axes[idx].set_ylabel('Frequency')\n",
                "\n",
                "# Hide extra subplots / –°–∫—Ä—ã—Ç—å –ª–∏—à–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–∏\n",
                "for idx in range(len(df.columns), len(axes)):\n",
                "    axes[idx].set_visible(False)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Notable Patterns / –ù–∞–±–ª—é–¥–∞–µ–º—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã:**\n",
                "- **Orientation** and **Glazing_Area_Distribution** show discrete values (categorical-like)\n",
                "- **Overall_Height** has only 2 unique values (3.5m and 7m)\n",
                "- **Surface_Area**, **Wall_Area**, **Roof_Area** show multi-modal distributions\n",
                "- Target variables (**Heating_Load**, **Cooling_Load**) are continuous but show some clustering"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Q4 (5 marks)\n",
                "Calculate value counts and distinct values for each feature."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculate number of unique values per feature / –ü–æ–¥—Å—á–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π\n",
                "print(\"=== Number of Unique Values per Feature ===\")\n",
                "for col in df.columns:\n",
                "    n_unique = df[col].nunique()\n",
                "    print(f\"{col:30s}: {n_unique:3d} unique values\")\n",
                "    \n",
                "print(\"\\n=== Value Counts for Categorical-like Features ===\")\n",
                "# Show value counts for features with fewer unique values\n",
                "categorical_like = ['Orientation', 'Glazing_Area_Distribution', 'Overall_Height']\n",
                "for col in categorical_like:\n",
                "    print(f\"\\n{col}:\")\n",
                "    print(df[col].value_counts().sort_index())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Q4.1 (5 marks)\n",
                "Calculate the total number of duplicated rows."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check for duplicate rows / –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤\n",
                "n_duplicates = df.duplicated().sum()\n",
                "print(f\"Total number of duplicate rows: {n_duplicates}\")\n",
                "\n",
                "if n_duplicates > 0:\n",
                "    print(f\"\\nPercentage of duplicates: {n_duplicates / len(df) * 100:.2f}%\")\n",
                "    print(\"\\nNote: Duplicates are expected since buildings were simulated with identical parameters.\")\n",
                "else:\n",
                "    print(\"No duplicate rows found.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Q5 (10 marks)\n",
                "Calculate correlation coefficients with the target and display in descending order."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculate correlations with Heating_Load target / –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ —Å —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π\n",
                "correlations = df.corr()['Heating_Load'].drop('Heating_Load').sort_values(ascending=False)\n",
                "\n",
                "print(\"=== Correlation with Heating_Load (sorted) ===\")\n",
                "print(correlations)\n",
                "\n",
                "# Visualize correlations / –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π\n",
                "plt.figure(figsize=(10, 6))\n",
                "correlations.plot(kind='barh', color='steelblue')\n",
                "plt.title('Feature Correlation with Heating Load')\n",
                "plt.xlabel('Correlation Coefficient')\n",
                "plt.axvline(x=0, color='black', linestyle='--', linewidth=0.8)\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"\\n‚ö†Ô∏è CAUTION: Cooling_Load has very high correlation (0.976).\")\n",
                "print(\"This is DATA LEAKAGE because both are outputs of the same simulation.\")\n",
                "print(\"We must remove Cooling_Load when predicting Heating_Load.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Q6 (10 marks)\n",
                "Visualize scatter matrix for features with weaker correlations."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Select features with weaker correlations (abs < 0.5) / –í—ã–±—Ä–∞—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å–æ —Å–ª–∞–±–æ–π –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–µ–π\n",
                "weak_corr_features = correlations[abs(correlations) < 0.5].index.tolist()\n",
                "features_to_plot = weak_corr_features + ['Heating_Load']\n",
                "\n",
                "print(f\"Features with weak correlation (|r| < 0.5): {weak_corr_features}\")\n",
                "\n",
                "# Create scatter matrix / –°–æ–∑–¥–∞—Ç—å –º–∞—Ç—Ä–∏—Ü—É –¥–∏–∞–≥—Ä–∞–º–º —Ä–∞—Å—Å–µ—è–Ω–∏—è\n",
                "from pandas.plotting import scatter_matrix\n",
                "\n",
                "scatter_matrix(df[features_to_plot], figsize=(12, 10), alpha=0.6, diagonal='hist')\n",
                "plt.suptitle('Scatter Matrix: Features with Weak Correlation to Heating Load', y=1.0)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part B: Preprocessing & Custom Transformers"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Q7 (15 marks)\n",
                "Implement custom transformer components for feature engineering."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.base import BaseEstimator, TransformerMixin  # Base classes for custom transformers\n",
                "\n",
                "# Custom transformer to calculate ratios / –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–π\n",
                "class RatioTransformer(BaseEstimator, TransformerMixin):\n",
                "    \"\"\"Creates ratio features from specified column pairs.\"\"\"\n",
                "    \n",
                "    def __init__(self, ratio_pairs):\n",
                "        \"\"\"\n",
                "        ratio_pairs: list of tuples (numerator_col, denominator_col, new_col_name)\n",
                "        \"\"\"\n",
                "        self.ratio_pairs = ratio_pairs\n",
                "    \n",
                "    def fit(self, X, y=None):\n",
                "        return self  # No fitting needed / –û–±—É—á–µ–Ω–∏–µ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è\n",
                "    \n",
                "    def transform(self, X):\n",
                "        X_copy = X.copy()  # Avoid modifying original / –ò–∑–±–µ–∂–∞—Ç—å –∏–∑–º–µ–Ω–µ–Ω–∏—è –æ—Ä–∏–≥–∏–Ω–∞–ª–∞\n",
                "        \n",
                "        for num_col, denom_col, new_col in self.ratio_pairs:\n",
                "            # Calculate ratio with small epsilon to avoid division by zero\n",
                "            X_copy[new_col] = X_copy[num_col] / (X_copy[denom_col] + 1e-10)\n",
                "        \n",
                "        return X_copy\n",
                "\n",
                "# Test the transformer / –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞\n",
                "ratio_transformer = RatioTransformer(\n",
                "    ratio_pairs=[\n",
                "        ('Wall_Area', 'Surface_Area', 'Wall_to_Surface_Ratio')\n",
                "    ]\n",
                ")\n",
                "\n",
                "# Apply to check if it works / –ü—Ä–∏–º–µ–Ω–∏—Ç—å –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏\n",
                "test_df = ratio_transformer.fit_transform(df.head())\n",
                "print(\"New feature created:\")\n",
                "print(test_df[['Wall_Area', 'Surface_Area', 'Wall_to_Surface_Ratio']].head())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part C: Build Preprocessing Pipeline"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Q8 (15 marks)\n",
                "Construct a ColumnTransformer pipeline."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.pipeline import Pipeline  # Pipeline construction / –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –ø–∞–π–ø–ª–∞–π–Ω–∞\n",
                "from sklearn.compose import ColumnTransformer  # Column-wise transformations / –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø–æ –∫–æ–ª–æ–Ω–∫–∞–º\n",
                "from sklearn.preprocessing import StandardScaler, OneHotEncoder  # Scalers and encoders\n",
                "\n",
                "# Define feature groups / –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –≥—Ä—É–ø–ø—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
                "categorical_features = ['Orientation', 'Glazing_Area_Distribution']\n",
                "numerical_features = ['Relative_Compactness', 'Surface_Area', 'Wall_Area', 'Roof_Area',\n",
                "                      'Overall_Height', 'Glazing_Area']\n",
                "\n",
                "# Create preprocessing pipeline / –°–æ–∑–¥–∞—Ç—å –ø–∞–π–ø–ª–∞–π–Ω –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏\n",
                "preprocessor = ColumnTransformer(\n",
                "    transformers=[\n",
                "        # Encode categorical features / –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
                "        ('cat', OneHotEncoder(drop='first', sparse_output=False), categorical_features),\n",
                "        \n",
                "        # Scale numerical features / –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
                "        ('num', StandardScaler(), numerical_features)\n",
                "    ],\n",
                "    remainder='drop'  # Drop any features not specified / –£–¥–∞–ª–∏—Ç—å –Ω–µ—É–∫–∞–∑–∞–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏\n",
                ")\n",
                "\n",
                "print(\"Preprocessing pipeline created successfully!\")\n",
                "print(f\"Categorical features: {categorical_features}\")\n",
                "print(f\"Numerical features: {numerical_features}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part D: Modeling"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Q9 (5 marks)\n",
                "Split the dataset into train/test (80/20). Drop Cooling_Load to avoid data leakage."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.model_selection import train_test_split  # Train/test split\n",
                "\n",
                "# Separate features and target / –†–∞–∑–¥–µ–ª–∏—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏ —Ü–µ–ª—å\n",
                "X = df.drop(['Heating_Load', 'Cooling_Load'], axis=1)  # Remove both targets / –£–¥–∞–ª–∏—Ç—å –æ–±–µ —Ü–µ–ª–∏\n",
                "y = df['Heating_Load']  # Target variable / –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è\n",
                "\n",
                "# Split into train and test sets / –†–∞–∑–¥–µ–ª–∏—Ç—å –Ω–∞ –æ–±—É—á–∞—é—â—É—é –∏ —Ç–µ—Å—Ç–æ–≤—É—é –≤—ã–±–æ—Ä–∫–∏\n",
                "X_train, X_test, y_train, y_test = train_test_split(\n",
                "    X, y, test_size=0.2, random_state=42\n",
                ")\n",
                "\n",
                "print(f\"Training set size: {len(X_train)} samples\")\n",
                "print(f\"Test set size: {len(X_test)} samples\")\n",
                "print(f\"\\nFeatures shape: {X_train.shape}\")\n",
                "print(f\"Target shape: {y_train.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Q10 (5 marks)\n",
                "Fit the preprocessing pipeline and transform both sets."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Fit on training data only / –û–±—É—á–∏—Ç—å —Ç–æ–ª—å–∫–æ –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
                "X_train_processed = preprocessor.fit_transform(X_train)\n",
                "\n",
                "# Transform test data / –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ\n",
                "X_test_processed = preprocessor.transform(X_test)\n",
                "\n",
                "print(f\"Processed training shape: {X_train_processed.shape}\")\n",
                "print(f\"Processed test shape: {X_test_processed.shape}\")\n",
                "print(f\"\\nNumber of features after preprocessing: {X_train_processed.shape[1]}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Q11 (10 marks)\n",
                "Train Linear Regression and evaluate on test set."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.linear_model import LinearRegression  # Linear regression model\n",
                "from sklearn.metrics import mean_squared_error, r2_score  # Evaluation metrics\n",
                "\n",
                "# Initialize and train Linear Regression / –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∏ –æ–±—É—á–∏—Ç—å –ª–∏–Ω–µ–π–Ω—É—é —Ä–µ–≥—Ä–µ—Å—Å–∏—é\n",
                "lr_model = LinearRegression()\n",
                "lr_model.fit(X_train_processed, y_train)\n",
                "\n",
                "# Make predictions / –°–¥–µ–ª–∞—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è\n",
                "y_pred_lr = lr_model.predict(X_test_processed)\n",
                "\n",
                "# Calculate metrics / –í—ã—á–∏—Å–ª–∏—Ç—å –º–µ—Ç—Ä–∏–∫–∏\n",
                "rmse_lr = np.sqrt(mean_squared_error(y_test, y_pred_lr))\n",
                "r2_lr = r2_score(y_test, y_pred_lr)\n",
                "\n",
                "print(\"=== Linear Regression Results ===\")\n",
                "print(f\"RMSE: {rmse_lr:.4f}\")\n",
                "print(f\"R¬≤ Score: {r2_lr:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Q12 (10 marks)\n",
                "Train Random Forest Regressor with n_estimators=200."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.ensemble import RandomForestRegressor  # Random Forest regressor\n",
                "\n",
                "# Initialize and train Random Forest / –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∏ –æ–±—É—á–∏—Ç—å Random Forest\n",
                "rf_model = RandomForestRegressor(n_estimators=200, random_state=42, n_jobs=-1)\n",
                "rf_model.fit(X_train_processed, y_train)\n",
                "\n",
                "# Make predictions / –°–¥–µ–ª–∞—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è\n",
                "y_pred_rf = rf_model.predict(X_test_processed)\n",
                "\n",
                "# Calculate metrics / –í—ã—á–∏—Å–ª–∏—Ç—å –º–µ—Ç—Ä–∏–∫–∏\n",
                "rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))\n",
                "r2_rf = r2_score(y_test, y_pred_rf)\n",
                "\n",
                "print(\"=== Random Forest Results ===\")\n",
                "print(f\"RMSE: {rmse_rf:.4f}\")\n",
                "print(f\"R¬≤ Score: {r2_rf:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Q13 (10 marks)\n",
                "Train Gradient Boosting Regressor with n_estimators=200."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.ensemble import GradientBoostingRegressor  # Gradient Boosting regressor\n",
                "\n",
                "# Initialize and train Gradient Boosting / –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∏ –æ–±—É—á–∏—Ç—å Gradient Boosting\n",
                "gb_model = GradientBoostingRegressor(n_estimators=200, random_state=42)\n",
                "gb_model.fit(X_train_processed, y_train)\n",
                "\n",
                "# Make predictions / –°–¥–µ–ª–∞—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è\n",
                "y_pred_gb = gb_model.predict(X_test_processed)\n",
                "\n",
                "# Calculate metrics / –í—ã—á–∏—Å–ª–∏—Ç—å –º–µ—Ç—Ä–∏–∫–∏\n",
                "rmse_gb = np.sqrt(mean_squared_error(y_test, y_pred_gb))\n",
                "r2_gb = r2_score(y_test, y_pred_gb)\n",
                "\n",
                "print(\"=== Gradient Boosting Results ===\")\n",
                "print(f\"RMSE: {rmse_gb:.4f}\")\n",
                "print(f\"R¬≤ Score: {r2_gb:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Q14 (10 marks)\n",
                "Compare all models and explain R¬≤ metric."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create comparison summary / –°–æ–∑–¥–∞—Ç—å —Å–≤–æ–¥–Ω—É—é —Ç–∞–±–ª–∏—Ü—É —Å—Ä–∞–≤–Ω–µ–Ω–∏—è\n",
                "results = pd.DataFrame({\n",
                "    'Model': ['Linear Regression', 'Random Forest', 'Gradient Boosting'],\n",
                "    'RMSE': [rmse_lr, rmse_rf, rmse_gb],\n",
                "    'R¬≤ Score': [r2_lr, r2_rf, r2_gb]\n",
                "})\n",
                "\n",
                "results = results.sort_values('R¬≤ Score', ascending=False).reset_index(drop=True)\n",
                "print(\"=== Model Comparison ===\")\n",
                "print(results.to_string(index=False))\n",
                "\n",
                "# Visualize comparison / –í–∏–∑—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# RMSE comparison / –°—Ä–∞–≤–Ω–µ–Ω–∏–µ RMSE\n",
                "axes[0].bar(results['Model'], results['RMSE'], color=['steelblue', 'forestgreen', 'coral'])\n",
                "axes[0].set_ylabel('RMSE (lower is better)')\n",
                "axes[0].set_title('Model Comparison: RMSE')\n",
                "axes[0].tick_params(axis='x', rotation=15)\n",
                "\n",
                "# R¬≤ comparison / –°—Ä–∞–≤–Ω–µ–Ω–∏–µ R¬≤\n",
                "axes[1].bar(results['Model'], results['R¬≤ Score'], color=['steelblue', 'forestgreen', 'coral'])\n",
                "axes[1].set_ylabel('R¬≤ Score (higher is better)')\n",
                "axes[1].set_title('Model Comparison: R¬≤ Score')\n",
                "axes[1].tick_params(axis='x', rotation=15)\n",
                "axes[1].axhline(y=0.9, color='red', linestyle='--', label='Excellent (0.9)')\n",
                "axes[1].legend()\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Detailed Feedback & R¬≤ Explanation\n",
                "\n",
                "### What is R¬≤ (Coefficient of Determination)?\n",
                "\n",
                "**R¬≤** measures how well your model explains the variance in the target variable. It ranges from 0 to 1 (can be negative for very bad models).\n",
                "\n",
                "**Formula:**\n",
                "$$R^2 = 1 - \\frac{SS_{res}}{SS_{tot}} = 1 - \\frac{\\sum(y_i - \\hat{y}_i)^2}{\\sum(y_i - \\bar{y})^2}$$\n",
                "\n",
                "Where:\n",
                "- $SS_{res}$ = Sum of squared residuals (model errors)\n",
                "- $SS_{tot}$ = Total sum of squares (variance in data)\n",
                "- $\\bar{y}$ = Mean of actual values\n",
                "\n",
                "**Interpretation:**\n",
                "- **R¬≤ = 1.0**: Perfect predictions (model explains 100% of variance)\n",
                "- **R¬≤ = 0.9**: Model explains 90% of variance (excellent)\n",
                "- **R¬≤ = 0.5**: Model explains 50% of variance (moderate)\n",
                "- **R¬≤ = 0.0**: Model is no better than predicting the mean\n",
                "- **R¬≤ < 0.0**: Model is worse than predicting the mean\n",
                "\n",
                "### Model Analysis:\n",
                "\n",
                "Based on typical results for this dataset:\n",
                "\n",
                "1. **Gradient Boosting** usually achieves the best performance (R¬≤ ‚âà 0.98-0.99)\n",
                "   - Sequentially builds trees that correct previous errors\n",
                "   - Excellent at capturing non-linear relationships\n",
                "   \n",
                "2. **Random Forest** performs very well (R¬≤ ‚âà 0.97-0.98)\n",
                "   - Ensemble of trees reduces overfitting\n",
                "   - Robust to outliers and noise\n",
                "   \n",
                "3. **Linear Regression** performs decently (R¬≤ ‚âà 0.88-0.92)\n",
                "   - Assumes linear relationships\n",
                "   - Limited by its inability to capture complex interactions\n",
                "\n",
                "### Conclusion:\n",
                "Tree-based models (Random Forest & Gradient Boosting) outperform Linear Regression because the relationship between building features and heating load is **non-linear** and involves **complex interactions** between variables."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}