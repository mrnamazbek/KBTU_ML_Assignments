# Defense Guide: Assignment 7 - Energy Efficiency Dataset

## üìñ Table of Contents
1. [Dataset Overview](#dataset-overview)
2. [Line-by-Line Code Explanations](#line-by-line-explanations)
3. [Model Deep Dive](#model-deep-dive)
4. [Common Questions & Answers](#qa-section)

---

## Dataset Overview

### üá¨üáß English
The **Energy Efficiency Dataset** contains 768 simulated buildings with 8 input features and 2 output targets (Heating Load and Cooling Load). The buildings were simulated using Ecotect software with varying parameters like glazing area, orientation, and height.

**Why this dataset?**  
It demonstrates real-world building energy optimization, where architects need to predict energy requirements before construction.

### üá∑üá∫ –†—É—Å—Å–∫–∏–π
**–î–∞—Ç–∞—Å–µ—Ç Energy Efficiency** —Å–æ–¥–µ—Ä–∂–∏—Ç 768 —Å–∏–º—É–ª—è—Ü–∏–π –∑–¥–∞–Ω–∏–π —Å 8 –≤—Ö–æ–¥–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –∏ 2 —Ü–µ–ª–µ–≤—ã–º–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–º–∏ (Heating Load –∏ Cooling Load). –ó–¥–∞–Ω–∏—è –±—ã–ª–∏ —Å–º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω—ã –≤ –ø—Ä–æ–≥—Ä–∞–º–º–µ Ecotect —Å —Ä–∞–∑–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏: –ø–ª–æ—â–∞–¥—å –æ—Å—Ç–µ–∫–ª–µ–Ω–∏—è, –æ—Ä–∏–µ–Ω—Ç–∞—Ü–∏—è, –≤—ã—Å–æ—Ç–∞.

**–ó–∞—á–µ–º —ç—Ç–æ—Ç –¥–∞—Ç–∞—Å–µ—Ç?**  
–û–Ω –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —Ä–µ–∞–ª—å–Ω—É—é –∑–∞–¥–∞—á—É –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —ç–Ω–µ—Ä–≥–æ–ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏—è –∑–¥–∞–Ω–∏–π, –≥–¥–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç–æ—Ä–∞–º –Ω—É–∂–Ω–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å —Ä–∞—Å—Ö–æ–¥ —ç–Ω–µ—Ä–≥–∏–∏ –¥–æ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–∞.

---

## Line-by-Line Explanations

### Q1: Data Loading / –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö

#### üá¨üáß English
```python
import pandas as pd  # Data manipulation library
import numpy as np   # Numerical operations
import matplotlib.pyplot as plt  # Plotting library
import seaborn as sns  # Statistical visualization
```

**Line-by-line:**
1. **`pandas`**: The core library for working with tabular data (think Excel-like tables in Python).
2. **`numpy`**: Handles numerical arrays and mathematical operations efficiently.
3. **`matplotlib`**: Low-level plotting library. Think of it as the canvas.
4. **`seaborn`**: High-level plotting built on matplotlib. Provides beautiful statistical plots.

```python
df = pd.read_excel('ENB2012_data.xlsx')
```
- **`read_excel`**: Reads an Excel file (.xlsx) into a DataFrame.
- **DataFrame**: A table with rows (samples) and columns (features).

```python
columns = ['Relative_Compactness', 'Surface_Area', ...]
df.columns = columns
```
- **Purpose**: The Excel file has no headers, so we manually assign meaningful column names.
- **Why?**: Working with `df['Surface_Area']` is clearer than `df[1]`.

#### üá∑üá∫ –†—É—Å—Å–∫–∏–π
```python
import pandas as pd  # –ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —Ç–∞–±–ª–∏—Ü–∞–º–∏
import numpy as np   # –ß–∏—Å–ª–æ–≤—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏
import matplotlib.pyplot as plt  # –ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–æ–≤
import seaborn as sns  # –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –≥—Ä–∞—Ñ–∏–∫–∏
```

**–ü–æ—Å—Ç—Ä–æ—á–Ω–æ:**
1. **`pandas`**: –û—Å–Ω–æ–≤–Ω–∞—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —Ç–∞–±–ª–∏—Ü–∞–º–∏ (–∫–∞–∫ Excel –≤ Python).
2. **`numpy`**: –†–∞–±–æ—Ç–∞–µ—Ç —Å —á–∏—Å–ª–æ–≤—ã–º–∏ –º–∞—Å—Å–∏–≤–∞–º–∏ –∏ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–º–∏ –æ–ø–µ—Ä–∞—Ü–∏—è–º–∏.
3. **`matplotlib`**: –ë–∞–∑–æ–≤–∞—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–æ–≤. –≠—Ç–æ –∫–∞–∫ —Ö–æ–ª—Å—Ç.
4. **`seaborn`**: –í—ã—Å–æ–∫–æ—É—Ä–æ–≤–Ω–µ–≤–∞—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ –¥–ª—è –∫—Ä–∞—Å–∏–≤—ã—Ö —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö –≥—Ä–∞—Ñ–∏–∫–æ–≤.

```python
df = pd.read_excel('ENB2012_data.xlsx')
```
- **`read_excel`**: –ß–∏—Ç–∞–µ—Ç Excel —Ñ–∞–π–ª –≤ DataFrame.
- **DataFrame**: –¢–∞–±–ª–∏—Ü–∞ —Å–æ —Å—Ç—Ä–æ–∫–∞–º–∏ (–æ–±—Ä–∞–∑—Ü—ã) –∏ —Å—Ç–æ–ª–±—Ü–∞–º–∏ (–ø—Ä–∏–∑–Ω–∞–∫–∏).

```python
columns = ['Relative_Compactness', 'Surface_Area', ...]
df.columns = columns
```
- **–¶–µ–ª—å**: Excel —Ñ–∞–π–ª –±–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤, –ø–æ—ç—Ç–æ–º—É –º—ã –≤—Ä—É—á–Ω—É—é –ø—Ä–∏—Å–≤–∞–∏–≤–∞–µ–º –∏–º–µ–Ω–∞ —Å—Ç–æ–ª–±—Ü–∞–º.
- **–ü–æ—á–µ–º—É?**: –†–∞–±–æ—Ç–∞—Ç—å —Å `df['Surface_Area']` –ø–æ–Ω—è—Ç–Ω–µ–µ, —á–µ–º —Å `df[1]`.

---

### Q2: Dataset Summary / –°–≤–æ–¥–∫–∞ –ø–æ –¥–∞–Ω–Ω—ã–º

#### üá¨üáß English
```python
print(f\"Number of rows: {len(df)}\")
```
- **`len(df)`**: Returns the number of rows in the DataFrame.
- **f-string**: Python's formatted string literal (f\"text {variable}\").

```python
df.isnull().sum()
```
- **`df.isnull()`**: Returns a boolean DataFrame (True where value is missing).
- **`.sum()`**: Counts True values per column (number of missing values).

```python
df.describe()
```
- **Purpose**: Generates summary statistics (count, mean, std, min, 25%, 50%, 75%, max).
- **Use case**: Quickly spot outliers or unusual distributions.

#### üá∑üá∫ –†—É—Å—Å–∫–∏–π
```python
print(f\"Number of rows: {len(df)}\")
```
- **`len(df)`**: –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ –≤ —Ç–∞–±–ª–∏—Ü–µ.
- **f-string**: –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å—Ç—Ä–æ–∫–∞ Python (f\"—Ç–µ–∫—Å—Ç {–ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è}\").

```python
df.isnull().sum()
```
- **`df.isnull()`**: –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–∞–±–ª–∏—Ü—É —Å True/False (True = –ø—Ä–æ–ø—É—â–µ–Ω–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ).
- **`.sum()`**: –°—á–∏—Ç–∞–µ—Ç True –ø–æ –∫–∞–∂–¥–æ–º—É —Å—Ç–æ–ª–±—Ü—É (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ–ø—É—Å–∫–æ–≤).

```python
df.describe()
```
- **–¶–µ–ª—å**: –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ, —Å—Ä–µ–¥–Ω–µ–µ, —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ, –º–∏–Ω, –º–∞–∫—Å, –∫–≤–∞—Ä—Ç–∏–ª–∏).
- **–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ**: –ë—ã—Å—Ç—Ä–æ –Ω–∞–π—Ç–∏ –≤—ã–±—Ä–æ—Å—ã –∏–ª–∏ –Ω–µ–æ–±—ã—á–Ω—ã–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è.

---

### Q3: Feature Distributions / –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤

#### üá¨üáß English
```python
fig, axes = plt.subplots(4, 3, figsize=(15, 12))
axes = axes.ravel()
```
- **`plt.subplots(4, 3)`**: Creates a 4x3 grid of subplots (12 total).
- **`figsize=(15, 12)`**: Sets figure size in inches (width, height).
- **`axes.ravel()`**: Flattens 2D array of axes into 1D for easy iteration.

```python
for idx, col in enumerate(df.columns):
    axes[idx].hist(df[col].dropna(), bins=30, edgecolor='black', alpha=0.7)
```
- **`enumerate(df.columns)`**: Loops with index and column name.
- **`df[col].dropna()`**: Removes missing values before plotting.
- **`bins=30`**: Divides data into 30 equal-width ranges.
- **`edgecolor='black'`**: Adds black borders to bars.
- **`alpha=0.7`**: Sets transparency (0=invisible, 1=opaque).

#### üá∑üá∫ –†—É—Å—Å–∫–∏–π
```python
fig, axes = plt.subplots(4, 3, figsize=(15, 12))
axes = axes.ravel()
```
- **`plt.subplots(4, 3)`**: –°–æ–∑–¥–∞–µ—Ç —Å–µ—Ç–∫—É 4x3 –≥—Ä–∞—Ñ–∏–∫–æ–≤ (–≤—Å–µ–≥–æ 12).
- **`figsize=(15, 12)`**: –†–∞–∑–º–µ—Ä —Ñ–∏–≥—É—Ä—ã –≤ –¥—é–π–º–∞—Ö (—à–∏—Ä–∏–Ω–∞, –≤—ã—Å–æ—Ç–∞).
- **`axes.ravel()`**: –ü—Ä–µ–≤—Ä–∞—â–∞–µ—Ç 2D –º–∞—Å—Å–∏–≤ –≥—Ä–∞—Ñ–∏–∫–æ–≤ –≤ 1D –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞.

```python
for idx, col in enumerate(df.columns):
    axes[idx].hist(df[col].dropna(), bins=30, edgecolor='black', alpha=0.7)
```
- **`enumerate(df.columns)`**: –¶–∏–∫–ª —Å –∏–Ω–¥–µ–∫—Å–æ–º –∏ –Ω–∞–∑–≤–∞–Ω–∏–µ–º —Å—Ç–æ–ª–±—Ü–∞.
- **`df[col].dropna()`**: –£–¥–∞–ª—è–µ—Ç –ø—Ä–æ–ø—É—Å–∫–∏ –ø–µ—Ä–µ–¥ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ–º –≥—Ä–∞—Ñ–∏–∫–∞.
- **`bins=30`**: –î–µ–ª–∏—Ç –¥–∞–Ω–Ω—ã–µ –Ω–∞ 30 –¥–∏–∞–ø–∞–∑–æ–Ω–æ–≤ –æ–¥–∏–Ω–∞–∫–æ–≤–æ–π —à–∏—Ä–∏–Ω—ã.
- **`edgecolor='black'`**: –î–æ–±–∞–≤–ª—è–µ—Ç —á–µ—Ä–Ω—ã–µ –≥—Ä–∞–Ω–∏—Ü—ã —Å—Ç–æ–ª–±—Ü–∞–º.
- **`alpha=0.7`**: –ü—Ä–æ–∑—Ä–∞—á–Ω–æ—Å—Ç—å (0=–Ω–µ–≤–∏–¥–∏–º—ã–π, 1=–Ω–µ–ø—Ä–æ–∑—Ä–∞—á–Ω—ã–π).

---

### Q5: Correlation Analysis / –ê–Ω–∞–ª–∏–∑ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π

#### üá¨üáß English
```python
correlations = df.corr()['Heating_Load'].drop('Heating_Load').sort_values(ascending=False)
```
**Breaking it down:**
1. **`df.corr()`**: Computes pairwise correlations between ALL columns (Pearson correlation).
2. **`['Heating_Load']`**: Selects only correlations with Heating_Load.
3. **`.drop('Heating_Load')`**: Removes self-correlation (always 1.0).
4. **`.sort_values(ascending=False)`**: Sorts from highest to lowest correlation.

**What is correlation?**  
A measure of linear relationship between two variables (-1 to +1):
- **+1**: Perfect positive (when X increases, Y increases).
- **0**: No linear relationship.
- **-1**: Perfect negative (when X increases, Y decreases).

```python
correlations.plot(kind='barh', color='steelblue')
```
- **`kind='barh'`**: Horizontal bar chart.
- **Why horizontal?**: Easier to read long feature names.

#### üá∑üá∫ –†—É—Å—Å–∫–∏–π
```python
correlations = df.corr()['Heating_Load'].drop('Heating_Load').sort_values(ascending=False)
```
**–†–∞–∑–±–æ—Ä:**
1. **`df.corr()`**: –í—ã—á–∏—Å–ª—è–µ—Ç –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –º–µ–∂–¥—É –í–°–ï–ú–ò —Å—Ç–æ–ª–±—Ü–∞–º–∏ (–∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è –ü–∏—Ä—Å–æ–Ω–∞).
2. **`['Heating_Load']`**: –í—ã–±–∏—Ä–∞–µ—Ç —Ç–æ–ª—å–∫–æ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ —Å Heating_Load.
3. **`.drop('Heating_Load')`**: –£–¥–∞–ª—è–µ—Ç –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—é —Å —Å–∞–º–∏–º —Å–æ–±–æ–π (–≤—Å–µ–≥–¥–∞ 1.0).
4. **`.sort_values(ascending=False)`**: –°–æ—Ä—Ç–∏—Ä—É–µ—Ç –æ—Ç –±–æ–ª—å—à–µ–≥–æ –∫ –º–µ–Ω—å—à–µ–º—É.

**–ß—Ç–æ —Ç–∞–∫–æ–µ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è?**  
–ú–µ—Ä–∞ –ª–∏–Ω–µ–π–Ω–æ–π —Å–≤—è–∑–∏ –º–µ–∂–¥—É –¥–≤—É–º—è –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–º–∏ (–æ—Ç -1 –¥–æ +1):
- **+1**: –ò–¥–µ–∞–ª—å–Ω–∞—è –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–∞—è (–∫–æ–≥–¥–∞ X —Ä–∞—Å—Ç–µ—Ç, Y —Ä–∞—Å—Ç–µ—Ç).
- **0**: –ù–µ—Ç –ª–∏–Ω–µ–π–Ω–æ–π —Å–≤—è–∑–∏.
- **-1**: –ò–¥–µ–∞–ª—å–Ω–∞—è –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–∞—è (–∫–æ–≥–¥–∞ X —Ä–∞—Å—Ç–µ—Ç, Y –ø–∞–¥–∞–µ—Ç).

```python
correlations.plot(kind='barh', color='steelblue')
```
- **`kind='barh'`**: –ì–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω–∞—è —Å—Ç–æ–ª–±—á–∞—Ç–∞—è –¥–∏–∞–≥—Ä–∞–º–º–∞.
- **–ü–æ—á–µ–º—É –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω–æ?**: –õ–µ–≥—á–µ —á–∏—Ç–∞—Ç—å –¥–ª–∏–Ω–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.

---

### Q7: Custom Transformer / –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä

#### üá¨üáß English
```python
from sklearn.base import BaseEstimator, TransformerMixin
```
- **`BaseEstimator`**: Provides `get_params()` and `set_params()` methods.
- **`TransformerMixin`**: Adds `fit_transform()` method automatically.

**Why custom transformers?**  
To create reusable, pipeline-compatible feature engineering steps.

```python
class RatioTransformer(BaseEstimator, TransformerMixin):
    def __init__(self, ratio_pairs):
        self.ratio_pairs = ratio_pairs
```
- **`__init__`**: Constructor method. Stores configuration (which ratios to create).
- **`self.ratio_pairs`**: List of tuples like `('Wall_Area', 'Surface_Area', 'New_Ratio')`.

```python
def fit(self, X, y=None):
    return self
```
- **Purpose**: Required by sklearn, even if no fitting is needed.
- **`return self`**: Allows method chaining (`transformer.fit(X).transform(X)`).

```python
def transform(self, X):
    X_copy = X.copy()
    for num_col, denom_col, new_col in self.ratio_pairs:
        X_copy[new_col] = X_copy[num_col] / (X_copy[denom_col] + 1e-10)
    return X_copy
```
- **`X_copy = X.copy()`**: Avoids modifying the original DataFrame.
- **`+ 1e-10`**: Prevents division by zero (adds tiny epsilon).
- **Purpose**: Creates ratio features (e.g., Wall/Surface ratio captures building shape).

#### üá∑üá∫ –†—É—Å—Å–∫–∏–π
```python
from sklearn.base import BaseEstimator, TransformerMixin
```
- **`BaseEstimator`**: –ü—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥—ã `get_params()` –∏ `set_params()`.
- **`TransformerMixin`**: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –¥–æ–±–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ `fit_transform()`.

**–ó–∞—á–µ–º –∫–∞—Å—Ç–æ–º–Ω—ã–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã?**  
–ß—Ç–æ–±—ã —Å–æ–∑–¥–∞—Ç—å –ø–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ —à–∞–≥–∏ feature engineering, —Å–æ–≤–º–µ—Å—Ç–∏–º—ã–µ —Å –ø–∞–π–ø–ª–∞–π–Ω–∞–º–∏.

```python
class RatioTransformer(BaseEstimator, TransformerMixin):
    def __init__(self, ratio_pairs):
        self.ratio_pairs = ratio_pairs
```
- **`__init__`**: –ö–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä. –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ (–∫–∞–∫–∏–µ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏—è —Å–æ–∑–¥–∞–≤–∞—Ç—å).
- **`self.ratio_pairs`**: –°–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π –≤–∏–¥–∞ `('Wall_Area', 'Surface_Area', 'NewRatio')`.

```python
def fit(self, X, y=None):
    return self
```
- **–¶–µ–ª—å**: –¢—Ä–µ–±—É–µ—Ç—Å—è sklearn, –¥–∞–∂–µ –µ—Å–ª–∏ –æ–±—É—á–µ–Ω–∏–µ –Ω–µ –Ω—É–∂–Ω–æ.
- **`return self`**: –ü–æ–∑–≤–æ–ª—è–µ—Ç —Ü–µ–ø–æ—á–∫—É –º–µ—Ç–æ–¥–æ–≤ (`transformer.fit(X).transform(X)`).

```python
def transform(self, X):
    X_copy = X.copy()
    for num_col, denom_col, new_col in self.ratio_pairs:
        X_copy[new_col] = X_copy[num_col] / (X_copy[denom_col] + 1e-10)
    return X_copy
```
- **`X_copy = X.copy()`**: –ò–∑–±–µ–≥–∞–µ—Ç –∏–∑–º–µ–Ω–µ–Ω–∏—è –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–π —Ç–∞–±–ª–∏—Ü—ã.
- **`+ 1e-10`**: –ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç –¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –Ω–æ–ª—å (–¥–æ–±–∞–≤–ª—è–µ—Ç –º–∞–ª–µ–Ω—å–∫–æ–µ —á–∏—Å–ª–æ).
- **–¶–µ–ª—å**: –°–æ–∑–¥–∞–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏-—Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏—è (–Ω–∞–ø—Ä–∏–º–µ—Ä, Wall/Surface –æ–ø–∏—Å—ã–≤–∞–µ—Ç —Ñ–æ—Ä–º—É –∑–¥–∞–Ω–∏—è).

---

### Q8: Preprocessing Pipeline / –ü–∞–π–ø–ª–∞–π–Ω –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏

#### üá¨üáß English
```python
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
```
- **`ColumnTransformer`**: Applies different transformations to different column groups.
- **`StandardScaler`**: Standardizes features (mean=0, std=1).
- **`OneHotEncoder`**: Converts categorical variables to binary vectors.

```python
preprocessor = ColumnTransformer(
    transformers=[
        ('cat', OneHotEncoder(drop='first', sparse_output=False), categorical_features),
        ('num', StandardScaler(), numerical_features)
    ],
    remainder='drop'
)
```
**Parameters explained:**
- **`transformers`**: List of (name, transformer, columns) tuples.
- **`drop='first'`**: Drops first category to avoid multicollinearity (dummy variable trap).
- **`sparse_output=False`**: Returns dense array instead of sparse matrix.
- **`remainder='drop'`**: Drops columns not specified in transformers.

#### üá∑üá∫ –†—É—Å—Å–∫–∏–π
```python
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
```
- **`ColumnTransformer`**: –ü—Ä–∏–º–µ–Ω—è–µ—Ç —Ä–∞–∑–Ω—ã–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∫ —Ä–∞–∑–Ω—ã–º –≥—Ä—É–ø–ø–∞–º —Å—Ç–æ–ª–±—Ü–æ–≤.
- **`StandardScaler`**: –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä—É–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏ (—Å—Ä–µ–¥–Ω–µ–µ=0, —Å—Ç–¥.–æ—Ç–∫–ª.=1).
- **`OneHotEncoder`**: –ü—Ä–µ–≤—Ä–∞—â–∞–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –≤ –±–∏–Ω–∞—Ä–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä—ã.

```python
preprocessor = ColumnTransformer(
    transformers=[
        ('cat', OneHotEncoder(drop='first', sparse_output=False), categorical_features),
        ('num', StandardScaler(), numerical_features)
    ],
    remainder='drop'
)
```
**–ü–∞—Ä–∞–º–µ—Ç—Ä—ã:**
- **`transformers`**: –°–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (–∏–º—è, —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä, —Å—Ç–æ–ª–±—Ü—ã).
- **`drop='first'`**: –£–¥–∞–ª—è–µ—Ç –ø–µ—Ä–≤—É—é –∫–∞—Ç–µ–≥–æ—Ä–∏—é, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –º—É–ª—å—Ç–∏–∫–æ–ª–ª–∏–Ω–µ–∞—Ä–Ω–æ—Å—Ç–∏.
- **`sparse_output=False`**: –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ–±—ã—á–Ω—ã–π –º–∞—Å—Å–∏–≤ –≤–º–µ—Å—Ç–æ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–π –º–∞—Ç—Ä–∏—Ü—ã.
- **`remainder='drop'`**: –£–¥–∞–ª—è–µ—Ç —Å—Ç–æ–ª–±—Ü—ã, –Ω–µ —É–∫–∞–∑–∞–Ω–Ω—ã–µ –≤ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞—Ö.

---

### Q9-Q13: Model Training / –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π

#### üá¨üáß English
```python
from sklearn.model_selection import train_test_split

X = df.drop(['Heating_Load', 'Cooling_Load'], axis=1)
y = df['Heating_Load']
```
- **`axis=1`**: Drop columns (axis=0 would drop rows).
- **Why drop Cooling_Load?**: It's data leakage (both are outputs of the same simulation).

```python
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)
```
- **`test_size=0.2`**: 20% for testing, 80% for training.
- **`random_state=42`**: Seed for reproducibility (same split every time).

```python
X_train_processed = preprocessor.fit_transform(X_train)
X_test_processed = preprocessor.transform(X_test)
```
**Critical concept:**
- **Fit on train only**: Learn scaling parameters from training data.
- **Transform both**: Apply learned parameters to both sets.
- **Why?**: Prevents test data leakage during preprocessing.

#### üá∑üá∫ –†—É—Å—Å–∫–∏–π
```python
from sklearn.model_selection import train_test_split

X = df.drop(['Heating_Load', 'Cooling_Load'], axis=1)
y = df['Heating_Load']
```
- **`axis=1`**: –£–¥–∞–ª–∏—Ç—å —Å—Ç–æ–ª–±—Ü—ã (axis=0 —É–¥–∞–ª–∏–ª –±—ã —Å—Ç—Ä–æ–∫–∏).
- **–ó–∞—á–µ–º —É–¥–∞–ª—è—Ç—å Cooling_Load?**: –≠—Ç–æ —É—Ç–µ—á–∫–∞ –¥–∞–Ω–Ω—ã—Ö (–æ–±–∞ - –≤—ã—Ö–æ–¥—ã –æ–¥–Ω–æ–π —Å–∏–º—É–ª—è—Ü–∏–∏).

```python
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)
```
- **`test_size=0.2`**: 20% –Ω–∞ —Ç–µ—Å—Ç, 80% –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ.
- **`random_state=42`**: –ó–µ—Ä–Ω–æ –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏ (–æ–¥–∏–Ω–∞–∫–æ–≤–æ–µ –¥–µ–ª–µ–Ω–∏–µ –∫–∞–∂–¥—ã–π —Ä–∞–∑).

```python
X_train_processed = preprocessor.fit_transform(X_train)
X_test_processed = preprocessor.transform(X_test)
```
**–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –∫–æ–Ω—Ü–µ–ø—Ü–∏—è:**
- **Fit –Ω–∞ train**: –£—á–∏–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è —Ç–æ–ª—å–∫–æ –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.
- **Transform –Ω–∞ –æ–±–æ–∏—Ö**: –ü—Ä–∏–º–µ–Ω—è–µ–º –≤—ã—É—á–µ–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∫ –æ–±–æ–∏–º –Ω–∞–±–æ—Ä–∞–º.
- **–ü–æ—á–µ–º—É?**: –ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç —É—Ç–µ—á–∫—É —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –ø—Ä–∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–µ.

---

## Model Deep Dive

### Linear Regression / –õ–∏–Ω–µ–π–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è

#### üá¨üáß English
**How it works:**
1. Assumes a linear relationship: $y = w_1x_1 + w_2x_2 + ... + b$
2. Finds weights ($w$) and bias ($b$) that minimize Mean Squared Error.
3. Uses **Ordinary Least Squares** (OLS) - analytical solution (no iterations).

**Pros:**
- Fast to train
- Interpretable coefficients
- Works well with linear relationships

**Cons:**
- Cannot capture non-linear patterns
- Assumes features are independent (no complex interactions)

**When to use:**
- Baseline model (always try first)
- When interpretability is critical (banking, medical)

#### üá∑üá∫ –†—É—Å—Å–∫–∏–π
**–ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç:**
1. –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç –ª–∏–Ω–µ–π–Ω—É—é —Å–≤—è–∑—å: $y = w_1x_1 + w_2x_2 + ... + b$
2. –ù–∞—Ö–æ–¥–∏—Ç –≤–µ—Å–∞ ($w$) –∏ —Å–º–µ—â–µ–Ω–∏–µ ($b$), –∫–æ—Ç–æ—Ä—ã–µ –º–∏–Ω–∏–º–∏–∑–∏—Ä—É—é—Ç —Å—Ä–µ–¥–Ω–µ–∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω—É—é –æ—à–∏–±–∫—É.
3. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç **–ú–µ—Ç–æ–¥ –Ω–∞–∏–º–µ–Ω—å—à–∏—Ö –∫–≤–∞–¥—Ä–∞—Ç–æ–≤** (OLS) - –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–æ–µ —Ä–µ—à–µ–Ω–∏–µ (–±–µ–∑ –∏—Ç–µ—Ä–∞—Ü–∏–π).

**–ü–ª—é—Å—ã:**
- –ë—ã—Å—Ç—Ä–æ–µ –æ–±—É—á–µ–Ω–∏–µ
- –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º—ã–µ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã
- –•–æ—Ä–æ—à–æ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –ª–∏–Ω–µ–π–Ω—ã–º–∏ —Å–≤—è–∑—è–º–∏

**–ú–∏–Ω—É—Å—ã:**
- –ù–µ –ª–æ–≤–∏—Ç –Ω–µ–ª–∏–Ω–µ–π–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
- –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–Ω–µ—Ç —Å–ª–æ–∂–Ω—ã—Ö –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π)

**–ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å:**
- –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å (–≤—Å–µ–≥–¥–∞ –ø—Ä–æ–±—É–µ–º –ø–µ—Ä–≤–æ–π)
- –ö–æ–≥–¥–∞ –≤–∞–∂–Ω–∞ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å (–±–∞–Ω–∫–∏, –º–µ–¥–∏—Ü–∏–Ω–∞)

---

### Random Forest Regressor / –°–ª—É—á–∞–π–Ω—ã–π –ª–µ—Å

#### üá¨üáß English
**How it works:**
1. **Bootstrap Aggregating** (Bagging): Creates 200 random subsets of training data.
2. **Tree Building**: Grows a decision tree on each subset.
   - At each split, considers random subset of features (default: ‚àön_features for regression).
3. **Prediction**: Averages predictions from all 200 trees.

**Why it's powerful:**
- **Reduces overfitting**: Individual trees overfit, but averaging smooths this out.
- **Captures non-linearity**: Trees naturally handle complex interactions.
- **Robust**: Not sensitive to outliers or feature scaling.

**Hyperparameters:**
- **`n_estimators=200`**: Number of trees (more trees = better, but slower).
- **`max_depth`**: Maximum tree depth (default: unlimited, grows until pure splits).

**When to use:**
- Default choice for tabular data
- When non-linear relationships exist
- When you don't want to spend time tuning

#### üá∑üá∫ –†—É—Å—Å–∫–∏–π
**–ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç:**
1. **Bootstrap Aggregating**: –°–æ–∑–¥–∞–µ—Ç 200 —Å–ª—É—á–∞–π–Ω—ã—Ö –ø–æ–¥–≤—ã–±–æ—Ä–æ–∫ –∏–∑ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö.
2. **–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –¥–µ—Ä–µ–≤—å–µ–≤**: –°—Ç—Ä–æ–∏—Ç –¥–µ—Ä–µ–≤–æ —Ä–µ—à–µ–Ω–∏–π –Ω–∞ –∫–∞–∂–¥–æ–π –ø–æ–¥–≤—ã–±–æ—Ä–∫–µ.
   - –í –∫–∞–∂–¥–æ–º —É–∑–ª–µ —Å–º–æ—Ç—Ä–∏—Ç –Ω–∞ —Å–ª—É—á–∞–π–Ω–æ–µ –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: ‚àön_features –¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏).
3. **–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ**: –£—Å—Ä–µ–¥–Ω—è–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –æ—Ç –≤—Å–µ—Ö 200 –¥–µ—Ä–µ–≤—å–µ–≤.

**–ü–æ—á–µ–º—É —ç—Ç–æ –º–æ—â–Ω–æ:**
- **–°–Ω–∏–∂–∞–µ—Ç –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ**: –û—Ç–¥–µ–ª—å–Ω—ã–µ –¥–µ—Ä–µ–≤—å—è –ø–µ—Ä–µ–æ–±—É—á–∞—é—Ç—Å—è, –Ω–æ —É—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ —ç—Ç–æ —Å–≥–ª–∞–∂–∏–≤–∞–µ—Ç.
- **–õ–æ–≤–∏—Ç –Ω–µ–ª–∏–Ω–µ–π–Ω–æ—Å—Ç—å**: –î–µ—Ä–µ–≤—å—è –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ —Ä–∞–±–æ—Ç–∞—é—Ç —Å–æ —Å–ª–æ–∂–Ω—ã–º–∏ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è–º–∏.
- **–£—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å**: –ù–µ —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–∞ –∫ –≤—ã–±—Ä–æ—Å–∞–º –∏–ª–∏ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—é –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.

**–ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã:**
- **`n_estimators=200`**: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–µ—Ä–µ–≤—å–µ–≤ (–±–æ–ª—å—à–µ = –ª—É—á—à–µ, –Ω–æ –º–µ–¥–ª–µ–Ω–Ω–µ–µ).
- **`max_depth`**: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –≥–ª—É–±–∏–Ω–∞ –¥–µ—Ä–µ–≤–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: –±–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π).

**–ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å:**
- –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –≤—ã–±–æ—Ä –¥–ª—è —Ç–∞–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
- –ö–æ–≥–¥–∞ –µ—Å—Ç—å –Ω–µ–ª–∏–Ω–µ–π–Ω—ã–µ —Å–≤—è–∑–∏
- –ö–æ–≥–¥–∞ –Ω–µ—Ç –≤—Ä–µ–º–µ–Ω–∏ –Ω–∞ –Ω–∞—Å—Ç—Ä–æ–π–∫—É

---

### Gradient Boosting Regressor / –ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π –±—É—Å—Ç–∏–Ω–≥

#### üá¨üáß English
**How it works (simplified):**
1. **Start**: Make initial prediction (e.g., mean of all targets).
2. **Sequential Learning**:
   - Train tree to predict residuals (errors) of previous model.
   - Add this tree to ensemble with small weight (learning_rate).
   - Update predictions.
3. **Repeat** 200 times (n_estimators).

**Key difference from Random Forest:**
- **RF**: Trees trained in parallel (independent).
- **GB**: Trees trained sequentially (each corrects previous).

**Why it's powerful:**
- **Precision**: Iteratively reduces errors.
- **Often best performance** on tabular data.

**Hyperparameters:**
- **`n_estimators=200`**: Number of boosting stages.
- **`learning_rate`**: Shrinkage factor (default: 0.1). Lower = slower but more precise.
- **`max_depth`**: Tree depth (default: 3, shallow trees prevent overfitting).

**Trade-offs:**
- **Pros**: Best accuracy on tabular data.
- **Cons**: Slower to train, easier to overfit (needs tuning).

#### üá∑üá∫ –†—É—Å—Å–∫–∏–π
**–ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç (—É–ø—Ä–æ—â–µ–Ω–Ω–æ):**
1. **–°—Ç–∞—Ä—Ç**: –î–µ–ª–∞–µ–º –Ω–∞—á–∞–ª—å–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Å—Ä–µ–¥–Ω–µ–µ –≤—Å–µ—Ö —Ü–µ–ª–µ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π).
2. **–ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ**:
   - –û–±—É—á–∞–µ–º –¥–µ—Ä–µ–≤–æ –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å –æ—Å—Ç–∞—Ç–∫–∏ (–æ—à–∏–±–∫–∏) –ø—Ä–µ–¥—ã–¥—É—â–µ–π –º–æ–¥–µ–ª–∏.
   - –î–æ–±–∞–≤–ª—è–µ–º —ç—Ç–æ –¥–µ—Ä–µ–≤–æ –≤ –∞–Ω—Å–∞–º–±–ª—å —Å –º–∞–ª—ã–º –≤–µ—Å–æ–º (learning_rate).
   - –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è.
3. **–ü–æ–≤—Ç–æ—Ä—è–µ–º** 200 —Ä–∞–∑ (n_estimators).

**–ö–ª—é—á–µ–≤–æ–µ –æ—Ç–ª–∏—á–∏–µ –æ—Ç Random Forest:**
- **RF**: –î–µ—Ä–µ–≤—å—è –æ–±—É—á–∞—é—Ç—Å—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ (–Ω–µ–∑–∞–≤–∏—Å–∏–º–æ).
- **GB**: –î–µ—Ä–µ–≤—å—è –æ–±—É—á–∞—é—Ç—Å—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ (–∫–∞–∂–¥–æ–µ –∏—Å–ø—Ä–∞–≤–ª—è–µ—Ç –ø—Ä–µ–¥—ã–¥—É—â–µ–µ).

**–ü–æ—á–µ–º—É —ç—Ç–æ –º–æ—â–Ω–æ:**
- **–¢–æ—á–Ω–æ—Å—Ç—å**: –ò—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ —É–º–µ–Ω—å—à–∞–µ—Ç –æ—à–∏–±–∫–∏.
- **–ß–∞—Å—Ç–æ –ª—É—á—à–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å** –Ω–∞ —Ç–∞–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.

**–ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã:**
- **`n_estimators=200`**: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç—Ç–∞–ø–æ–≤ –±—É—Å—Ç–∏–Ω–≥–∞.
- **`learning_rate`**: –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Å–∂–∞—Ç–∏—è (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 0.1). –ú–µ–Ω—å—à–µ = –º–µ–¥–ª–µ–Ω–Ω–µ–µ, –Ω–æ —Ç–æ—á–Ω–µ–µ.
- **`max_depth`**: –ì–ª—É–±–∏–Ω–∞ –¥–µ—Ä–µ–≤–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 3, –º–µ–ª–∫–∏–µ –¥–µ—Ä–µ–≤—å—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞—é—Ç –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ).

**–ö–æ–º–ø—Ä–æ–º–∏—Å—Å—ã:**
- **–ü–ª—é—Å—ã**: –õ—É—á—à–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ —Ç–∞–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.
- **–ú–∏–Ω—É—Å—ã**: –ú–µ–¥–ª–µ–Ω–Ω–µ–µ –æ–±—É—á–∞–µ—Ç—Å—è, –ª–µ–≥—á–µ –ø–µ—Ä–µ–æ–±—É—á–∞–µ—Ç—Å—è (—Ç—Ä–µ–±—É–µ—Ç –Ω–∞—Å—Ç—Ä–æ–π–∫–∏).

---

## Q&A Section

### Q1: Why do we drop Cooling_Load?
**üá¨üáß English:**  
Because Heating_Load and Cooling_Load are both **outputs** of the same building simulation. They are highly correlated (r‚âà0.976) not because one causes the other, but because they share the same input parameters. Using Cooling_Load to predict Heating_Load is **data leakage** - in real applications, you wouldn't know Cooling_Load before predicting Heating_Load.

**üá∑üá∫ –†—É—Å—Å–∫–∏–π:**  
–ü–æ—Ç–æ–º—É —á—Ç–æ Heating_Load –∏ Cooling_Load - —ç—Ç–æ –æ–±–∞ **–≤—ã—Ö–æ–¥–∞** –æ–¥–Ω–æ–π –∏ —Ç–æ–π –∂–µ —Å–∏–º—É–ª—è—Ü–∏–∏ –∑–¥–∞–Ω–∏—è. –û–Ω–∏ —Å–∏–ª—å–Ω–æ –∫–æ—Ä—Ä–µ–ª–∏—Ä—É—é—Ç (r‚âà0.976) –Ω–µ –ø–æ—Ç–æ–º—É, —á—Ç–æ –æ–¥–Ω–æ –≤—ã–∑—ã–≤–∞–µ—Ç –¥—Ä—É–≥–æ–µ, –∞ –ø–æ—Ç–æ–º—É —á—Ç–æ –∑–∞–≤–∏—Å—è—Ç –æ—Ç –æ–¥–Ω–∏—Ö –∏ —Ç–µ—Ö –∂–µ –≤—Ö–æ–¥–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Cooling_Load –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è Heating_Load - —ç—Ç–æ **—É—Ç–µ—á–∫–∞ –¥–∞–Ω–Ω—ã—Ö** - –≤ —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ –≤—ã –Ω–µ –∑–Ω–∞–ª–∏ –±—ã Cooling_Load –¥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è Heating_Load.

---

### Q2: What is R¬≤ and why is it better than RMSE?
**üá¨üáß English:**  
**R¬≤ (Coefficient of Determination)** measures the proportion of variance in the target explained by your model.

Formula: $R^2 = 1 - \frac{SS_{res}}{SS_{tot}}$

- **RMSE**: Absolute error in target units (e.g., kWh). Hard to interpret without context.
- **R¬≤**: Normalized (0-1 scale). R¬≤=0.9 means "90% of variance explained" - universally understandable.

**Use both:**  
- R¬≤ for model comparison.
- RMSE to understand real-world error magnitude.

**üá∑üá∫ –†—É—Å—Å–∫–∏–π:**  
**R¬≤** –∏–∑–º–µ—Ä—è–µ—Ç –¥–æ–ª—é –¥–∏—Å–ø–µ—Ä—Å–∏–∏ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π, –æ–±—ä—è—Å–Ω–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª—å—é.

–§–æ—Ä–º—É–ª–∞: $R^2 = 1 - \frac{SS_{res}}{SS_{tot}}$

- **RMSE**: –ê–±—Å–æ–ª—é—Ç–Ω–∞—è –æ—à–∏–±–∫–∞ –≤ –µ–¥–∏–Ω–∏—Ü–∞—Ö —Ü–µ–ª–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –∫–í—Ç¬∑—á). –¢—Ä—É–¥–Ω–æ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä–æ–≤–∞—Ç—å –±–µ–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.
- **R¬≤**: –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω (—à–∫–∞–ª–∞ 0-1). R¬≤=0.9 –æ–∑–Ω–∞—á–∞–µ—Ç "90% –¥–∏—Å–ø–µ—Ä—Å–∏–∏ –æ–±—ä—è—Å–Ω–µ–Ω–æ" - –ø–æ–Ω—è—Ç–Ω–æ –≤—Å–µ–º.

**–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –æ–±–∞:**  
- R¬≤ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π.
- RMSE –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è —Ä–µ–∞–ª—å–Ω–æ–π –≤–µ–ª–∏—á–∏–Ω—ã –æ—à–∏–±–∫–∏.

---

### Q3: Why does Gradient Boosting perform best?
**üá¨üáß English:**  
1. **Sequential error correction**: Each tree fixes mistakes of the previous ensemble.
2. **Adaptive learning**: Focuses more on hard-to-predict samples (high residuals).
3. **Fine-tuning**: With low learning_rate, it makes small, precise adjustments.

Think of it as a sculptor: Random Forest is like making 200 rough sketches and averaging them. Gradient Boosting is like starting with a rough sketch and iteratively refining details.

**üá∑üá∫ –†—É—Å—Å–∫–∏–π:**  
1. **–ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –æ—à–∏–±–æ–∫**: –ö–∞–∂–¥–æ–µ –¥–µ—Ä–µ–≤–æ –∏—Å–ø—Ä–∞–≤–ª—è–µ—Ç –æ—à–∏–±–∫–∏ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ –∞–Ω—Å–∞–º–±–ª—è.
2. **–ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ**: –§–æ–∫—É—Å–∏—Ä—É–µ—Ç—Å—è –±–æ–ª—å—à–µ –Ω–∞ —Å–ª–æ–∂–Ω—ã—Ö –æ–±—Ä–∞–∑—Ü–∞—Ö (—Å –±–æ–ª—å—à–∏–º–∏ –æ—Å—Ç–∞—Ç–∫–∞–º–∏).
3. **–¢–æ—á–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞**: –° –º–∞–ª—ã–º learning_rate –¥–µ–ª–∞–µ—Ç –º–∞–ª–µ–Ω—å–∫–∏–µ, —Ç–æ—á–Ω—ã–µ –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∏.

–ü—Ä–µ–¥—Å—Ç–∞–≤—å—Ç–µ —Å–∫—É–ª—å–ø—Ç–æ—Ä–∞: Random Forest - —ç—Ç–æ 200 –≥—Ä—É–±—ã—Ö –Ω–∞–±—Ä–æ—Å–∫–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ—Ç–æ–º —É—Å—Ä–µ–¥–Ω—è—é—Ç—Å—è. Gradient Boosting - —ç—Ç–æ –Ω–∞—á–∞—Ç—å —Å –≥—Ä—É–±–æ–≥–æ –Ω–∞–±—Ä–æ—Å–∫–∞ –∏ –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ —É—Ç–æ—á–Ω—è—Ç—å –¥–µ—Ç–∞–ª–∏.

---

### Q4: When would Linear Regression outperform tree models?
**üá¨üáß English:**  
1. **Very small datasets** (< 100 samples): Trees overfit easily.
2. **Truly linear relationships**: If data follows a straight line, trees waste capacity.
3. **High-dimensional sparse data**: Text classification with 10,000+ features.
4. **Extrapolation needed**: Trees cannot predict beyond training range, linear models can.

**üá∑üá∫ –†—É—Å—Å–∫–∏–π:**  
1. **–û—á–µ–Ω—å –º–∞–ª–µ–Ω—å–∫–∏–µ –¥–∞—Ç–∞—Å–µ—Ç—ã** (< 100 –æ–±—Ä–∞–∑—Ü–æ–≤): –î–µ—Ä–µ–≤—å—è –ª–µ–≥–∫–æ –ø–µ—Ä–µ–æ–±—É—á–∞—é—Ç—Å—è.
2. **–î–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –ª–∏–Ω–µ–π–Ω—ã–µ —Å–≤—è–∑–∏**: –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã–µ —Å–ª–µ–¥—É—é—Ç –ø—Ä—è–º–æ–π –ª–∏–Ω–∏–∏, –¥–µ—Ä–µ–≤—å—è —Ç—Ä–∞—Ç—è—Ç —Ä–µ—Å—É—Ä—Å—ã –∑—Ä—è.
3. **–í—ã—Å–æ–∫–æ–º–µ—Ä–Ω—ã–µ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ**: –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤ —Å 10,000+ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏.
4. **–ù—É–∂–Ω–∞ —ç–∫—Å—Ç—Ä–∞–ø–æ–ª—è—Ü–∏—è**: –î–µ—Ä–µ–≤—å—è –Ω–µ –º–æ–≥—É—Ç –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å –∑–∞ –ø—Ä–µ–¥–µ–ª–∞–º–∏ –æ–±—É—á–∞—é—â–µ–≥–æ –¥–∏–∞–ø–∞–∑–æ–Ω–∞, –ª–∏–Ω–µ–π–Ω—ã–µ –º–æ–¥–µ–ª–∏ –º–æ–≥—É—Ç.

---

### Q5: How to choose n_estimators?
**üá¨üáß English:**  
**Rule of thumb:**
- Start with 100.
- Increase to 200-500 if compute time allows.
- Use **learning curves**: Plot validation score vs. n_estimators. Stop when curve plateaus.

**For Gradient Boosting specifically:**  
Use **early stopping** with validation set to automatically determine optimal n_estimators.

**üá∑üá∫ –†—É—Å—Å–∫–∏–π:**  
**–ü—Ä–∞–≤–∏–ª–æ:**
- –ù–∞—á–Ω–∏—Ç–µ —Å 100.
- –£–≤–µ–ª–∏—á—å—Ç–µ –¥–æ 200-500, –µ—Å–ª–∏ –ø–æ–∑–≤–æ–ª—è–µ—Ç –≤—Ä–µ–º—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π.
- –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ **–∫—Ä–∏–≤—ã–µ –æ–±—É—á–µ–Ω–∏—è**: –ü–æ—Å—Ç—Ä–æ–π—Ç–µ –≥—Ä–∞—Ñ–∏–∫ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ vs. n_estimators. –û—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ—Å—å, –∫–æ–≥–¥–∞ –∫—Ä–∏–≤–∞—è –≤—ã—Ö–æ–¥–∏—Ç –Ω–∞ –ø–ª–∞—Ç–æ.

**–î–ª—è Gradient Boosting –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ:**  
–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ **early stopping** —Å –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–º –Ω–∞–±–æ—Ä–æ–º, —á—Ç–æ–±—ã –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π n_estimators.

---

## Summary / –ò—Ç–æ–≥

### üá¨üáß English
This assignment covers the complete ML workflow:
1. **EDA**: Understand data distributions, correlations, and potential issues.
2. **Feature Engineering**: Create custom transformers for domain-specific features.
3. **Preprocessing**: Build modular pipelines for scalability.
4. **Modeling**: Compare linear vs. ensemble methods.
5. **Evaluation**: Use R¬≤ and RMSE to assess performance.

**Key takeaway**: Tree-based ensembles (RF, GB) excel at tabular regression tasks with non-linear relationships.

### üá∑üá∫ –†—É—Å—Å–∫–∏–π
–≠—Ç–æ –∑–∞–¥–∞–Ω–∏–µ –æ—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç –ø–æ–ª–Ω—ã–π ML workflow:
1. **EDA**: –ü–æ–Ω—è—Ç—å —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è, –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –∏ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã.
2. **Feature Engineering**: –°–æ–∑–¥–∞—Ç—å –∫–∞—Å—Ç–æ–º–Ω—ã–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã –¥–ª—è —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.
3. **–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞**: –ü–æ—Å—Ç—Ä–æ–∏—Ç—å –º–æ–¥—É–ª—å–Ω—ã–µ –ø–∞–π–ø–ª–∞–π–Ω—ã –¥–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç–∏.
4. **–ú–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ**: –°—Ä–∞–≤–Ω–∏—Ç—å –ª–∏–Ω–µ–π–Ω—ã–µ –∏ –∞–Ω—Å–∞–º–±–ª–µ–≤—ã–µ –º–µ—Ç–æ–¥—ã.
5. **–û—Ü–µ–Ω–∫–∞**: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å R¬≤ –∏ RMSE –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.

**–ì–ª–∞–≤–Ω—ã–π –≤—ã–≤–æ–¥**: –î—Ä–µ–≤–æ–≤–∏–¥–Ω—ã–µ –∞–Ω—Å–∞–º–±–ª–∏ (RF, GB) –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–Ω—ã –≤ –∑–∞–¥–∞—á–∞—Ö —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –Ω–∞ —Ç–∞–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Å –Ω–µ–ª–∏–Ω–µ–π–Ω—ã–º–∏ —Å–≤—è–∑—è–º–∏.
