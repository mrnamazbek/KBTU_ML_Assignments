# üõ°Ô∏è Defense Guide: Assignment 2 (Life Satisfaction)
# üá∑üá∫ –ì–∞–π–¥ –ø–æ –∑–∞—â–∏—Ç–µ: –ó–∞–¥–∞–Ω–∏–µ 2 (–£–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–µ–Ω–Ω–æ—Å—Ç—å –∂–∏–∑–Ω—å—é)

---

## üéØ Goal / –¶–µ–ª—å
**üá¨üáß English:**  
Predict if people in a country are happy (**Life Satisfaction**) based on how much money they make (**GDP per capita**). We compare two models: **Linear Regression** and **K-Nearest Neighbors (KNN)**.

**üá∑üá∫ –†—É—Å—Å–∫–∏–π:**  
–ü—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å, —Å—á–∞—Å—Ç–ª–∏–≤—ã –ª–∏ –ª—é–¥–∏ –≤ —Å—Ç—Ä–∞–Ω–µ (**–£–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–µ–Ω–Ω–æ—Å—Ç—å –∂–∏–∑–Ω—å—é**), –æ—Å–Ω–æ–≤—ã–≤–∞—è—Å—å –Ω–∞ —Ç–æ–º, —Å–∫–æ–ª—å–∫–æ –¥–µ–Ω–µ–≥ –æ–Ω–∏ –∑–∞—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç (**–í–í–ü –Ω–∞ –¥—É—à—É –Ω–∞—Å–µ–ª–µ–Ω–∏—è**). –ú—ã —Å—Ä–∞–≤–Ω–∏–≤–∞–µ–º –¥–≤–µ –º–æ–¥–µ–ª–∏: **–õ–∏–Ω–µ–π–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è** –∏ **–ú–µ—Ç–æ–¥ –±–ª–∏–∂–∞–π—à–∏—Ö —Å–æ—Å–µ–¥–µ–π (KNN)**.

---

## üß† Deep Code Analysis / –ì–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ –∫–æ–¥–∞

### 1. Data Preparation / –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
```python
X = df[['GDP per capita']].values
y = df['Life satisfaction'].values
```
*   **üá¨üáß Logic:**
    *   `X` (Features): Must be a **2D array** (matrix). That's why we use double brackets `[['...']]` or `.values` with reshaping. Scikit-Learn expects a list of rows, where each row is a list of features.
    *   `y` (Target): Is a **1D array** (vector).
*   **üá∑üá∫ –õ–æ–≥–∏–∫–∞:**
    *   `X` (–ü—Ä–∏–∑–Ω–∞–∫–∏): –î–æ–ª–∂–µ–Ω –±—ã—Ç—å **–¥–≤—É–º–µ—Ä–Ω—ã–º –º–∞—Å—Å–∏–≤–æ–º** (–º–∞—Ç—Ä–∏—Ü–µ–π). –ü–æ—ç—Ç–æ–º—É –º—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–≤–æ–π–Ω—ã–µ —Å–∫–æ–±–∫–∏ `[['...']]`. Scikit-Learn –æ–∂–∏–¥–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Å—Ç—Ä–æ–∫, –≥–¥–µ –∫–∞–∂–¥–∞—è —Å—Ç—Ä–æ–∫–∞ ‚Äî —ç—Ç–æ —Å–ø–∏—Å–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.
    *   `y` (–¶–µ–ª—å): –≠—Ç–æ **–æ–¥–Ω–æ–º–µ—Ä–Ω—ã–π –º–∞—Å—Å–∏–≤** (–≤–µ–∫—Ç–æ—Ä).

### 2. Linear Regression / –õ–∏–Ω–µ–π–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è
```python
model = LinearRegression()
model.fit(X, y)
```
*   **üá¨üáß Logic:** The model tries to draw a **straight line** ($y = mx + b$) that minimizes the error (distance) between the line and the data points.
*   **üá∑üá∫ –õ–æ–≥–∏–∫–∞:** –ú–æ–¥–µ–ª—å –ø—ã—Ç–∞–µ—Ç—Å—è –ø—Ä–æ–≤–µ—Å—Ç–∏ **–ø—Ä—è–º—É—é –ª–∏–Ω–∏—é** ($y = mx + b$), –∫–æ—Ç–æ—Ä–∞—è –º–∏–Ω–∏–º–∏–∑–∏—Ä—É–µ—Ç –æ—à–∏–±–∫—É (—Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ) –º–µ–∂–¥—É –ª–∏–Ω–∏–µ–π –∏ —Ç–æ—á–∫–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö.

### 3. K-Nearest Neighbors (KNN) / –ú–µ—Ç–æ–¥ –±–ª–∏–∂–∞–π—à–∏—Ö —Å–æ—Å–µ–¥–µ–π
```python
model = KNeighborsRegressor(n_neighbors=3)
```
*   **üá¨üáß Logic:** To predict happiness for a new country, the model finds the **3 countries** with the most similar GDP and calculates their average happiness.
*   **üá∑üá∫ –õ–æ–≥–∏–∫–∞:** –ß—Ç–æ–±—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å —Å—á–∞—Å—Ç—å–µ –¥–ª—è –Ω–æ–≤–æ–π —Å—Ç—Ä–∞–Ω—ã, –º–æ–¥–µ–ª—å –Ω–∞—Ö–æ–¥–∏—Ç **3 —Å—Ç—Ä–∞–Ω—ã** —Å —Å–∞–º—ã–º –ø–æ—Ö–æ–∂–∏–º –í–í–ü –∏ –≤—ã—á–∏—Å–ª—è–µ—Ç –∏—Ö —Å—Ä–µ–¥–Ω–µ–µ —Å—á–∞—Å—Ç—å–µ.

---

## üìâ Weak Points & Improvements / –°–ª–∞–±—ã–µ –º–µ—Å—Ç–∞ –∏ —É–ª—É—á—à–µ–Ω–∏—è

### 1. Single Feature / –û–¥–∏–Ω –ø—Ä–∏–∑–Ω–∞–∫
*   **üá¨üáß Weakness:** We only use **GDP**. Happiness depends on many things (health, freedom, corruption, weather).
*   **üá∑üá∫ –°–ª–∞–±–æ—Å—Ç—å:** –ú—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ **–í–í–ü**. –°—á–∞—Å—Ç—å–µ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –º–Ω–æ–≥–∏—Ö –≤–µ—â–µ–π (–∑–¥–æ—Ä–æ–≤—å–µ, —Å–≤–æ–±–æ–¥–∞, –∫–æ—Ä—Ä—É–ø—Ü–∏—è, –ø–æ–≥–æ–¥–∞).
*   **üöÄ Improvement:** Use **Multivariate Regression** (add more columns like 'Health', 'Freedom'). / –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å **–ú–Ω–æ–≥–æ—Ñ–∞–∫—Ç–æ—Ä–Ω—É—é —Ä–µ–≥—Ä–µ—Å—Å–∏—é** (–¥–æ–±–∞–≤–∏—Ç—å –∫–æ–ª–æ–Ω–∫–∏ '–ó–¥–æ—Ä–æ–≤—å–µ', '–°–≤–æ–±–æ–¥–∞').

### 2. Linearity Assumption / –ü—Ä–µ–¥–ø–æ–ª–æ–∂–µ–Ω–∏–µ –ª–∏–Ω–µ–π–Ω–æ—Å—Ç–∏
*   **üá¨üáß Weakness:** Linear Regression assumes the relationship is a straight line. But maybe after a certain point, more money doesn't make you happier (diminishing returns).
*   **üá∑üá∫ –°–ª–∞–±–æ—Å—Ç—å:** –õ–∏–Ω–µ–π–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç, —á—Ç–æ —Å–≤—è–∑—å ‚Äî —ç—Ç–æ –ø—Ä—è–º–∞—è –ª–∏–Ω–∏—è. –ù–æ, –≤–æ–∑–º–æ–∂–Ω–æ, –ø–æ—Å–ª–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–π —Ç–æ—á–∫–∏ –¥–µ–Ω—å–≥–∏ –ø–µ—Ä–µ—Å—Ç–∞—é—Ç –ø—Ä–∏–Ω–æ—Å–∏—Ç—å —Å—á–∞—Å—Ç—å–µ (—É–±—ã–≤–∞—é—â–∞—è –ø–æ–ª–µ–∑–Ω–æ—Å—Ç—å).
*   **üöÄ Improvement:** Use **Polynomial Regression** (curved line). / –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å **–ü–æ–ª–∏–Ω–æ–º–∏–∞–ª—å–Ω—É—é —Ä–µ–≥—Ä–µ—Å—Å–∏—é** (–∏–∑–æ–≥–Ω—É—Ç–∞—è –ª–∏–Ω–∏—è).

---

## ‚ùì Professor Questions / –í–æ–ø—Ä–æ—Å—ã –ø—Ä–æ—Ñ–µ—Å—Å–æ—Ä–∞

### Q1: Why did you choose Linear Regression?
### –í1: –ü–æ—á–µ–º—É –≤—ã –≤—ã–±—Ä–∞–ª–∏ –ª–∏–Ω–µ–π–Ω—É—é —Ä–µ–≥—Ä–µ—Å—Å–∏—é?
*   **üá¨üáß Answer:** It is the simplest model. The scatter plot showed a general upward trend, so a straight line is a good starting point (baseline).
*   **üá∑üá∫ –û—Ç–≤–µ—Ç:** –≠—Ç–æ —Å–∞–º–∞—è –ø—Ä–æ—Å—Ç–∞—è –º–æ–¥–µ–ª—å. –¢–æ—á–µ—á–Ω—ã–π –≥—Ä–∞—Ñ–∏–∫ –ø–æ–∫–∞–∑–∞–ª –æ–±—â–∏–π —Ç—Ä–µ–Ω–¥ –≤–≤–µ—Ä—Ö, –ø–æ—ç—Ç–æ–º—É –ø—Ä—è–º–∞—è –ª–∏–Ω–∏—è ‚Äî —Ö–æ—Ä–æ—à–∞—è –æ—Ç–ø—Ä–∞–≤–Ω–∞—è —Ç–æ—á–∫–∞ (–±–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å).

### Q2: What is the difference between `fit` and `predict`?
### –í2: –í —á–µ–º —Ä–∞–∑–Ω–∏—Ü–∞ –º–µ–∂–¥—É `fit` –∏ `predict`?
*   **üá¨üáß Answer:**
    *   `fit(X, y)`: **Training**. The model looks at the data and calculates the best parameters (slope and intercept).
    *   `predict(X)`: **Inference**. The model uses the calculated parameters to guess the target for new data.
*   **üá∑üá∫ –û—Ç–≤–µ—Ç:**
    *   `fit(X, y)`: **–û–±—É—á–µ–Ω–∏–µ**. –ú–æ–¥–µ–ª—å —Å–º–æ—Ç—Ä–∏—Ç –Ω–∞ –¥–∞–Ω–Ω—ã–µ –∏ –≤—ã—á–∏—Å–ª—è–µ—Ç –ª—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (–Ω–∞–∫–ª–æ–Ω –∏ —Å–¥–≤–∏–≥).
    *   `predict(X)`: **–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ**. –ú–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≤—ã—á–∏—Å–ª–µ–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã, —á—Ç–æ–±—ã —É–≥–∞–¥–∞—Ç—å –æ—Ç–≤–µ—Ç –¥–ª—è –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö.

### Q3: What happens if `n_neighbors=1` in KNN?
### –í3: –ß—Ç–æ –±—É–¥–µ—Ç, –µ—Å–ª–∏ `n_neighbors=1` –≤ KNN?
*   **üá¨üáß Answer:** The model becomes **unstable (overfitting)**. It will blindly copy the value of the single nearest neighbor, even if that point is an outlier or noise.
*   **üá∑üá∫ –û—Ç–≤–µ—Ç:** –ú–æ–¥–µ–ª—å —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è **–Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ–π (–ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ)**. –û–Ω–∞ –±—É–¥–µ—Ç —Å–ª–µ–ø–æ –∫–æ–ø–∏—Ä–æ–≤–∞—Ç—å –∑–Ω–∞—á–µ–Ω–∏–µ –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –±–ª–∏–∂–∞–π—à–µ–≥–æ —Å–æ—Å–µ–¥–∞, –¥–∞–∂–µ –µ—Å–ª–∏ —ç—Ç–∞ —Ç–æ—á–∫–∞ ‚Äî –≤—ã–±—Ä–æ—Å –∏–ª–∏ —à—É–º.

---

## üìê Math Intuition / –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∏–Ω—Ç—É–∏—Ü–∏—è

### Linear Regression Equation
$$ y = \theta_0 + \theta_1 x $$
*   $\theta_0$ (Intercept): Where the line starts on the Y-axis. (–ë–∞–∑–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å —Å—á–∞—Å—Ç—å—è).
*   $\theta_1$ (Slope): How much happiness increases for every $1 increase in GDP. (–°–∫–æ—Ä–æ—Å—Ç—å —Ä–æ—Å—Ç–∞ —Å—á–∞—Å—Ç—å—è).

### KNN Formula
$$ y = \frac{1}{k} \sum_{i=1}^{k} y_i $$
*   **English:** Average of the $k$ nearest neighbors.
*   **–†—É—Å—Å–∫–∏–π:** –°—Ä–µ–¥–Ω–µ–µ –∞—Ä–∏—Ñ–º–µ—Ç–∏—á–µ—Å–∫–æ–µ $k$ –±–ª–∏–∂–∞–π—à–∏—Ö —Å–æ—Å–µ–¥–µ–π.
