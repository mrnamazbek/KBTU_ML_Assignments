# üõ°Ô∏è Defense Guide: Assignment 2 (Life Satisfaction)
# –ì–∞–π–¥ –ø–æ –∑–∞—â–∏—Ç–µ: –ó–∞–¥–∞–Ω–∏–µ 2 (–£–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–µ–Ω–Ω–æ—Å—Ç—å –∂–∏–∑–Ω—å—é)

---

## üéØ Goal / –¶–µ–ª—å
**üá¨üáß English:** Predict if people in a country are happy (**Life Satisfaction**) based on how much money they make (**GDP per capita**).
**üá∑üá∫ –†—É—Å—Å–∫–∏–π:** –ü—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å, —Å—á–∞—Å—Ç–ª–∏–≤—ã –ª–∏ –ª—é–¥–∏ –≤ —Å—Ç—Ä–∞–Ω–µ (**–£–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–µ–Ω–Ω–æ—Å—Ç—å –∂–∏–∑–Ω—å—é**), –æ—Å–Ω–æ–≤—ã–≤–∞—è—Å—å –Ω–∞ —Ç–æ–º, —Å–∫–æ–ª—å–∫–æ –¥–µ–Ω–µ–≥ –æ–Ω–∏ –∑–∞—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç (**–í–í–ü –Ω–∞ –¥—É—à—É –Ω–∞—Å–µ–ª–µ–Ω–∏—è**).

---

## üß† Deep Code Analysis / –ì–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ –∫–æ–¥–∞

### 1. Data Preparation / –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
```python
X = np.c_[country_stats["GDP per capita"]]
y = np.c_[country_stats["Life satisfaction"]]
```
*   **üá¨üáß Explanation:** `np.c_` translates the data into a column (vertical list). Scikit-Learn expects input `X` to be a 2D array (matrix), not a flat list.
*   **üá∑üá∫ –û–±—ä—è—Å–Ω–µ–Ω–∏–µ:** `np.c_` –ø—Ä–µ–≤—Ä–∞—â–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –≤ —Å—Ç–æ–ª–±–µ—Ü (–≤–µ—Ä—Ç–∏–∫–∞–ª—å–Ω—ã–π —Å–ø–∏—Å–æ–∫). Scikit-Learn —Ç—Ä–µ–±—É–µ—Ç, —á—Ç–æ–±—ã –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ `X` –±—ã–ª–∏ –¥–≤—É–º–µ—Ä–Ω—ã–º –º–∞—Å—Å–∏–≤–æ–º (–º–∞—Ç—Ä–∏—Ü–µ–π), –∞ –Ω–µ –ø–ª–æ—Å–∫–∏–º —Å–ø–∏—Å–∫–æ–º.

### 2. Linear Regression / –õ–∏–Ω–µ–π–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è
```python
model = sklearn.linear_model.LinearRegression()
model.fit(X, y)
```
*   **üá¨üáß Logic:** It tries to draw a straight line that minimizes the distance to all data points.
*   **üá∑üá∫ –õ–æ–≥–∏–∫–∞:** –ú–æ–¥–µ–ª—å –ø—ã—Ç–∞–µ—Ç—Å—è –ø—Ä–æ–≤–µ—Å—Ç–∏ –ø—Ä—è–º—É—é –ª–∏–Ω–∏—é —Ç–∞–∫, —á—Ç–æ–±—ã —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –æ—Ç –Ω–µ–µ –¥–æ –≤—Å–µ—Ö —Ç–æ—á–µ–∫ –¥–∞–Ω–Ω—ã—Ö –±—ã–ª–æ –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º.
*   **üîë Key Parameter:** None (it's a simple equation $y = mx + b$).
*   **ÔøΩ –ö–ª—é—á–µ–≤–æ–π –ø–∞—Ä–∞–º–µ—Ç—Ä:** –ù–µ—Ç (—ç—Ç–æ –ø—Ä–æ—Å—Ç–æ–µ —É—Ä–∞–≤–Ω–µ–Ω–∏–µ $y = mx + b$).

### 3. K-Nearest Neighbors / –ú–µ—Ç–æ–¥ –±–ª–∏–∂–∞–π—à–∏—Ö —Å–æ—Å–µ–¥–µ–π
```python
model = sklearn.neighbors.KNeighborsRegressor(n_neighbors=3)
```
*   **üá¨üáß Logic:** To predict for a new country, it finds the 3 countries with the most similar GDP and averages their happiness.
*   **üá∑üá∫ –õ–æ–≥–∏–∫–∞:** –ß—Ç–æ–±—ã —Å–¥–µ–ª–∞—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è –Ω–æ–≤–æ–π —Å—Ç—Ä–∞–Ω—ã, –∞–ª–≥–æ—Ä–∏—Ç–º –Ω–∞—Ö–æ–¥–∏—Ç 3 —Å—Ç—Ä–∞–Ω—ã —Å —Å–∞–º—ã–º –ø–æ—Ö–æ–∂–∏–º –í–í–ü –∏ –±–µ—Ä–µ—Ç —Å—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –∏—Ö —Å—á–∞—Å—Ç—å—è.
*   **üîë Key Parameter (`n_neighbors=3`):**
    *   **Why 3?** Small enough to capture local patterns, big enough to ignore noise.
    *   **–ü–æ—á–µ–º—É 3?** –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –º–∞–ª–æ, —á—Ç–æ–±—ã —É–ª–æ–≤–∏—Ç—å –ª–æ–∫–∞–ª—å–Ω—ã–µ –∑–∞–∫–æ–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç–∏, –Ω–æ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –º–Ω–æ–≥–æ, —á—Ç–æ–±—ã –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å —à—É–º.

---

## üìâ Weak Points & Improvements / –°–ª–∞–±—ã–µ –º–µ—Å—Ç–∞ –∏ —É–ª—É—á—à–µ–Ω–∏—è

1.  **üá¨üáß Weakness:** We only use **one feature** (GDP). Happiness depends on health, freedom, corruption, etc.
    *   **üá∑üá∫ –°–ª–∞–±–æ—Å—Ç—å:** –ú—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ **–æ–¥–∏–Ω –ø—Ä–∏–∑–Ω–∞–∫** (–í–í–ü). –°—á–∞—Å—Ç—å–µ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –∑–¥–æ—Ä–æ–≤—å—è, —Å–≤–æ–±–æ–¥—ã, –∫–æ—Ä—Ä—É–ø—Ü–∏–∏ –∏ —Ç.–¥.
    *   **üöÄ Improvement:** Add more features (Multi-variate regression). / –î–æ–±–∞–≤–∏—Ç—å –±–æ–ª—å—à–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–ú–Ω–æ–≥–æ—Ñ–∞–∫—Ç–æ—Ä–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è).

2.  **üá¨üáß Weakness:** Linear Regression assumes the world is simple (straight line). Real life is complex.
    *   **üá∑üá∫ –°–ª–∞–±–æ—Å—Ç—å:** –õ–∏–Ω–µ–π–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç, —á—Ç–æ –º–∏—Ä –ø—Ä–æ—Å—Ç (–ø—Ä—è–º–∞—è –ª–∏–Ω–∏—è). –†–µ–∞–ª—å–Ω–∞—è –∂–∏–∑–Ω—å —Å–ª–æ–∂–Ω–µ–µ.
    *   **üöÄ Improvement:** Use Polynomial Regression (curved line). / –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–æ–ª–∏–Ω–æ–º–∏–∞–ª—å–Ω—É—é —Ä–µ–≥—Ä–µ—Å—Å–∏—é (–∏–∑–æ–≥–Ω—É—Ç–∞—è –ª–∏–Ω–∏—è).

---

## ‚ùì Professor Questions / –í–æ–ø—Ä–æ—Å—ã –ø—Ä–æ—Ñ–µ—Å—Å–æ—Ä–∞

### Q1: Why did you choose Linear Regression?
### –í1: –ü–æ—á–µ–º—É –≤—ã –≤—ã–±—Ä–∞–ª–∏ –ª–∏–Ω–µ–π–Ω—É—é —Ä–µ–≥—Ä–µ—Å—Å–∏—é?
*   **üá¨üáß Answer:** Because when I plotted the data, I saw a linear trend. As money goes up, happiness goes up. It's a simple baseline model.
*   **üá∑üá∫ –û—Ç–≤–µ—Ç:** –ü–æ—Ç–æ–º—É —á—Ç–æ, –∫–æ–≥–¥–∞ —è –ø–æ—Å—Ç—Ä–æ–∏–ª –≥—Ä–∞—Ñ–∏–∫, —è —É–≤–∏–¥–µ–ª –ª–∏–Ω–µ–π–Ω—ã–π —Ç—Ä–µ–Ω–¥. –ß–µ–º –±–æ–ª—å—à–µ –¥–µ–Ω–µ–≥, —Ç–µ–º –±–æ–ª—å—à–µ —Å—á–∞—Å—Ç—å—è. –≠—Ç–æ –ø—Ä–æ—Å—Ç–∞—è –±–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å.

### Q2: What is the difference between `fit` and `predict`?
### –í2: –í —á–µ–º —Ä–∞–∑–Ω–∏—Ü–∞ –º–µ–∂–¥—É `fit` –∏ `predict`?
*   **üá¨üáß Answer:** `fit` is for **learning** (finding the pattern in training data). `predict` is for **using** that pattern to guess answers for new data.
*   **üá∑üá∫ –û—Ç–≤–µ—Ç:** `fit` ‚Äî —ç—Ç–æ **–æ–±—É—á–µ–Ω–∏–µ** (–ø–æ–∏—Å–∫ –∑–∞–∫–æ–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç–µ–π –≤ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö). `predict` ‚Äî —ç—Ç–æ **–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ** —ç—Ç–∏—Ö –∑–∞–∫–æ–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç–µ–π –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö.

### Q3: What happens if `n_neighbors=1` in KNN?
### –í3: –ß—Ç–æ –±—É–¥–µ—Ç, –µ—Å–ª–∏ `n_neighbors=1` –≤ KNN?
*   **üá¨üáß Answer:** The model becomes **overfitted**. It will just copy the closest point exactly, even if that point is an outlier (noise).
*   **üá∑üá∫ –û—Ç–≤–µ—Ç:** –ú–æ–¥–µ–ª—å **–ø–µ—Ä–µ–æ–±—É—á–∏—Ç—Å—è**. –û–Ω–∞ –±—É–¥–µ—Ç –ø—Ä–æ—Å—Ç–æ –∫–æ–ø–∏—Ä–æ–≤–∞—Ç—å –±–ª–∏–∂–∞–π—à—É—é —Ç–æ—á–∫—É —Ç–æ—á—å-–≤-—Ç–æ—á—å, –¥–∞–∂–µ –µ—Å–ª–∏ —ç—Ç–∞ —Ç–æ—á–∫–∞ ‚Äî –≤—ã–±—Ä–æ—Å (—à—É–º).

---

## üìê Math Intuition / –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∏–Ω—Ç—É–∏—Ü–∏—è

**Linear Regression:**
$$ y = \theta_0 + \theta_1 x $$
*   We want to find $\theta_0$ (intercept/—Å–¥–≤–∏–≥) and $\theta_1$ (slope/–Ω–∞–∫–ª–æ–Ω).
*   –ú—ã —Ö–æ—Ç–∏–º –Ω–∞–π—Ç–∏ $\theta_0$ (–≥–¥–µ –ª–∏–Ω–∏—è –ø–µ—Ä–µ—Å–µ–∫–∞–µ—Ç –æ—Å—å Y) –∏ $\theta_1$ (–Ω–∞—Å–∫–æ–ª—å–∫–æ –∫—Ä—É—Ç–æ –ª–∏–Ω–∏—è –∏–¥–µ—Ç –≤–≤–µ—Ä—Ö).

**KNN:**
$$ y = \frac{1}{k} \sum_{i=1}^{k} y_i $$
*   We just take the average of the $k$ nearest neighbors.
*   –ú—ã –ø—Ä–æ—Å—Ç–æ –±–µ—Ä–µ–º —Å—Ä–µ–¥–Ω–µ–µ –∞—Ä–∏—Ñ–º–µ—Ç–∏—á–µ—Å–∫–æ–µ $k$ –±–ª–∏–∂–∞–π—à–∏—Ö —Å–æ—Å–µ–¥–µ–π.
