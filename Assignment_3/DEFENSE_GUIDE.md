# üõ°Ô∏è Defense Guide: Assignment 3 (Iris Classification)
# üá∑üá∫ –ì–∞–π–¥ –ø–æ –∑–∞—â–∏—Ç–µ: –ó–∞–¥–∞–Ω–∏–µ 3 (–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –ò—Ä–∏—Å–æ–≤)

---

## üéØ Goal / –¶–µ–ª—å
**üá¨üáß English:**  
Teach the computer to recognize **3 types of Iris flowers** (Setosa, Versicolor, Virginica) by looking at their petal and sepal measurements. We use the **K-Nearest Neighbors (KNN)** algorithm.

**üá∑üá∫ –†—É—Å—Å–∫–∏–π:**  
–ù–∞—É—á–∏—Ç—å –∫–æ–º–ø—å—é—Ç–µ—Ä —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞—Ç—å **3 –≤–∏–¥–∞ —Ü–≤–µ—Ç–æ–≤ –ò—Ä–∏—Å–∞** (Setosa, Versicolor, Virginica), –≥–ª—è–¥—è –Ω–∞ —Ä–∞–∑–º–µ—Ä—ã –∏—Ö –ª–µ–ø–µ—Å—Ç–∫–æ–≤ –∏ —á–∞—à–µ–ª–∏—Å—Ç–∏–∫–æ–≤. –ú—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º –∞–ª–≥–æ—Ä–∏—Ç–º **–ú–µ—Ç–æ–¥ –±–ª–∏–∂–∞–π—à–∏—Ö —Å–æ—Å–µ–¥–µ–π (KNN)**.

---

## üß† Deep Code Analysis / –ì–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ –∫–æ–¥–∞

### 1. Pairplot Visualization / –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è Pairplot
```python
sns.pairplot(df, hue="species")
```
*   **üá¨üáß Logic:** Creates a matrix of scatter plots for every pair of features. The `hue="species"` argument colors the points based on the flower type.
*   **üá∑üá∫ –õ–æ–≥–∏–∫–∞:** –°–æ–∑–¥–∞–µ—Ç –º–∞—Ç—Ä–∏—Ü—É —Ç–æ—á–µ—á–Ω—ã—Ö –≥—Ä–∞—Ñ–∏–∫–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–π –ø–∞—Ä—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤. –ê—Ä–≥—É–º–µ–Ω—Ç `hue="species"` —Ä–∞—Å–∫—Ä–∞—à–∏–≤–∞–µ—Ç —Ç–æ—á–∫–∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –≤–∏–¥–∞ —Ü–≤–µ—Ç–∫–∞.
*   **Why?** To identify which features best separate the classes. We observed that **Petal Length** and **Petal Width** provide the clearest separation.
*   **–ó–∞—á–µ–º?** –ß—Ç–æ–±—ã –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å, –∫–∞–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –ª—É—á—à–µ –≤—Å–µ–≥–æ —Ä–∞–∑–¥–µ–ª—è—é—Ç –∫–ª–∞—Å—Å—ã. –ú—ã –∑–∞–º–µ—Ç–∏–ª–∏, —á—Ç–æ **–î–ª–∏–Ω–∞ –ª–µ–ø–µ—Å—Ç–∫–∞** –∏ **–®–∏—Ä–∏–Ω–∞ –ª–µ–ø–µ—Å—Ç–∫–∞** –¥–∞—é—Ç —Å–∞–º–æ–µ —á–µ—Ç–∫–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ.

### 2. K-Nearest Neighbors (KNN) / –ú–µ—Ç–æ–¥ –±–ª–∏–∂–∞–π—à–∏—Ö —Å–æ—Å–µ–¥–µ–π
```python
knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(X_train, y_train)
```
*   **üá¨üáß Logic:** The model stores the training data. To classify a new flower, it looks at the **3 closest** flowers in the feature space.
*   **üá∑üá∫ –õ–æ–≥–∏–∫–∞:** –ú–æ–¥–µ–ª—å –∑–∞–ø–æ–º–∏–Ω–∞–µ—Ç —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ. –ß—Ç–æ–±—ã –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å –Ω–æ–≤—ã–π —Ü–≤–µ—Ç–æ–∫, –æ–Ω–∞ —Å–º–æ—Ç—Ä–∏—Ç –Ω–∞ **3 –±–ª–∏–∂–∞–π—à–∏—Ö** —Ü–≤–µ—Ç–∫–∞ –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.
*   **Voting:** If 2 neighbors are 'Versicolor' and 1 is 'Virginica', the model predicts 'Versicolor' (Majority Vote).
*   **–ì–æ–ª–æ—Å–æ–≤–∞–Ω–∏–µ:** –ï—Å–ª–∏ 2 —Å–æ—Å–µ–¥–∞ ‚Äî 'Versicolor', –∞ 1 ‚Äî 'Virginica', –º–æ–¥–µ–ª—å –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç 'Versicolor' (–ì–æ–ª–æ—Å–æ–≤–∞–Ω–∏–µ –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–∞).

### 3. Train-Test Split / –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ –∏ —Ç–µ—Å—Ç
```python
train_test_split(X, y, test_size=0.25, stratify=y)
```
*   **üá¨üáß Logic:** We hide 25% of data to test the model later. `stratify=y` ensures that the proportion of flower types is the same in both training and test sets (e.g., 33% of each type).
*   **üá∑üá∫ –õ–æ–≥–∏–∫–∞:** –ú—ã –ø—Ä—è—á–µ–º 25% –¥–∞–Ω–Ω—ã—Ö, —á—Ç–æ–±—ã –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –º–æ–¥–µ–ª—å –ø–æ–∑–∂–µ. `stratify=y` –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç, —á—Ç–æ –ø—Ä–æ–ø–æ—Ä—Ü–∏—è –≤–∏–¥–æ–≤ —Ü–≤–µ—Ç–æ–≤ –±—É–¥–µ—Ç –æ–¥–∏–Ω–∞–∫–æ–≤–æ–π –∏ –≤ –æ–±—É—á–∞—é—â–µ–π, –∏ –≤ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∞—Ö (–Ω–∞–ø—Ä–∏–º–µ—Ä, –ø–æ 33% –∫–∞–∂–¥–æ–≥–æ –≤–∏–¥–∞).

---

## üìâ Weak Points & Improvements / –°–ª–∞–±—ã–µ –º–µ—Å—Ç–∞ –∏ —É–ª—É—á—à–µ–Ω–∏—è

### 1. Computational Cost / –í—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å
*   **üá¨üáß Weakness:** KNN is **lazy**. It doesn't learn a formula; it memorizes data. Prediction is slow on large datasets because it calculates distance to *every* point.
*   **üá∑üá∫ –°–ª–∞–±–æ—Å—Ç—å:** KNN ‚Äî **–ª–µ–Ω–∏–≤—ã–π** –∞–ª–≥–æ—Ä–∏—Ç–º. –û–Ω –Ω–µ —É—á–∏—Ç —Ñ–æ—Ä–º—É–ª—É, –∞ –∑–∞–ø–æ–º–∏–Ω–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ. –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –º–µ–¥–ª–µ–Ω–Ω–æ–µ –Ω–∞ –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö, —Ç–∞–∫ –∫–∞–∫ –æ–Ω —Å—á–∏—Ç–∞–µ—Ç —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –¥–æ *–∫–∞–∂–¥–æ–π* —Ç–æ—á–∫–∏.
*   **üöÄ Improvement:** Use **Decision Trees** or **Logistic Regression** (eager learners, fast prediction). / –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å **–î–µ—Ä–µ–≤—å—è —Ä–µ—à–µ–Ω–∏–π** –∏–ª–∏ **–õ–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫—É—é —Ä–µ–≥—Ä–µ—Å—Å–∏—é** (–∞–∫—Ç–∏–≤–Ω—ã–µ —É—á–µ–Ω–∏–∫–∏, –±—ã—Å—Ç—Ä–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ).

### 2. Feature Scaling / –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
*   **üá¨üáß Weakness:** KNN depends on distance. If one feature is large (e.g., 1000mm) and another small (e.g., 1cm), the large one dominates.
*   **üá∑üá∫ –°–ª–∞–±–æ—Å—Ç—å:** KNN –∑–∞–≤–∏—Å–∏—Ç –æ—Ç —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è. –ï—Å–ª–∏ –æ–¥–∏–Ω –ø—Ä–∏–∑–Ω–∞–∫ –±–æ–ª—å—à–æ–π (–Ω–∞–ø—Ä–∏–º–µ—Ä, 1000 –º–º), –∞ –¥—Ä—É–≥–æ–π –º–∞–ª–µ–Ω—å–∫–∏–π (–Ω–∞–ø—Ä–∏–º–µ—Ä, 1 —Å–º), –±–æ–ª—å—à–æ–π –±—É–¥–µ—Ç –¥–æ–º–∏–Ω–∏—Ä–æ–≤–∞—Ç—å.
*   **üöÄ Improvement:** Always use **StandardScaler** or **MinMaxScaler** before KNN. / –í—Å–µ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ **StandardScaler** –∏–ª–∏ **MinMaxScaler** –ø–µ—Ä–µ–¥ KNN.

---

## ‚ùì Professor Questions / –í–æ–ø—Ä–æ—Å—ã –ø—Ä–æ—Ñ–µ—Å—Å–æ—Ä–∞

### Q1: Why did you choose k=3?
### –í1: –ü–æ—á–µ–º—É –≤—ã –≤—ã–±—Ä–∞–ª–∏ k=3?
*   **üá¨üáß Answer:** `k=1` is too sensitive to noise (overfitting). Large `k` (e.g., 50) smooths out boundaries too much (underfitting). `k=3` or `k=5` is usually a good balance.
*   **üá∑üá∫ –û—Ç–≤–µ—Ç:** `k=1` —Å–ª–∏—à–∫–æ–º —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª–µ–Ω –∫ —à—É–º—É (–ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ). –ë–æ–ª—å—à–æ–µ `k` (–Ω–∞–ø—Ä–∏–º–µ—Ä, 50) —Å–ª–∏—à–∫–æ–º —Å–≥–ª–∞–∂–∏–≤–∞–µ—Ç –≥—Ä–∞–Ω–∏—Ü—ã (–Ω–µ–¥–æ–æ–±—É—á–µ–Ω–∏–µ). `k=3` –∏–ª–∏ `k=5` ‚Äî –æ–±—ã—á–Ω–æ —Ö–æ—Ä–æ—à–∏–π –±–∞–ª–∞–Ω—Å.

### Q2: What is the "Curse of Dimensionality"?
### –í2: –ß—Ç–æ —Ç–∞–∫–æ–µ "–ü—Ä–æ–∫–ª—è—Ç–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏"?
*   **üá¨üáß Answer:** In very high dimensions (many features), all points become far away from each other. KNN stops working well because "nearest" neighbors aren't actually close.
*   **üá∑üá∫ –û—Ç–≤–µ—Ç:** –í –æ—á–µ–Ω—å –≤—ã—Å–æ–∫–∏—Ö —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—è—Ö (–º–Ω–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤) –≤—Å–µ —Ç–æ—á–∫–∏ —Å—Ç–∞–Ω–æ–≤—è—Ç—Å—è –¥–∞–ª–µ–∫–∏ –¥—Ä—É–≥ –æ—Ç –¥—Ä—É–≥–∞. KNN –ø–µ—Ä–µ—Å—Ç–∞–µ—Ç —Ö–æ—Ä–æ—à–æ —Ä–∞–±–æ—Ç–∞—Ç—å, –ø–æ—Ç–æ–º—É —á—Ç–æ "–±–ª–∏–∂–∞–π—à–∏–µ" —Å–æ—Å–µ–¥–∏ –Ω–∞ —Å–∞–º–æ–º –¥–µ–ª–µ –Ω–µ –±–ª–∏–∑–∫–æ.

---

## üìê Math Intuition / –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∏–Ω—Ç—É–∏—Ü–∏—è

### Euclidean Distance / –ï–≤–∫–ª–∏–¥–æ–≤–æ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ
$$ d(p, q) = \sqrt{(q_1 - p_1)^2 + (q_2 - p_2)^2 + \dots + (q_n - p_n)^2} $$

*   **üá¨üáß EN:** It's the straight-line distance between two points in n-dimensional space. It's an extension of the Pythagorean theorem.
*   **üá∑üá∫ RU:** –≠—Ç–æ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –ø–æ –ø—Ä—è–º–æ–π –º–µ–∂–¥—É –¥–≤—É–º—è —Ç–æ—á–∫–∞–º–∏ –≤ n-–º–µ—Ä–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ. –≠—Ç–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Ç–µ–æ—Ä–µ–º—ã –ü–∏—Ñ–∞–≥–æ—Ä–∞.
