{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Assignment 6: Titanic Survival Prediction \ud83d\udea2\ud83e\uddca\n",
                "\n",
                "## \ud83d\udcda Learning Objectives\n",
                "- Perform **Exploratory Data Analysis (EDA)** and Visualization.\n",
                "- Implement **custom transformers** and feature engineering.\n",
                "- Train and compare multiple machine learning models (**Logistic Regression**, **Random Forest**, **SVM**)."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 1: Data Loading and EDA (20 marks)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Q1 (5 marks)\n",
                "Load the Titanic dataset using `seaborn.load_dataset('titanic')`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import seaborn as sns # Import libraries / \u0418\u043c\u043f\u043e\u0440\u0442 \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# Load dataset\n",
                "df = sns.load_dataset('titanic') # Load Titanic dataset / \u0417\u0430\u0433\u0440\u0443\u0437\u0438\u0442\u044c \u0434\u0430\u0442\u0430\u0441\u0435\u0442 \u0422\u0438\u0442\u0430\u043d\u0438\u043a\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Q2.1 (5 marks)\n",
                "Plot the distribution of passenger ages."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.figure(figsize=(8, 5))\n",
                "sns.histplot(df['age'].dropna(), kde=True, color='skyblue', bins=30) # Plot age distribution / \u041f\u043e\u0441\u0442\u0440\u043e\u0438\u0442\u044c \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0432\u043e\u0437\u0440\u0430\u0441\u0442\u0430\n",
                "plt.title('Distribution of Passenger Ages')\n",
                "plt.xlabel('Age')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Q2.2 (5 marks)\n",
                "Compare survival rates between male and female passengers using a bar plot."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.figure(figsize=(6, 4))\n",
                "sns.barplot(x='sex', y='survived', data=df, palette='pastel') # Plot survival rate / \u041f\u043e\u0441\u0442\u0440\u043e\u0438\u0442\u044c \u0433\u0440\u0430\u0444\u0438\u043a \u0432\u044b\u0436\u0438\u0432\u0430\u0435\u043c\u043e\u0441\u0442\u0438\n",
                "plt.title('Survival Rate by Gender')\n",
                "plt.ylabel('Survival Probability')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Q2.3 (5 marks)\n",
                "Generate a correlation heatmap for numerical features and display a scatter matrix."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.figure(figsize=(10, 8))\n",
                "sns.heatmap(df.corr(numeric_only=True), annot=True, cmap='coolwarm', fmt=\".2f\") # Plot correlation heatmap / \u041f\u043e\u0441\u0442\u0440\u043e\u0438\u0442\u044c \u0442\u0435\u043f\u043b\u043e\u0432\u0443\u044e \u043a\u0430\u0440\u0442\u0443 \u043a\u043e\u0440\u0440\u0435\u043b\u044f\u0446\u0438\u0438\n",
                "plt.title('Correlation Heatmap')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Q2.4 (5 marks)\n",
                "**Question:** Briefly describe your observations from the plots.\n",
                "\n",
                "**Answer:**\n",
                "- **Age**: The age distribution is slightly right-skewed, with a peak around 20-30 years and a smaller peak for infants.\n",
                "- **Gender**: Females had a significantly higher chance of survival compared to males (approx 74% vs 19%).\n",
                "- **Correlation**: `fare` and `survived` have a moderate positive correlation, while `pclass` and `survived` have a negative correlation (higher class number = lower survival)."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 2: Feature Engineering and Transformers (55 marks)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Q3 (5 marks)\n",
                "Explore missing data in the dataset."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(df.isnull().sum()) # Check for missing values / \u041f\u0440\u043e\u0432\u0435\u0440\u043a\u0430 \u043d\u0430 \u043f\u0440\u043e\u043f\u0443\u0449\u0435\u043d\u043d\u044b\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Q4 (5 marks)\n",
                "Select the following features: `pclass`, `sex`, `age`, `sibsp`, `parch`, `fare`, `embarked`, and the target variable `survived`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "features = ['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked'] # Define features / \u041e\u043f\u0440\u0435\u0434\u0435\u043b\u0438\u0442\u044c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438\n",
                "target = 'survived'\n",
                "\n",
                "X = df[features].copy()\n",
                "y = df[target].copy()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Q5 (10 marks)\n",
                "Handle missing values using `SimpleImputer`:\n",
                "- Use `median` strategy for numerical features.\n",
                "- Use `most_frequent` (mode) strategy for categorical features."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.impute import SimpleImputer # Import Imputer / \u0418\u043c\u043f\u043e\u0440\u0442 Imputer\n",
                "\n",
                "# Separate numerical and categorical columns\n",
                "num_cols = ['age', 'sibsp', 'parch', 'fare']\n",
                "cat_cols = ['pclass', 'sex', 'embarked'] # pclass is ordinal but often treated as cat or num. Let's treat as cat for encoding or num for median. Here we treat age/fare as main num.\n",
                "\n",
                "# Impute Numerical\n",
                "imputer_num = SimpleImputer(strategy='median') # Import Imputer / \u0418\u043c\u043f\u043e\u0440\u0442 Imputer\n",
                "X[num_cols] = imputer_num.fit_transform(X[num_cols])\n",
                "\n",
                "# Impute Categorical\n",
                "imputer_cat = SimpleImputer(strategy='most_frequent') # Import Imputer / \u0418\u043c\u043f\u043e\u0440\u0442 Imputer\n",
                "X[cat_cols] = imputer_cat.fit_transform(X[cat_cols])\n",
                "\n",
                "print(\"Missing values after imputation:\")\n",
                "print(X.isnull().sum())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Q6 (10 marks)\n",
                "Apply `OneHotEncoder` to categorical features and display the first 5 rows of the encoded data."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "X_encoded = pd.get_dummies(X, columns=cat_cols, drop_first=True) # One-Hot Encoding / One-Hot \u041a\u043e\u0434\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435\n",
                "X_encoded.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Q7 (15 marks)\n",
                "Create a custom transformer class named `FamilySizeAdder`.\n",
                "- It must accept a NumPy array with columns corresponding to `[age, sibsp, parch, fare]`.\n",
                "- It should return an array with an extra column `family_size`, calculated as `sibsp + parch + 1`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.base import BaseEstimator, TransformerMixin\n",
                "\n",
                "class FamilySizeAdder(BaseEstimator, TransformerMixin): # Custom Transformer class / \u041a\u043b\u0430\u0441\u0441 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u0441\u043a\u043e\u0433\u043e \u0442\u0440\u0430\u043d\u0441\u0444\u043e\u0440\u043c\u0435\u0440\u0430\n",
                "    def __init__(self):\n",
                "        pass\n",
                "    \n",
                "    def fit(self, X, y=None):\n",
                "        return self\n",
                "    \n",
                "    def transform(self, X): # Transformation logic / \u041b\u043e\u0433\u0438\u043a\u0430 \u0442\u0440\u0430\u043d\u0441\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u0438\n",
                "        # Assuming X is a numpy array with columns [age, sibsp, parch, fare]\n",
                "        # sibsp is at index 1, parch is at index 2\n",
                "        sibsp = X[:, 1]\n",
                "        parch = X[:, 2]\n",
                "        family_size = sibsp + parch + 1 # Calculate family size / \u0412\u044b\u0447\u0438\u0441\u043b\u0438\u0442\u044c \u0440\u0430\u0437\u043c\u0435\u0440 \u0441\u0435\u043c\u044c\u0438\n",
                "        return np.c_[X, family_size]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Q7.1\n",
                "Use `FamilySizeAdder` to add the `family_size` feature to your numerical data."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Extract numerical part as numpy array for the transformer\n",
                "X_num = X[num_cols].values\n",
                "\n",
                "attr_adder = FamilySizeAdder()\n",
                "X_num_extra = attr_adder.transform(X_num)\n",
                "\n",
                "print(\"Shape before:\", X_num.shape)\n",
                "print(\"Shape after:\", X_num_extra.shape)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Q8 (10 marks)\n",
                "Use `FunctionTransformer` to apply a log transformation to the `fare` column. Ensure the output is an array.\n",
                "*Hint: Add 1 to the fare before taking the log to handle zero values.*"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.preprocessing import FunctionTransformer # Import FunctionTransformer / \u0418\u043c\u043f\u043e\u0440\u0442 FunctionTransformer\n",
                "\n",
                "log_transformer = FunctionTransformer(np.log1p, validate=True) # Import FunctionTransformer / \u0418\u043c\u043f\u043e\u0440\u0442 FunctionTransformer\n",
                "\n",
                "# Apply to 'fare' (which is the last column in our X_num array: age, sibsp, parch, fare)\n",
                "fare_col = X_num[:, 3].reshape(-1, 1)\n",
                "fare_log = log_transformer.transform(fare_col)\n",
                "\n",
                "print(\"First 5 log fares:\\n\", fare_log[:5])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Q9 (10 marks)\n",
                "Combine the outputs of your custom transformer and the function transformer into a single array."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Replace the original fare column with log fare in the array that has family size\n",
                "# X_num_extra columns: age, sibsp, parch, fare, family_size\n",
                "X_final_num = X_num_extra.copy()\n",
                "X_final_num[:, 3] = fare_log.ravel()\n",
                "\n",
                "print(\"Final numerical array shape:\", X_final_num.shape)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 3: Model Training and Comparison (25 marks)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Q10 (5 marks)\n",
                "Split the data into training (80%) and testing (20%) sets using stratified sampling on the target variable."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Combine numerical and categorical features for final dataset\n",
                "# Note: For simplicity in this step-by-step flow, we'll just use the pandas encoded version \n",
                "# and add family_size manually to keep it aligned with previous steps.\n",
                "\n",
                "X_final = X_encoded.copy()\n",
                "X_final['family_size'] = X['sibsp'] + X['parch'] + 1\n",
                "X_final['fare'] = np.log1p(X_final['fare']) # Log transformation / \u041b\u043e\u0433\u0430\u0440\u0438\u0444\u043c\u0438\u0447\u0435\u0441\u043a\u043e\u0435 \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u043d\u0438\u0435\n",
                "\n",
                "from sklearn.model_selection import train_test_split # Split dataset / \u0420\u0430\u0437\u0434\u0435\u043b\u0438\u0442\u044c \u0434\u0430\u0442\u0430\u0441\u0435\u0442\n",
                "\n",
                "X_train, X_test, y_train, y_test = train_test_split(X_final, y, test_size=0.2, stratify=y, random_state=42) # Split dataset / \u0420\u0430\u0437\u0434\u0435\u043b\u0438\u0442\u044c \u0434\u0430\u0442\u0430\u0441\u0435\u0442\n",
                "\n",
                "print(\"Train shape:\", X_train.shape)\n",
                "print(\"Test shape:\", X_test.shape)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Q11 (20 marks)\n",
                "Train and evaluate the following models using **scaled features** (StandardScaler):\n",
                "1. Logistic Regression\n",
                "2. Random Forest Classifier (`n_estimators=150`)\n",
                "3. Support Vector Machine (SVM)\n",
                "\n",
                "Report the accuracy score for each on the test set."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.preprocessing import StandardScaler # Import Scaler / \u0418\u043c\u043f\u043e\u0440\u0442 Scaler\n",
                "from sklearn.linear_model import LogisticRegression # Initialize Logistic Regression / \u0418\u043d\u0438\u0446\u0438\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u041b\u043e\u0433\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0439 \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u0438\n",
                "from sklearn.ensemble import RandomForestClassifier # Initialize Random Forest / \u0418\u043d\u0438\u0446\u0438\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u0421\u043b\u0443\u0447\u0430\u0439\u043d\u043e\u0433\u043e \u043b\u0435\u0441\u0430\n",
                "from sklearn.svm import SVC # Initialize SVM / \u0418\u043d\u0438\u0446\u0438\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f SVM\n",
                "from sklearn.metrics import accuracy_score\n",
                "\n",
                "# Scale features\n",
                "scaler = StandardScaler() # Import Scaler / \u0418\u043c\u043f\u043e\u0440\u0442 Scaler\n",
                "X_train_scaled = scaler.fit_transform(X_train) # Scale training data / \u041c\u0430\u0441\u0448\u0442\u0430\u0431\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0438\u0435 \u0434\u0430\u043d\u043d\u044b\u0435\n",
                "X_test_scaled = scaler.transform(X_test)\n",
                "\n",
                "models = {\n",
                "    \"Logistic Regression\": LogisticRegression(random_state=42), # Initialize Logistic Regression / \u0418\u043d\u0438\u0446\u0438\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u041b\u043e\u0433\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0439 \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u0438\n",
                "    \"Random Forest\": RandomForestClassifier(n_estimators=150, random_state=42), # Initialize Random Forest / \u0418\u043d\u0438\u0446\u0438\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u0421\u043b\u0443\u0447\u0430\u0439\u043d\u043e\u0433\u043e \u043b\u0435\u0441\u0430\n",
                "    \"SVM\": SVC(random_state=42) # Initialize SVM / \u0418\u043d\u0438\u0446\u0438\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f SVM\n",
                "}\n",
                "\n",
                "print(\"--- Results with Scaling ---\")\n",
                "for name, model in models.items():\n",
                "    model.fit(X_train_scaled, y_train) # Train the model / \u041e\u0431\u0443\u0447\u0438\u0442\u044c \u043c\u043e\u0434\u0435\u043b\u044c\n",
                "    acc = model.score(X_test_scaled, y_test) # Calculate accuracy / \u0412\u044b\u0447\u0438\u0441\u043b\u0438\u0442\u044c \u0442\u043e\u0447\u043d\u043e\u0441\u0442\u044c\n",
                "    print(f\"{name}: {acc:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Q11.1\n",
                "Repeat the training and evaluation process (same models) but **without feature scaling**."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"--- Results WITHOUT Scaling ---\")\n",
                "for name, model in models.items():\n",
                "    # Note: SVM and LogReg might fail to converge or perform poorly without scaling\n",
                "    model.fit(X_train, y_train) # Train the model / \u041e\u0431\u0443\u0447\u0438\u0442\u044c \u043c\u043e\u0434\u0435\u043b\u044c\n",
                "    acc = model.score(X_test, y_test) # Calculate accuracy / \u0412\u044b\u0447\u0438\u0441\u043b\u0438\u0442\u044c \u0442\u043e\u0447\u043d\u043e\u0441\u0442\u044c\n",
                "    print(f\"{name}: {acc:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Q12 (5 marks)\n",
                "**Question:** Which model performs the best, and why? Discuss the effect of scaling.\n",
                "\n",
                "**Answer:**\n",
                "- **Best Model**: Typically, **Random Forest** performs very well on this dataset because it handles non-linear relationships and interactions (like age vs class) effectively.\n",
                "- **Effect of Scaling**: \n",
                "    - **SVM** and **Logistic Regression** are distance/gradient-based, so they perform **much better** with scaling. Without scaling, features with large ranges (like `fare`) dominate the objective function.\n",
                "    - **Random Forest** is tree-based and is generally **invariant to scaling**, so its performance remains similar regardless of scaling."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}