# Deep Dive: ML Models & Parameters / –ì–ª—É–±–æ–∫–æ–µ –ø–æ–≥—Ä—É–∂–µ–Ω–∏–µ: ML –º–æ–¥–µ–ª–∏ –∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã

## 1. The `max_iter` Parameter: How to Choose? / –ü–∞—Ä–∞–º–µ—Ç—Ä `max_iter`: –ö–∞–∫ –≤—ã–±—Ä–∞—Ç—å?

### üá¨üáß English
**Visual Concept**: Imagine a blindfolded hiker trying to find the lowest point in a valley (Minimum Error). `max_iter` is the maximum number of steps they are allowed to take.

```mermaid
graph TD
    A[Start at Random Point] --> B{Step Downhill?}
    B -- Yes --> C[Take Step]
    C --> D{Is it the Bottom?}
    D -- Yes --> E[Success! Converged]
    D -- No --> F{Steps > max_iter?}
    F -- No --> B
    F -- Yes --> G[Stop! ConvergenceWarning]
    style G fill:#ffcccc,stroke:#333
    style E fill:#ccffcc,stroke:#333
```

**Real-World Example**:
*   **Finance**: Optimizing portfolio weights where you need a precise solution within milliseconds.
*   **Robotics**: Inverse kinematics solvers where you need a solution before the robot moves to the next frame.

**How to determine the optimal value?**
It depends on the **complexity of the optimization surface**.
1.  **Scale Data first!** (StandardScaler). This smooths the terrain.
2.  **Increase `max_iter`**: `100` -> `1000` -> `5000`.

### üá∑üá∫ –†—É—Å—Å–∫–∏–π
**–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è**: –ü—Ä–µ–¥—Å—Ç–∞–≤—å—Ç–µ —Ç—É—Ä–∏—Å—Ç–∞ —Å –∑–∞–≤—è–∑–∞–Ω–Ω—ã–º–∏ –≥–ª–∞–∑–∞–º–∏, –∫–æ—Ç–æ—Ä—ã–π –ø—ã—Ç–∞–µ—Ç—Å—è –Ω–∞–π—Ç–∏ —Å–∞–º—É—é –Ω–∏–∑–∫—É—é —Ç–æ—á–∫—É –≤ –¥–æ–ª–∏–Ω–µ (–ú–∏–Ω–∏–º–∞–ª—å–Ω—É—é –æ—à–∏–±–∫—É). `max_iter` ‚Äî —ç—Ç–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤, –∫–æ—Ç–æ—Ä–æ–µ –µ–º—É —Ä–∞–∑—Ä–µ—à–µ–Ω–æ —Å–¥–µ–ª–∞—Ç—å.

**–ü—Ä–∏–º–µ—Ä –∏–∑ –∂–∏–∑–Ω–∏**:
*   **–§–∏–Ω–∞–Ω—Å—ã**: –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–æ—Ä—Ç—Ñ–µ–ª—è –∞–∫—Ü–∏–π, –≥–¥–µ –Ω—É–∂–Ω–æ —Ç–æ—á–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ –∑–∞ –º–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥—ã.
*   **–†–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∞**: –†–∞—Å—á–µ—Ç –¥–≤–∏–∂–µ–Ω–∏—è —Ä–æ–±–æ—Ç–∞, –≥–¥–µ —Ä–µ—à–µ–Ω–∏–µ –Ω—É–∂–Ω–æ –Ω–∞–π—Ç–∏ –¥–æ —Å–ª–µ–¥—É—é—â–µ–≥–æ –∫–∞–¥—Ä–∞.

---

## 2. Logistic Regression (–õ–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è –†–µ–≥—Ä–µ—Å—Å–∏—è)

### üá¨üáß English
**Visual Concept**: Finding a linear boundary.
```mermaid
graph LR
    Input[Inputs: Age, Fare, Sex] --> Sum(Weighted Sum z)
    Sum --> Sigmoid{Sigmoid Function}
    Sigmoid --> Prob[Probability 0.75]
    Prob --> Class{Threshold > 0.5?}
    Class -- Yes --> Survived(Survived)
    Class -- No --> Died(Died)
```

**Common Use Cases**:
*   **Credit Scoring**: Predicting if a user will default on a loan (Yes/No).
*   **Medical Diagnosis**: Healthy vs. Sick based on blood test results.
*   **Spam Detection**: Is this email Spam or Not Spam?

**Trend**: 
Still the **#1 industry standard** for baselines. It provides **interpretable coefficients** (e.g., "being male reduces odds by 50%"), which is crucial in banking and medicine where "black box" models are illegal.

### üá∑üá∫ –†—É—Å—Å–∫–∏–π
**–ß–∞—Å—Ç—ã–µ –ø—Ä–∏–º–µ—Ä—ã**:
*   **–ö—Ä–µ–¥–∏—Ç–Ω—ã–π —Å–∫–æ—Ä–∏–Ω–≥**: –í–µ—Ä–Ω–µ—Ç –ª–∏ –∫–ª–∏–µ–Ω—Ç –∫—Ä–µ–¥–∏—Ç (–î–∞/–ù–µ—Ç).
*   **–ú–µ–¥–∏—Ü–∏–Ω–∞**: –ó–¥–æ—Ä–æ–≤/–ë–æ–ª–µ–Ω –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–æ–≤.
*   **–°–ø–∞–º-—Ñ–∏–ª—å—Ç—Ä—ã**: –°–ø–∞–º –∏–ª–∏ –Ω–µ —Å–ø–∞–º.

**–¢—Ä–µ–Ω–¥**:
–û—Å—Ç–∞–µ—Ç—Å—è **–∑–æ–ª–æ—Ç—ã–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–º** –∫–∞–∫ –±–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å. –û–Ω–∞ –¥–∞–µ—Ç **–∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º—ã–µ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã** (–Ω–∞–ø—Ä–∏–º–µ—Ä, "–º—É–∂—Å–∫–æ–π –ø–æ–ª —Å–Ω–∏–∂–∞–µ—Ç —à–∞–Ω—Å—ã –Ω–∞ 50%"), —á—Ç–æ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–æ –≤ –±–∞–Ω–∫–∞—Ö –∏ –º–µ–¥–∏—Ü–∏–Ω–µ, –≥–¥–µ "—á–µ—Ä–Ω—ã–µ —è—â–∏–∫–∏" –∑–∞–ø—Ä–µ—â–µ–Ω—ã.

---

## 3. Random Forest (–°–ª—É—á–∞–π–Ω—ã–π –õ–µ—Å)

### üá¨üáß English
**Visual Concept**: Wisdom of the Crowds.
```mermaid
graph TD
    Data[Dataset] --> B1[Bootstrap Sample 1]
    Data --> B2[Bootstrap Sample 2]
    Data --> B3[Bootstrap Sample ...]
    B1 --> T1[Tree 1: 'Survived']
    B2 --> T2[Tree 2: 'Died']
    B3 --> T3[Tree 3: 'Survived']
    T1 & T2 & T3 --> Vote{Majority Vote}
    Vote --> Final[Final Prediction: 'Survived']
```

**Common Use Cases**:
*   **E-commerce Recommendation**: Predicting if a user will buy an item based on history.
*   **Fraud Detection**: Detecting anomaly transactions in real-time.
*   **Kinect/Gaming**: Real-time body pose estimation (used in Xbox Kinect original algorithms).

**Trend**:
It is the **"King of Tabular Data"**. For structured data (Excel-like tables), Kaggle competitions show that Gradient Boosting (XGBoost/CatBoost) and Random Forests still often outperform deep neural networks. They are "robust out of the box" and require less tuning.

### üá∑üá∫ –†—É—Å—Å–∫–∏–π
**–ß–∞—Å—Ç—ã–µ –ø—Ä–∏–º–µ—Ä—ã**:
*   **–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏**: –ö—É–ø–∏—Ç –ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Ç–æ–≤–∞—Ä.
*   **–ê–Ω—Ç–∏—Ñ—Ä–æ–¥**: –ü–æ–∏—Å–∫ –º–æ—à–µ–Ω–Ω–∏—á–µ—Å–∫–∏—Ö —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π.
*   **–ì–µ–π–º–¥–µ–≤**: –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –ø–æ–∑—ã —Ç–µ–ª–∞ (–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–æ—Å—å –≤ Xbox Kinect).

**–¢—Ä–µ–Ω–¥**:
–≠—Ç–æ **"–ö–æ—Ä–æ–ª—å —Ç–∞–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"**. –î–ª—è –æ–±—ã—á–Ω—ã—Ö —Ç–∞–±–ª–∏—Ü (–∫–∞–∫ –≤ Excel) Random Forest –∏ –ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π –ë—É—Å—Ç–∏–Ω–≥ (XGBoost) –≤—Å–µ –µ—â–µ —á–∞—Å—Ç–æ –ø–æ–±–µ–∂–¥–∞—é—Ç –Ω–µ–π—Ä–æ—Å–µ—Ç–∏. –û–Ω–∏ —Ä–∞–±–æ—Ç–∞—é—Ç "–∏–∑ –∫–æ—Ä–æ–±–∫–∏" –∏ —Ç—Ä–µ–±—É—é—Ç –º–µ–Ω—å—à–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏.

---

## 4. Support Vector Machine / SVM (–ú–µ—Ç–æ–¥ –û–ø–æ—Ä–Ω—ã—Ö –í–µ–∫—Ç–æ—Ä–æ–≤)

### üá¨üáß English
**Visual Concept**: Maximizing the "Street" width.
```mermaid
graph TD
    A((Class A)) --- Margin1
    B((Class B)) --- Margin2
    Margin1 --- Separator[Hyperplane / Decision Boundary]
    Margin2 --- Separator
    style Separator stroke-width:4px,fill:#fff,stroke:#333
```
*Ideally, SVM creates the widest possible gap between the nearest dots of Class A and Class B.*

**Common Use Cases**:
*   **Text Classification**: Categorizing news articles into topics (Politics, Sports) - works well with high-dimensional sparse data.
*   **Bioinformatics**: Classifying gene expression data.
*   **Image Recognition (Legacy)**: Used for face detection before Deep Learning took over.

**Trend**:
**Declining popularity** for large datasets because it is slow ($O(n^2)$ complexity). However, it is still **excellent for small, complex datasets** with high dimensionality where neural networks would overfit.

### üá∑üá∫ –†—É—Å—Å–∫–∏–π
**–ß–∞—Å—Ç—ã–µ –ø—Ä–∏–º–µ—Ä—ã**:
*   **–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞**: –ù–æ–≤–æ—Å—Ç–∏ –ø–æ —Ä—É–±—Ä–∏–∫–∞–º (–ü–æ–ª–∏—Ç–∏–∫–∞, –°–ø–æ—Ä—Ç).
*   **–ë–∏–æ–∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–∫–∞**: –ê–Ω–∞–ª–∏–∑ –≥–µ–Ω–æ–≤.
*   **–†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –æ–±—Ä–∞–∑–æ–≤**: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–ª—Å—è –¥–ª—è –ª–∏—Ü –¥–æ —ç—Ä—ã –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è.

**–¢—Ä–µ–Ω–¥**:
**–ü–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—å –ø–∞–¥–∞–µ—Ç** –Ω–∞ –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö, —Ç–∞–∫ –∫–∞–∫ –æ–Ω –º–µ–¥–ª–µ–Ω–Ω—ã–π. –ù–æ –æ–Ω –≤—Å–µ –µ—â–µ **–Ω–µ–∑–∞–º–µ–Ω–∏–º –¥–ª—è –º–∞–ª–µ–Ω—å–∫–∏—Ö, —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á**, –≥–¥–µ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ –ø—Ä–æ—Å—Ç–æ –∑–∞–ø–æ–º–Ω—è—Ç –¥–∞–Ω–Ω—ã–µ (–ø–µ—Ä–µ–æ–±—É—á–∞—Ç—Å—è).
