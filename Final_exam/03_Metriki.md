# Модуль 3: Метрики качества моделей

## 3.1 Accuracy vs AUC-ROC

### 1) Интуиция и суть

**Простыми словами (ELI5):**  
Представ, что ты детектор пожара в здании:
- **Accuracy** (точность): "сколько раз я был прав?" — но если пожары редки (1 раз в год), можно никогда не сработать и иметь 99.9% accuracy!
- **AUC-ROC**: "насколько хорошо я отличаю пожар от его отсутствия при **любом** пороге чувствительности?" — более честная метрика при редких событиях.

**Академически:**  
**Accuracy** — доля правильных предсказаний. **AUC-ROC** — площадь под кривой (ROC curve), показывающая способность модели ранжировать объекты: отделять положительный класс от отрицательного независимо от порога классификации.

**Физический смысл:**  
AUC-ROC — это **вероятность**, что случайный положительный пример будет иметь более высокий score (предсказанную вероятность), чем случайный отрицательный.

### 2) Зачем и когда (Business Value)

**Какую проблему решает:**

**Accuracy:**
- Простая, интуитивная метрика
- Подходит для **сбалансированных** задач

**AUC-ROC:**
- Устойчива к **дисбалансу классов**
- Оценивает модель **на всех возможных порогах** (threshold-independent)
- Показывает **качество ранжирования**

**Где используется:**

**Accuracy:**
- Сбалансированные задачи: распознавание рукописных цифр (MNIST)

**AUC-ROC (индустриальный стандарт при дисбалансе):**
- **Медицина**: диагностика редких заболеваний (1% больных)
- **Fraud detection**: мошенничество (0.1% транзакций)
- **Рекомендательные системы**: CTR prediction (кликают 3% пользователей)
- **Кредитный скоринг**: дефолты (5% заемщиков)

**Когда применять нельзя:**

**Accuracy:**
- ❌ Дисбаланс классов (accuracy = 99% при accuracy всегда предсказывая 0 бесполезна!)

**AUC-ROC:**
- ❌ Мультиклассовая классификация напрямую (используйте One-vs-Rest AUC)
- ❌ Когда важен конкретный порог (используйте Precision/Recall при фиксированном threshold)

### 3) Математическое ядро

**Accuracy:**
```
Accuracy = (TP + TN) / (TP + TN + FP + FN)
```

где:
- TP (True Positive): правильно предсказанные положительные
- TN (True Negative): правильно предсказанные отрицательные
- FP (False Positive): ошибочно предсказанные как положительные (Type I error)
- FN (False Negative): ошибочно предсказанные как отрицательные (Type II error)

**Confusion Matrix:**
```
                 Predicted
                 0       1
Actual  0      TN      FP
        1      FN      TP
```

**ROC Curve (Receiver Operating Characteristic):**

Для разных порогов threshold строим:
- **TPR (True Positive Rate, Recall, Sensitivity):**
  ```
  TPR = TP / (TP + FN) = TP / P
  ```
  Доля положительных, которые правильно предсказаны.

- **FPR (False Positive Rate):**
  ```
  FPR = FP / (FP + TN) = FP / N
  ```
  Доля отрицательных, ошибочно предсказанных как положительные.

**ROC curve**: график (FPR, TPR) при варьировании threshold.

**AUC-ROC (Area Under the ROC Curve):**
```
AUC = ∫₀¹ TPR(FPR) d(FPR)
```

**Интерпретация AUC:**
- AUC = 1.0: идеальная модель (всех разделяет идеально)
- AUC = 0.5: случайное угадывание (бесполезная модель)
- AUC < 0.5: модель предсказывает наоборот (инвертируйте!)

**Вероятностная интерпретация:**
```
AUC = P(score(x_pos) > score(x_neg))
```
где x_pos — случайный положительный пример, x_neg — случайный отрицательный.

### 4) Middle-level нюансы

**Проблема 1: Дисбаланс классов**

**Пример:**
```
Fraud detection: 99.9% нормальных транзакций, 0.1% мошенничества.

Baseline (всегда предсказывает "нормальная"):
  Accuracy = 99.9%  ← выглядит отлично!
  AUC-ROC = 0.5     ← модель бесполезна!
```

**Почему Accuracy вводит в заблуждение:**  
Модель игнорирует меньшинство (мошенничество) и все равно получает высокую accuracy.

**Решение:**
- Используйте **AUC-ROC** или **Precision-Recall** (см. ниже) при дисбалансе

**Проблема 2: Выбор порога (threshold)**

AUC-ROC оценивает модель на **всех порогах**, но в продакшене нужен один конкретный!

**Стратегии выбора:**

1. **Максимизация F1-score** (баланс Precision и Recall)
2. **Youden's Index:** J = TPR - FPR (находим max J)
3. **Бизнес-метрика:** например, минимизируем cost = cost_FP · FP + cost_FN · FN

**Пример (медицина):**
```
FN (пропустили больного) — критично → выбираем порог для высокого Recall
FP (ложная тревога) — терпимо
```

**Проблема 3: Когда AUC может вводить в заблуждение**

**Сценарий:**  
Две модели с AUC = 0.85, но:
- Модель A: отлично при малых FPR (важно для fraud!)
- Модель B: отлично при высоких FPR (менее полезно)

**ROC curve может выглядеть по-разному при одинаковом AUC!**

**Решение:**
- Смотрите на **форму ROC curve**, не только AUC
- Используйте **Partial AUC** (AUC в интересующем диапазоне FPR)

### 5) Сравнение Accuracy vs AUC-ROC

| Метрика | Дисбаланс классов | Threshold | Интерпретация | Когда использовать |
|---------|------------------|----------|---------------|-------------------|
| **Accuracy** | ❌ Чувствительна | ⚠️ Зависит от порога | ✅ Простая | Сбалансированные классы |
| **AUC-ROC** | ✅ Устойчива | ✅ Не зависит от порога | ⚠️ Сложнее объяснить бизнесу | Дисбаланс, ранжирование, сравнение моделей |

---

## 3.2 Precision/Recall Trade-off

### 1) Интуиция и суть

**Простыми словами (ELI5):**  
Ты ловишь рыбу сетью:
- **Recall (полнота)**: сколько рыбы из всей рыбы в реке ты поймал? "Насколько хорошо я нашел всё, что нужно?"
- **Precision (точность)**: сколько в твоей сети именно рыбы, а не мусора? "Насколько я уверен в своих находках?"

**Trade-off:**  
Можно закинуть огромную сеть → высокий Recall (поймаешь всю рыбу!), но низкий Precision (и много мусора).  
Или маленькую сеть → высокий Precision (только рыба), но низкий Recall (мало рыбы).

**Академически:**  
**Precision** — доля истинно положительных среди предсказанных положительных.  
**Recall** — доля истинно положительных среди всех фактически положительных.  
Trade-off: улучшение одного часто ухудшает другое.

### 2) Зачем и когда (Business Value)

**Какую проблему решает:**
- Балансировка между "найти все" (Recall) и "не ошибаться" (Precision)
- Выбор стратегии в зависимости от **цены ошибок**

**Где это критически важно:**

**High Recall critical (не пропустить положительные):**
- **Медицина**: диагностика рака (лучше перестраховаться, чем пропустить больного)
- **Fraud detection**: отловить все мошенничества (даже если будут ложные срабатывания)
- **Поисковые системы**: показать все релевантные документы

**High Precision critical (не ошибаться):**
- **Spam filter**: не отправлять важные письма в спам
- **Рекомендательные системы**: показывать только то, что точно понравится
- **Юридические системы**: предъявлять обвинения только при уверенности

**Balanced (F1-score):**
- Большинство классификационных задач без явного приоритета

### 3) Математическое ядро

**Precision:**
```
Precision = TP / (TP + FP)
```
Из всех, кого модель назвала "положительными", сколько действительно положительные?

**Recall (Sensitivity, TPR):**
```
Recall = TP / (TP + FN) = TP / P
```
Из всех фактически положительных, сколько модель нашла?

**F1-Score (гармоническое среднее):**
```
F1 = 2 · (Precision · Recall) / (Precision + Recall)
   = 2TP / (2TP + FP + FN)
```

**Зачем гармоническое, а не арифметическое?**  
Гармоническое среднее жестче штрафует за дисбаланс:
```
Если Precision = 1.0, Recall = 0.1:
  Arithmetic mean = 0.55  (выглядит неплохо!)
  F1 (harmonic)   = 0.18  (честнее показывает проблему)
```

**F_β-Score (обобщение):**
```
F_β = (1 + β²) · (Precision · Recall) / (β² · Precision + Recall)
```

- **β = 1**: F1 (баланс)
- **β > 1** (например, β = 2): Recall важнее
- **β < 1** (например, β = 0.5): Precision важнее

**Precision-Recall Curve:**

График (Recall, Precision) при варьировании threshold.

**AUC-PR (Area Under Precision-Recall Curve):**
```
AUC-PR = ∫₀¹ Precision(Recall) d(Recall)
```

**Baseline для AUC-PR:**  
Случайный классификатор: AUC-PR = P / (P + N) (доля положительных).

### 4) Middle-level нюансы

**Проблема 1: Precision-Recall Trade-off**

**Снижение порога (threshold ↓):**
- Больше объектов классифицируются как "положительные"
- **Recall ↑** (находим больше положительных)
- **Precision ↓** (больше ложных срабатываний)

**Повышение порога (threshold ↑):**
- Меньше объектов классифицируются как "положительные"
- **Recall ↓** (пропускаем положительные)
- **Precision ↑** (меньше ложных срабатываний, более уверенные предсказания)

**Графически:**
```
threshold = 0.3 → y_pred = много 1 → High Recall, Low Precision
threshold = 0.7 → y_pred = мало 1  → Low Recall, High Precision
```

**Проблема 2: Метрики при дисбалансе классов**

**Дисбаланс 99:1 (99% класс 0, 1% класс 1).**

**Бесполезная модель (всегда предсказывает 0):**
```
Accuracy  = 99%  ← выглядит отлично!
Precision = undefined (TP + FP = 0, делим на 0)
Recall    = 0%   ← честно показывает проблему!
```

**Хорошая модель:**
```
Precision = 50% (половина "тревог" истинные)
Recall    = 80% (находим 80% редких случаев)
F1        = 0.62
```

**AUC-ROC vs AUC-PR при дисбалансе:**
- **AUC-ROC**: может быть завышенной (большое число TN "раздувает" метрику)
- **AUC-PR**: более честная при дисбалансе (фокус на положительном классе)

**Проблема 3: Multi-class Precision/Recall**

**Macro-averaging:**
```
Precision_macro = (1/K) Σ Precision_k
```
Считаем метрику для каждого класса, усредняем. Все классы **равнозначны**.

**Micro-averaging:**
```
Precision_micro = Σ TP_k / Σ (TP_k + FP_k)
```
Суммируем TP, FP по всем классам, затем считаем метрику. Большие классы имеют **больший вес**.

**Weighted-averaging:**
```
Precision_weighted = Σ (n_k / n) · Precision_k
```
Взвешиваем по размеру класса.

**Когда что использовать:**
- **Macro**: классы равноважны (редкие классы важны)
- **Micro**: общее качество (большие классы важнее)
- **Weighted**: баланс между macro и micro

### 5) Сравнение метрик

| Метрика | Фокус | Дисбаланс | Threshold | Интерпретация |
|---------|-------|-----------|----------|---------------|
| **Accuracy** | Общая правильность | ❌ | Зависит | ✅ Простая |
| **Precision** | "Не ошибаться" | ✅ | Зависит | ✅ Интуитивная |
| **Recall** | "Найти все" | ✅ | Зависит | ✅ Интуитивная |
| **F1** | Баланс P/R | ✅ | Зависит | ⚠️ Среднее |
| **AUC-ROC** | Ранжирование | ⚠️ Может завышаться | ✅ Не зависит | ⚠️ Сложнее |
| **AUC-PR** | Ранжирование (при дисбалансе) | ✅ Честная | ✅ Не зависит | ⚠️ Сложнее |

**Рекомендации:**
- **Сбалансированные классы**: Accuracy, F1
- **Дисбаланс + нужен threshold**: Precision/Recall на конкретном пороге, F1
- **Дисбаланс + сравнение моделей**: AUC-PR
- **Важно не пропустить положительные**: оптимизируйте Recall
- **Важно не ошибаться**: оптимизируйте Precision

---

## 3.3 LogLoss (Binary Cross-Entropy)

### 1) Интуиция и суть

**Простыми словами (ELI5):**  
Ты предсказываешь погоду числом от 0 (точно не будет дождя) до 1 (точно будет).  
- Если сказал "вероятность дождя = 0.9" и дождь был → **малый штраф** (почти угадал!)
- Если сказал "вероятность = 0.1" и дождь был → **огромный штраф** (почти уверен, но ошибся!)

LogLoss сильнее штрафует за **уверенные ошибки**.

**Академически:**  
LogLoss — это функция потерь, измеряющая качество **вероятностных предсказаний**. Она асимптотически штрафует за предсказания вероятности 0 или 1, когда истинный класс противоположный.

**Физический смысл:**  
LogLoss = **средняя отрицательная log-likelihood**. Минимизация LogLoss эквивалентна **максимизации правдоподобия** (Maximum Likelihood Estimation).

### 2) Зачем и когда (Business Value)

**Какую проблему решает:**
- Оценка качества **вероятностных предсказаний** (не просто 0/1, а [0, 1])
- Штрафует за **излишнюю уверенность** в ошибочных предсказаниях
- Функция потерь для **обучения** логистической регрессии, нейросетей

**Где используется:**
- **Kaggle competitions**: часто используют LogLoss для бинарной классификации
- **Обучение моделей**: LogReg, Neural Networks (binary classification)
- **Калибровка вероятностей**: оценка насколько модель "честно" выдает вероятности

**Когда применять:**
- ✅ Нужны калиброванные вероятности
- ✅ Важна "уверенность" модели
- ❌ Важен только порядок (ранжирование) → AUC-ROC достаточно
- ❌ Нужны жесткие предсказания 0/1 → Accuracy, F1

### 3) Математическое ядро

**Binary LogLoss:**
```
LogLoss = -(1/n) Σ [y_i log(p_i) + (1 - y_i) log(1 - p_i)]
```

где:
- y_i ∈ {0, 1} — истинная метка
- p_i ∈ [0, 1] — предсказанная вероятность класса 1

**Разбор по случаям:**

**Случай 1: y = 1 (истинный класс положительный)**
```
Loss = -log(p)
```
- p → 1 (правильно предсказали): loss → 0 ✅
- p → 0 (уверенно ошиблись): loss → ∞ ❌❌❌

**Случай 2: y = 0 (истинный класс отрицательный)**
```
Loss = -log(1 - p)
```
- p → 0 (правильно): loss → 0 ✅
- p → 1 (уверенно ошиблись): loss → ∞ ❌❌❌

**Multi-class LogLoss (Categorical Cross-Entropy):**
```
LogLoss = -(1/n) Σ Σ y_ij log(p_ij)
```
где:
- y_ij = 1, если объект i принадлежит классу j
- p_ij — предсказанная вероятность класса j для объекта i

**Связь с вероятностями:**

Минимизация LogLoss ⟺ Максимизация Likelihood:
```
min LogLoss ⟺ max Π p_i^(y_i) · (1 - p_i)^(1 - y_i)
```

### 4) Middle-level нюансы

**Проблема 1: LogLoss vs Accuracy**

**Пример:**

| Предсказание | Истина | Accuracy (p > 0.5?) | LogLoss |
|-------------|--------|-------------------|---------|
| p = 0.51 | y = 1 | ✅ Правильно (1) | -log(0.51) = 0.67 |
| p = 0.99 | y = 1 | ✅ Правильно (1) | -log(0.99) = 0.01 ✅✅ |

**Вывод:**  
Accuracy не различает "уверенность", LogLoss поощряет высокую уверенность в правильных предсказаниях.

**Проблема 2: Излишняя уверенность (Overconfidence)**

**Плохо калиброванная модель:**
```
Модель предсказывает p = 0.95 для многих объектов,
но на практике только 70% из них — класс 1.
```

**Последствия:**
- **Высокий LogLoss** (штраф за уверенные ошибки)
- Модель "врет" о вероятностях

**Решения:**
1. **Calibration:**  
   - Platt Scaling (логистическая регрессия на выходе модели)
   - Isotonic Regression
   - Temperature Scaling (для нейросетей)

2. **Regularization** (L1/L2) снижает overconfidence

**Проблема 3: LogLoss чувствителен к дисбалансу**

При сильном дисбалансе (например, 1% класс 1) модель может минимизировать LogLoss, просто предсказывая p ≈ 0.01 для всех.

**Решение:**
- **Class weights**: взвешивание loss для разных классов
  ```
  LogLoss_weighted = -Σ w_i [y_i log(p_i) + (1 - y_i) log(1 - p_i)]
  ```

**Проблема 4: Интерпретация значения LogLoss**

**Абсолютные значения:**
- LogLoss = 0: идеальные предсказания
- LogLoss < 0.5: обычно хорошая модель
- LogLoss > 1.0: плохая модель (хуже, чем предсказание константы 0.5)

**Сравнительно:**  
LogLoss лучше для **сравнения моделей**, чем для абсолютной оценки.

### 5) Сравнение метрик потерь

| Метрика | Тип предсказаний | Чувствительность к уверенности | Дифференцируемость | Использование |
|---------|-----------------|-------------------------------|-------------------|---------------|
| **LogLoss** | Вероятности [0,1] | ✅ Сильно штрафует уверенные ошибки | ✅ Да | Обучение LogReg, NN |
| **Hinge Loss (SVM)** | Расстояние до границы | ⚠️ Штрафует только ошибки в margin | ⚠️ Не везде (кусочно-линейная) | Обучение SVM |
| **0-1 Loss** | Жесткие {0, 1} | ❌ Нет | ❌ Нет | Теоретическая (не оптимизируется) |
| **MSE** | Непрерывные | ⚠️ Квадратичный штраф | ✅ Да | Регрессия |

---

## 3.4 R² (Coefficient of Determination) vs MSE

### 1) Интуиция и суть

**Простыми словами (ELI5):**  
Ты предсказываешь рост человека.  
- **MSE** отвечает на вопрос: "на сколько сантиметров я ошибаюсь в среднем (в квадрате)?"
- **R²** отвечает: "насколько моя модель лучше, чем тупое предсказание среднего роста для всех?"

**Академически:**  
**MSE** — средний квадрат ошибки, абсолютная мера точности.  
**R²** — доля дисперсии целевой переменной, объясненной моделью, относительная мера качества (от -∞ до 1).

**Физический смысл:**  
R² = 1 - (необъясненная дисперсия) / (полная дисперсия).

### 2) Зачем и когда (Business Value)

**MSE:**
- **Абсолютная** оценка ошибки в единицах целевой переменной²
- Функция потерь для обучения (OLS, Ridge, Lasso)

**R²:**
- **Относительная** оценка: насколько модель лучше baseline (предсказание среднего)
- Интерпретация: "% объясненной дисперсии"
- Сравнение моделей на разных данных

**Где используется:**
- **Регрессия**: обе метрики стандартны
- **Эконометрика**: R² часто в отчетах ("модель объясняет 75% дисперсии")
- **Финансы**: прогноз доходности (MSE в единицах %, R² для интерпретации)

**Когда применять:**
- **MSE**: когда важна абсолютная величина ошибки, обучение моделей
- **R²**: когда нужна интерпретация, сравнение моделей, объяснение "качества"

### 3) Математическое ядро

**MSE (Mean Squared Error):**
```
MSE = (1/n) Σ (y_i - ŷ_i)²
```

**RMSE (Root MSE):**
```
RMSE = √MSE
```
Преимущество: в тех же единицах, что и y.

**MAE (Mean Absolute Error):**
```
MAE = (1/n) Σ |y_i - ŷ_i|
```
Менее чувствителен к выбросам.

**R² (Coefficient of Determination):**
```
R² = 1 - SS_res / SS_tot
```

где:
- **SS_res (Residual Sum of Squares):**
  ```
  SS_res = Σ (y_i - ŷ_i)²
  ```
  Необъясненная дисперсия (ошибки модели).

- **SS_tot (Total Sum of Squares):**
  ```
  SS_tot = Σ (y_i - ȳ)²
  ```
  Полная дисперсия (если предсказываем среднее).

**Альтернативная форма:**
```
R² = 1 - MSE / Var(y)
```

**Adjusted R² (для множественной регрессии):**
```
R²_adj = 1 - (1 - R²) · (n - 1) / (n - p - 1)
```
где p — число признаков. Штрафует за добавление бесполезных признаков.

### 4) Middle-level нюансы

**Проблема 1: Интерпретация R²**

**R² = 0.85:**  
Модель объясняет 85% дисперсии целевой переменной.

**R² = 0:**  
Модель не лучше, чем предсказание среднего.

**R² < 0:**  
Модель **хуже**, чем предсказание среднего! (переобучение на train, плохо на test)

**Пример R² < 0:**
```
Модель переобучилась на train: R²_train = 0.95
На test: предсказания хаотичны, хуже среднего: R²_test = -0.3
```

**Проблема 2: R² vs Adjusted R²**

**Проблема обычного R²:**  
Всегда растет при добавлении признаков (даже случайных!)

**Пример:**
```
Модель с 5 признаками: R² = 0.80
Добавили 10 случайных признаков: R² = 0.83  ← выглядит лучше, но это overfitting!
```

**Adjusted R²:**  
Штрафует за лишние признаки. Если новый признак не улучшает модель статистически значимо, R²_adj уменьшится.

**Когда использовать:**
- **R²**: простая регрессия, фиксированный набор признаков
- **R²_adj**: множественная регрессия, сравнение моделей с разным числом признаков

**Проблема 3: MSE vs MAE**

**MSE сильнее штрафует выбросы (квадрат!):**

| Ошибка | MSE contribution | MAE contribution |
|--------|-----------------|-----------------|
| 1 | 1 | 1 |
| 2 | 4 | 2 |
| 10 | 100 ❗ | 10 |

**Когда что использовать:**
- **MSE (RMSE)**: стандарт, выбросы важны (хотим их минимизировать сильнее)
- **MAE**: выбросы — шум (робастнее к ним)
- **Huber Loss**: комбинация (квадратичная для малых ошибок, линейная для больших)

**Проблема 4: Метрики для сравнения моделей на разных данных**

**Проблема MSE:**  
MSE зависит от масштаба y.

**Пример:**
```
Модель A (цены квартир, млн $): MSE = 0.5
Модель B (цены квартир, руб.): MSE = 5,000,000
```

Нельзя сравнить напрямую!

**Решение:**
- **R²**: не зависит от масштаба (можно сравнивать)
- **MAPE (Mean Absolute Percentage Error):**
  ```
  MAPE = (1/n) Σ |y_i - ŷ_i| / |y_i|
  ```
  Относительная ошибка в %.

### 5) Сравнение метрик регрессии

| Метрика | Единицы | Чувствительность к выбросам | Интерпретация | Когда использовать |
|---------|---------|----------------------------|---------------|-------------------|
| **MSE** | y² | ❗ Высокая (квадрат) | ⚠️ Сложно (квадраты) | Обучение, выбросы критичны |
| **RMSE** | y | ❗ Высокая | ✅ Простая (те же единицы) | Отчеты, интерпретация |
| **MAE** | y | ✅ Низкая | ✅ Простая | Выбросы — шум |
| **R²** | Безразмерная [−∞, 1] | ⚠️ Средняя | ✅ "% объясненной дисперсии" | Сравнение моделей, интерпретация |
| **MAPE** | % | ⚠️ Зависит от y | ✅ Простая (%) | Сравнение на разных масштабах, бизнес-отчеты |

**Рекомендации:**
- **Обучение**: MSE
- **Отчеты, интерпретация**: RMSE, R²
- **Выбросы — проблема**: MAE, Huber Loss
- **Сравнение на разных данных**: R², MAPE

---

## Резюме модуля

Метрики — это **линза**, через которую мы оцениваем модели:
- **Accuracy vs AUC-ROC**: дисбаланс классов, порог vs ранжирование
- **Precision/Recall**: trade-off между "не ошибаться" и "найти все"
- **LogLoss**: оценка вероятностей, штраф за уверенные ошибки
- **R² vs MSE**: относительная vs абсолютная оценка для регрессии

Правильный выбор метрики = правильная оптимизация = успех в продакшене!
