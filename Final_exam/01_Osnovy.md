# Модуль 1: Основы Machine Learning

## 1.1 k-Nearest Neighbors (k-NN)

### 1) Интуиция и суть

**Простыми словами (ELI5):**  
Представь, что ты переехал в новый район и хочешь понять, какая там средняя цена квартир. Ты смотришь на 5 ближайших к тебе домов и усредняешь их цены. Это и есть k-NN: ты смотришь на k ближайших "соседей" в данных и делаешь вывод на основе их значений.

**Академически:**  
k-NN — это **непараметрический метод** (не делает предположений о распределении данных), основанный на принципе **локальности**: объекты с похожими признаками имеют похожие целевые значения. Алгоритм находит k ближайших обучающих примеров в пространстве признаков и принимает решение на основе большинства голосов (классификация) или среднего значения (регрессия).

**Физический смысл:**  
Метод основан на **метрике расстояния** (обычно евклидово расстояние) между векторами признаков. Чем меньше расстояние, тем более "похожи" объекты.

### 2) Зачем и когда (Business Value)

**Какую проблему решает:**
- Классификация и регрессия **без обучения модели** (lazy learning)
- Работает с **нелинейными зависимостями** "из коробки"
- Подходит для **малых данных** (small data regime)

**Где это индустриальный стандарт:**
- **Рекомендательные системы**: поиск похожих пользователей/товаров (collaborative filtering)
- **Медицина**: диагностика на основе похожих случаев
- **Финансы**: оценка кредитоспособности по похожим клиентам
- **Computer Vision**: распознавание образов (в связке с feature extraction)

**Когда применять нельзя:**
- ❌ Высокоразмерные данные (**проклятие размерности**)
- ❌ Большие датасеты (медленный инференс: O(n) для каждого предсказания)
- ❌ Данные с разными масштабами признаков (требуется нормализация)
- ❌ Шумные данные (выбросы сильно влияют на результат)

### 3) Математическое ядро

**Евклидово расстояние:**
```
d(x, x_i) = √(Σ(x_j - x_i_j)²)
```

**Классификация (голосование):**
```
ŷ = mode{y_i : x_i ∈ N_k(x)}
```
где N_k(x) — множество k ближайших соседей точки x.

**Регрессия (среднее):**
```
ŷ = (1/k) Σ y_i, где x_i ∈ N_k(x)
```

**Взвешенный вариант:**
```
ŷ = Σ w_i · y_i / Σ w_i,  где w_i = 1/d(x, x_i)
```
Более близкие соседи имеют больший вес.

### 4) Middle-level нюансы

**Проблема 1: Проклятие размерности (Curse of Dimensionality)**

**Суть:**  
При увеличении числа признаков объем пространства растет экспоненциально, и все точки становятся "далеко" друг от друга. Понятие "близости" теряет смысл.

**Математика:**  
В d-мерном пространстве объем гиперкуба растет как L^d, а объем гиперсферы как r^d. При большом d почти весь объем концентрируется у границ.

**Пример:**  
В 1D для покрытия 10% пространства нужно 10% данных. В 10D для покрытия 10% объема нужно 10^(1/10) ≈ 79% данных по каждой оси!

**Решение:**
- Снижение размерности: **PCA, t-SNE, UMAP**
- Отбор признаков: **feature selection** (Lasso, mutual information)
- Использование **специальных метрик** (Manhattan, Minkowski)

**Проблема 2: Выбор k (гиперпараметр)**

- **k слишком мал (k=1)**: алгоритм переобучается, чувствителен к шуму
- **k слишком велик**: граница размывается, недообучение (underfitting)

**Эмпирическое правило:**  
k = √n (где n — размер обучающей выборки), затем подбор через **кросс-валидацию**.

**Проблема 3: Вычислительная сложность**

- **Обучение**: O(1) — просто сохраняем данные
- **Предсказание**: O(n·d·log k) — для каждого объекта ищем k соседей

**Решение:**
- **KD-Tree, Ball Tree**: O(log n) для поиска соседей (работает плохо при d > 20)
- **Approximate Nearest Neighbors** (ANN): Annoy, FAISS, HNSW

**Проблема 4: Масштаб признаков**

Признаки с большим диапазоном доминируют в расстоянии.

**Пример:**  
Возраст [0-100] и зарплата [0-1,000,000] — зарплата полностью определит расстояние.

**Решение:**
- **Стандартизация**: (x - μ) / σ
- **Нормализация**: (x - min) / (max - min)

### 5) Сравнение с альтернативами

| Метод | Сильные стороны | Слабые стороны |
|-------|----------------|----------------|
| **k-NN** | ✅ Нет обучения<br>✅ Нелинейные границы<br>✅ Интерпретируем | ❌ Медленный инференс<br>❌ Проклятие размерности<br>❌ Чувствителен к шуму |
| **Logistic Regression** | ✅ Быстрый<br>✅ Вероятности<br>✅ Работает при d >> n | ❌ Только линейные границы<br>❌ Требует feature engineering |
| **Random Forest** | ✅ Нелинейные зависимости<br>✅ Устойчив к выбросам | ❌ Медленный инференс<br>❌ Менее интерпретируем |

---

## 1.2 Ordinary Least Squares (OLS)

### 1) Интуиция и суть

**Простыми словами (ELI5):**  
У тебя есть точки на графике, и ты хочешь провести через них прямую линию так, чтобы она была максимально близка ко всем точкам. OLS находит эту "наилучшую" прямую, минимизируя сумму квадратов расстояний от точек до линии.

**Академически:**  
OLS — это метод оценки параметров **линейной модели** путем минимизации суммы квадратов остатков (ошибок предсказания). Это **оптимальная несмещенная оценка** при выполнении предпосылок теоремы Гаусса-Маркова.

**Физический смысл:**  
Квадраты ошибок наказывают большие отклонения сильнее, чем малые. Это делает метод чувствительным к выбросам, но статистически эффективным.

### 2) Зачем и когда (Business Value)

**Какую проблему решает:**
- Моделирование **линейных зависимостей** между признаками и целевой переменной
- **Прогнозирование** непрерывных величин
- **Интерпретация влияния** признаков (коэффициенты регрессии)

**Где это индустриальный стандарт:**
- **Финансы**: прогноз доходности акций, risk modeling
- **Эконометрика**: оценка спроса, эластичности цен
- **Недвижимость**: прогноз цен на основе характеристик
- **Медицина**: анализ влияния факторов на здоровье (дозировка → эффект)

**Когда применять нельзя:**
- ❌ Нелинейные зависимости (нужны полиномиальные признаки или нелинейные модели)
- ❌ Мультиколлинеарность (сильно коррелированные признаки)
- ❌ Гетероскедастичность (непостоянная дисперсия ошибок)
- ❌ Выбросы (используйте Huber loss или RANSAC)

### 3) Математическое ядро

**Модель:**
```
y = Xβ + ε
```
где:
- y ∈ ℝⁿ — вектор целевых значений
- X ∈ ℝⁿˣᵖ — матрица признаков (design matrix)
- β ∈ ℝᵖ — вектор коэффициентов
- ε ∼ N(0, σ²I) — ошибки (Gaussian noise)

**Функция потерь (MSE):**
```
L(β) = ||y - Xβ||² = Σ(y_i - x_i^T β)²
```

**Нормальное уравнение (аналитическое решение):**
```
β̂ = (X^T X)^(-1) X^T y
```

**Условие применимости:**  
Матрица X^T X должна быть **обратимой** (полного ранга). Иначе — **мультиколлинеарность**.

**Предсказание:**
```
ŷ = Xβ̂
```

**Остатки:**
```
e = y - ŷ
```

### 4) Middle-level нюансы

**Проблема 1: Нормальное уравнение vs Градиентный спуск**

| Метод | Сложность | Когда использовать |
|-------|-----------|-------------------|
| **Нормальное уравнение** | O(p³ + np²) | Малое p (p < 10,000), нужно точное решение |
| **Градиентный спуск** | O(iterations · np) | Большое p, онлайн обучение, регуляризация |

**Градиентный спуск:**
```
β^(t+1) = β^(t) - α · ∇L(β^(t))
∇L(β) = -2X^T(y - Xβ)
```

**Проблема 2: Мультиколлинеарность**

**Признаки:**  
Два или более признака сильно коррелированы (например, площадь дома в м² и в футах²).

**Последствия:**
- Матрица X^T X близка к вырожденной → **неустойчивость** (X^T X)^(-1)
- Коэффициенты имеют **огромные значения** и **высокую дисперсию**
- Малое изменение данных → сильное изменение β

**Диагностика:**
- **VIF (Variance Inflation Factor)**: VIF > 10 — проблема
  ```
  VIF_j = 1 / (1 - R²_j)
  ```
  где R²_j — R² регрессии j-го признака на остальные.

**Решение:**
- Удалить коррелированные признаки
- **PCA** (главные компоненты)
- **Регуляризация**: Ridge (L2)

**Проблема 3: Выбросы**

OLS минимизирует квадраты → выбросы имеют **квадратичное влияние**.

**Решение:**
- **Robust regression**: Huber loss, RANSAC
- Удаление выбросов после анализа (Cook's distance, leverage)

**Проблема 4: Предпосылки модели (Assumptions)**

1. **Линейность**: E[y|X] = Xβ
2. **Независимость** ошибок: cov(ε_i, ε_j) = 0
3. **Гомоскедастичность**: Var(ε_i) = σ² (постоянная)
4. **Нормальность** ошибок: ε_i ∼ N(0, σ²)
5. **Отсутствие мультиколлинеарности**

**Диагностика:**
- Residual plots (остатки vs предсказания)
- Q-Q plot (нормальность)
- Breusch-Pagan test (гомоскедастичность)

### 5) Сравнение с альтернативами

| Метод | Преимущества | Недостатки |
|-------|-------------|-----------|
| **OLS** | ✅ Интерпретируемость<br>✅ Быстрый<br>✅ Статистические тесты | ❌ Только линейные зависимости<br>❌ Чувствителен к выбросам<br>❌ Мультиколлинеарность |
| **Ridge (L2)** | ✅ Решает мультиколлинеарность<br>✅ Стабильные коэффициенты | ❌ Все признаки остаются (не делает feature selection) |
| **Lasso (L1)** | ✅ Feature selection<br>✅ Устойчив к мультиколлинеарности | ❌ Нестабилен при коррелированных признаках |
| **Polynomial Regression** | ✅ Нелинейные зависимости | ❌ Переобучение при высокой степени |

---

## 1.3 Bias-Variance Trade-off

### 1) Интуиция и суть

**Простыми словами (ELI5):**  
Представь, что ты стреляешь по мишени:
- **Bias (смещение)** — насколько твои выстрелы систематически отклонены от центра
- **Variance (разброс)** — насколько разбросаны выстрелы друг относительно друга

Идеальный стрелок — в центре и кучно. Но обычно приходится выбирать: либо стрелять точно в среднем, но разбросанно (low bias, high variance), либо кучно, но мимо (high bias, low variance).

**Академически:**  
Это **фундаментальное разложение ошибки** модели на три компонента: смещение (насколько модель системно ошибается), дисперсию (насколько сильно меняются предсказания при разных обучающих выборках) и неустранимый шум.

**Физический смысл:**  
Любая модель — это компромисс между **сложностью** (способностью уловить паттерны) и **обобщающей способностью** (устойчивостью к шуму).

### 2) Зачем и когда (Business Value)

**Какую проблему решает:**
- Понимание **природы ошибок** модели
- Выбор **оптимальной сложности** модели
- Объяснение **переобучения** и **недообучения**

**Где это критически важно:**
- **Выбор модели**: когда решаете, использовать ли линейную регрессию или deep learning
- **Регуляризация**: понимание, какой тип регуляризации нужен
- **Диагностика**: почему модель плохо работает на тесте

**Практические решения:**
- High bias → **усложнить модель** (больше признаков, нелинейность, глубже сеть)
- High variance → **упростить модель** (регуляризация, меньше параметров, больше данных)

### 3) Математическое ядро

**Декомпозиция ошибки:**

Пусть ŷ(x) — предсказание модели, y — истинное значение, f(x) — истинная зависимость.

```
E[(y - ŷ)²] = Bias² + Variance + Irreducible Error
```

**Подробно:**

1. **Bias²** (квадрат смещения):
```
Bias² = (E[ŷ(x)] - f(x))²
```
Насколько в среднем предсказания модели отличаются от истинной функции.

2. **Variance** (дисперсия):
```
Variance = E[(ŷ(x) - E[ŷ(x)])²]
```
Насколько сильно меняются предсказания при разных обучающих выборках.

3. **Irreducible Error** (неустранимая ошибка):
```
σ² = E[(y - f(x))²]
```
Шум в данных, который невозможно смоделировать.

**Полная формула:**
```
MSE = E[(y - ŷ)²] = (E[ŷ] - f)² + E[(ŷ - E[ŷ])²] + σ²
      |_____________|   |________________|   |__|
           Bias²            Variance        Noise
```

### 4) Middle-level нюансы

**Проблема 1: Trade-off (компромисс)**

При увеличении сложности модели:
- **Bias ↓** — модель лучше "подгоняется" под данные
- **Variance ↑** — модель становится чувствительна к конкретной выборке

**Графически:**

```
Error
  │      Total Error
  │         /\
  │        /  \
  │   Variance
  │      /      \___
  │     /           Bias²
  │____/________________\_____ Complexity
     Simple          Complex
```

**Оптимальная сложность** — там, где суммарная ошибка минимальна.

**Проблема 2: Примеры моделей**

| Тип модели | Bias | Variance | Пример |
|------------|------|----------|--------|
| **Простые** | High | Low | Линейная регрессия, Naive Bayes |
| **Средние** | Medium | Medium | Decision Tree (ограниченной глубины) |
| **Сложные** | Low | High | Deep Neural Networks, KNN (k=1) |

**Проблема 3: Признаки переобучения (Overfitting)**

**Симптомы:**
- Train error << Test error (большой разрыв)
- Model performance ухудшается на новых данных
- Модель "запоминает" тренировочные данные

**High Variance сценарий:**
```python
Train MSE: 0.01
Test MSE:  0.50  ← проблема!
```

**Решения:**
- **Regularization**: L1/L2, dropout
- **Больше данных**: variance ∝ 1/n
- **Упростить модель**: меньше параметров
- **Early stopping**: остановить обучение до переобучения
- **Ансамбли**: bagging снижает variance

**Проблема 4: Признаки недообучения (Underfitting)**

**Симптомы:**
- Train error ≈ Test error, но обе **высокие**
- Модель не улавливает паттерны

**High Bias сценарий:**
```python
Train MSE: 0.45
Test MSE:  0.47  ← обе плохие
```

**Решения:**
- **Усложнить модель**: polynomial features, глубже сеть
- **Больше признаков**: feature engineering
- **Убрать регуляризацию**: уменьшить λ
- **Обучаться дольше**: больше epochs

### 5) Сравнение подходов к балансу

| Подход | Как влияет на Bias/Variance |
|--------|----------------------------|
| **Regularization (L2)** | Bias ↑, Variance ↓ |
| **Feature selection** | Bias ↑, Variance ↓ |
| **Больше данных** | Bias ≈, Variance ↓ |
| **Bagging (Random Forest)** | Bias ≈, Variance ↓↓ |
| **Boosting (XGBoost)** | Bias ↓↓, Variance ↑ (но контролируется) |

---

## 1.4 Кросс-валидация (Cross-Validation)

### 1) Интуиция и суть

**Простыми словами (ELI5):**  
Представь, что у тебя есть экзаменационные вопросы. Вместо того чтобы практиковаться на одном наборе вопросов и потом провалить реальный экзамен (потому что вопросы другие), ты делишь вопросы на части и тренируешься на каждой части, проверяясь на остальных. Так ты точнее оценишь свою реальную подготовку.

**Академически:**  
Кросс-валидация — это **статистическая техника** оценки качества модели путем разбиения данных на несколько фолдов (частей). Модель обучается на k-1 фолдах и тестируется на оставшемся, процесс повторяется k раз. Итоговая метрика — среднее по всем фолдам.

**Физический смысл:**  
Мы **симулируем множественные train/test разбиения**, чтобы получить более надежную оценку обобщающей способности модели и снизить влияние случайности в разбиении.

### 2) Зачем и когда (Business Value)

**Какую проблему решает:**
- **Надежная оценка** качества модели (меньше зависит от случайного разбиения)
- **Подбор гиперпараметров** (GridSearch, RandomSearch)
- **Сравнение моделей** на одинаковых условиях
- **Максимальное использование данных** (особенно при малых выборках)

**Где это индустриальный стандарт:**
- **AutoML системы**: автоматический подбор гиперпараметров
- **Медицина**: малые датасеты, важна точная оценка
- **Финансы**: временные ряды (Time Series CV)
- **Kaggle competitions**: оценка моделей перед отправкой

**Когда применять нельзя:**
- ❌ Очень большие данные (вычислительно дорого → используйте hold-out)
- ❌ Временные ряды с простым k-fold (нарушается временной порядок → Time Series CV)
- ❌ Сильный дисбаланс классов с простым k-fold → Stratified CV

### 3) Математическое ядро

**k-Fold Cross-Validation:**

1. Разбить данные на k равных частей (фолдов)
2. Для i = 1 до k:
   - Обучить модель на фолдах {1, ..., k} \ {i}
   - Протестировать на фолде i
   - Сохранить метрику score_i
3. Итоговая метрика:
```
CV_score = (1/k) Σ score_i
```

**Стандартное отклонение:**
```
σ_CV = √[(1/k) Σ (score_i - CV_score)²]
```
Показывает **стабильность** модели.

**Leave-One-Out CV (LOOCV):**

Частный случай k-fold, где k = n (размер датасета).
- Обучаемся на n-1 примерах, тестируем на 1
- Повторяем n раз

**Формула:**
```
CV_LOOCV = (1/n) Σ L(y_i, ŷ_{-i})
```
где ŷ_{-i} — предсказание модели, обученной без i-го примера.

### 4) Middle-level нюансы

**Проблема 1: Выбор k (гиперпараметр CV)**

| k | Преимущества | Недостатки |
|---|-------------|-----------|
| **k=5** | ✅ Быстро<br>✅ Баланс bias/variance | ⚠️ Меньше данных для обучения каждой модели |
| **k=10** | ✅ Стандарт<br>✅ Хорошая оценка | Дольше, чем k=5 |
| **k=n (LOOCV)** | ✅ Максимум данных для обучения<br>✅ Детерминированная оценка | ❌ Очень медленно<br>❌ High variance оценки |

**Эмпирическое правило:**  
Используйте **k=10** по умолчанию. Для малых данных (n < 100) → k=5 или LOOCV.

**Проблема 2: Stratified k-Fold (для дисбаланса классов)**

**Проблема обычного k-fold:**  
При дисбалансе (например, 5% положительных) случайное разбиение может дать фолд без положительных примеров вообще!

**Решение — Stratified:**  
В каждом фолде **сохраняется пропорция классов** как в исходной выборке.

**Пример:**
```
Dataset: 100 примеров, 90 класс 0, 10 класс 1 (10% class 1)

k=5 Stratified:
  Fold 1: 18 класс 0, 2 класс 1 (10%)
  Fold 2: 18 класс 0, 2 класс 1 (10%)
  ...
  Fold 5: 18 класс 0, 2 класс 1 (10%)
```

**Когда использовать:**
- Классификация с **дисбалансом классов**
- Малые датасеты

**Проблема 3: Time Series Cross-Validation**

**Проблема обычного k-fold для временных рядов:**  
Обучение на будущих данных, тестирование на прошлых → **data leakage**!

**Решение — Time Series Split:**

```
Fold 1: Train [1..100],   Test [101..120]
Fold 2: Train [1..120],   Test [121..140]
Fold 3: Train [1..140],   Test [141..160]
...
```

**Скользящее окно (Sliding Window):**

```
Fold 1: Train [1..100],   Test [101..120]
Fold 2: Train [21..120],  Test [121..140]
Fold 3: Train [41..140],  Test [141..160]
```

**Когда использовать:**
- **Временные ряды**: цены акций, спрос, температура
- **Данные с временной структурой**: логи пользователей

**Проблема 4: Data Leakage через preprocessing**

**НЕПРАВИЛЬНО:**
```python
# Масштабируем ВСЕ данные, потом делим
X_scaled = scaler.fit_transform(X)  # ← leakage!
cross_val_score(model, X_scaled, y, cv=5)
```

Scaler "видел" тестовые фолды → завышенная оценка.

**ПРАВИЛЬНО:**
```python
# Масштабируем отдельно для каждого фолда
pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('model', LogisticRegression())
])
cross_val_score(pipeline, X, y, cv=5)  # ✅
```

Scaler обучается только на train фолдах каждой итерации.

### 5) Сравнение методов валидации

| Метод | Когда использовать | Плюсы | Минусы |
|-------|-------------------|-------|--------|
| **Hold-out (train/test)** | Большие данные, быстрый эксперимент | ✅ Быстро | ❌ Высокая дисперсия оценки |
| **k-Fold CV** | Стандартная задача, средний размер данных | ✅ Надежная оценка<br>✅ Использует все данные | ❌ k раз медленнее |
| **Stratified k-Fold** | Дисбаланс классов | ✅ Сохраняет пропорции | Только для классификации |
| **Time Series CV** | Временные ряды | ✅ Нет data leakage | ❌ Меньше данных на ранних фолдах |
| **LOOCV** | Очень малые данные (n < 50) | ✅ Максимум данных для обучения | ❌ Очень медленно<br>❌ High variance |

---

## Практические советы

1. **Всегда используйте CV** для подбора гиперпараметров, а не для финальной оценки (для нее — отдельный hold-out test set)

2. **Вложенная CV** (Nested CV) для **одновременного подбора гиперпараметров и оценки**:
   - Внешняя CV: оценка модели
   - Внутренняя CV: подбор гиперпараметров

3. **Для временных рядов** ВСЕГДА используйте Time Series CV

4. **Для дисбаланса** используйте Stratified CV + соответствующие метрики (F1, AUC-ROC)

5. **Pipeline** в sklearn автоматизирует правильный preprocessing в CV

---

## Резюме модуля

Этот модуль заложил **фундамент ML**:
- **k-NN**: простота и мощь локальности, борьба с проклятием размерности
- **OLS**: линейная регрессия, нормальное уравнение, мультиколлинеарность
- **Bias-Variance**: ключевой trade-off, объясняющий переобучение
- **Cross-Validation**: правильная оценка моделей и защита от overfitting

Эти концепции — основа для понимания всех последующих методов ML.
