# –ú–æ–¥—É–ª—å 9: Reinforcement Learning (–û–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º)

> **–ó–∞—á–µ–º —ç—Ç–æ—Ç –º–æ–¥—É–ª—å?**  
> Reinforcement Learning (RL) ‚Äî —ç—Ç–æ –≤—ã—Å—à–∞—è —Ñ–æ—Ä–º–∞ –æ–±—É—á–µ–Ω–∏—è –ò–ò, –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –ø–æ—Ö–æ–∂–∞—è –Ω–∞ —Ç–æ, –∫–∞–∫ —É—á–∞—Ç—Å—è –∂–∏–≤—ã–µ —Å—É—â–µ—Å—Ç–≤–∞: –º–µ—Ç–æ–¥–æ–º –ø—Ä–æ–± –∏ –æ—à–∏–±–æ–∫.
> - **–ê–≤—Ç–æ–Ω–æ–º–Ω–æ—Å—Ç—å**: –∞–≥–µ–Ω—Ç —Å–∞–º –∏—Å—Å–ª–µ–¥—É–µ—Ç –º–∏—Ä, –Ω–µ —Ç—Ä–µ–±—É—è –º–∏–ª–ª–∏–æ–Ω—ã —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –∫–∞—Ä—Ç–∏–Ω–æ–∫ –æ—Ç —á–µ–ª–æ–≤–µ–∫–∞.
> - **–°–ª–æ–∂–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏**: –≤ –∑–∞–¥–∞—á–∞—Ö —Ç–∏–ø–∞ —à–∞—Ö–º–∞—Ç –∏–ª–∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —ç–ª–µ–∫—Ç—Ä–æ—Å–µ—Ç—å—é RL –Ω–∞—Ö–æ–¥–∏—Ç —Ä–µ—à–µ–Ω–∏—è, –¥–æ –∫–æ—Ç–æ—Ä—ã—Ö –ª—é–¥–∏ –Ω–µ –¥–æ–¥—É–º–∞–ª–∏—Å—å –∑–∞ —Å—Ç–æ–ª–µ—Ç–∏—è.
> - **–ê–¥–∞–ø—Ç–∏–≤–Ω–æ—Å—Ç—å**: RL —Å–∏—Å—Ç–µ–º—ã –º–æ–≥—É—Ç –¥–æ–æ–±—É—á–∞—Ç—å—Å—è –Ω–∞ –ª–µ—Ç—É, –ø–æ–¥—Å—Ç—Ä–∞–∏–≤–∞—è—Å—å –ø–æ–¥ –º–µ–Ω—è—é—â–∏–π—Å—è —Ä—ã–Ω–æ–∫ –∏–ª–∏ —É—Å–ª–æ–≤–∏—è —Å—Ä–µ–¥—ã.

---

## 9.1 MDP (Markov Decision Process)

### 1) –ò–Ω—Ç—É–∏—Ü–∏—è –∏ —Å—É—Ç—å

**–ü—Ä–æ—Å—Ç—ã–º–∏ —Å–ª–æ–≤–∞–º–∏ (ELI5):**  
–¢—ã –∏–≥—Ä–∞–µ—à—å –≤ –≤–∏–¥–µ–æ–∏–≥—Ä—É:
- **–°–æ—Å—Ç–æ—è–Ω–∏–µ (state)**: –≥–¥–µ —Ç—ã –Ω–∞—Ö–æ–¥–∏—à—å—Å—è –Ω–∞ –∫–∞—Ä—Ç–µ, —Å–∫–æ–ª—å–∫–æ –∑–¥–æ—Ä–æ–≤—å—è, –∫–∞–∫–∏–µ –ø—Ä–µ–¥–º–µ—Ç—ã
- **–î–µ–π—Å—Ç–≤–∏–µ (action)**: —á—Ç–æ —Ç—ã –¥–µ–ª–∞–µ—à—å (–∏–¥–∏ –≤–ø–µ—Ä–µ–¥, –∞—Ç–∞–∫—É–π, –≤–æ–∑—å–º–∏ –ø—Ä–µ–¥–º–µ—Ç)
- **–ù–∞–≥—Ä–∞–¥–∞ (reward)**: –æ—á–∫–∏ –∑–∞ —É–±–∏–π—Å—Ç–≤–æ –º–æ–Ω—Å—Ç—Ä–∞ (+10), —É—Ä–æ–Ω –æ—Ç –≤—Ä–∞–≥–∞ (-5)
- **–¶–µ–ª—å**: –≤—ã–±–∏—Ä–∞—Ç—å –¥–µ–π—Å—Ç–≤–∏—è —Ç–∞–∫, —á—Ç–æ–±—ã **–º–∞–∫—Å–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Å—É–º–º–∞—Ä–Ω—ã–µ –æ—á–∫–∏** –∑–∞ –≤—Å—é –∏–≥—Ä—É

MDP ‚Äî —ç—Ç–æ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –º–æ–¥–µ–ª—å —Ç–∞–∫–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞ –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π.

**–ê–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏:**  
MDP (Markov Decision Process) ‚Äî —ç—Ç–æ —Ñ–æ—Ä–º–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å **sequential decision making** –≤ —Å—Ç–æ—Ö–∞—Å—Ç–∏—á–µ—Å–∫–æ–π —Å—Ä–µ–¥–µ. –ö–ª—é—á–µ–≤–æ–µ —Å–≤–æ–π—Å—Ç–≤–æ ‚Äî **Markov property**: –±—É–¥—É—â–µ–µ –∑–∞–≤–∏—Å–∏—Ç —Ç–æ–ª—å–∫–æ –æ—Ç —Ç–µ–∫—É—â–µ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è, –∞ –Ω–µ –æ—Ç –∏—Å—Ç–æ—Ä–∏–∏.

**–§–∏–∑–∏—á–µ—Å–∫–∏–π —Å–º—ã—Å–ª:**  
–ê–≥–µ–Ω—Ç –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤—É–µ—Ç —Å–æ —Å—Ä–µ–¥–æ–π, –≤—ã–±–∏—Ä–∞—è –¥–µ–π—Å—Ç–≤–∏—è –¥–ª—è –º–∞–∫—Å–∏–º–∏–∑–∞—Ü–∏–∏ **cumulative reward**.

### 2) –ó–∞—á–µ–º –∏ –∫–æ–≥–¥–∞ (Business Value)

**–ö–∞–∫—É—é –ø—Ä–æ–±–ª–µ–º—É —Ä–µ—à–∞–µ—Ç:**
- **Optimal decision making**: –∫–∞–∫ –¥–µ–π—Å—Ç–≤–æ–≤–∞—Ç—å –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ –≤ —É—Å–ª–æ–≤–∏—è—Ö –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏
- **Long-term planning**: —É—á–µ—Ç –æ—Ç–ª–æ–∂–µ–Ω–Ω—ã—Ö –Ω–∞–≥—Ä–∞–¥ (delayed rewards)
- **Trade-off exploration/exploitation**: –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç—å –Ω–æ–≤–æ–µ vs –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∏–∑–≤–µ—Å—Ç–Ω–æ–µ

**–ì–¥–µ —ç—Ç–æ –∏–Ω–¥—É—Å—Ç—Ä–∏–∞–ª—å–Ω—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç:**
- **–ò–≥—Ä—ã**: AlphaGo, —à–∞—Ö–º–∞—Ç—ã, –ø–æ–∫–µ—Ä
- **–†–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∞**: —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ä–æ–±–æ—Ç–∞–º–∏, –¥—Ä–æ–Ω–∞–º–∏
- **–§–∏–Ω–∞–Ω—Å—ã**: –∞–ª–≥–æ—Ä–∏—Ç–º–∏—á–µ—Å–∫–∞—è —Ç–æ—Ä–≥–æ–≤–ª—è, —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø–æ—Ä—Ç—Ñ–µ–ª–µ–º
- **–†–µ–∫–æ–º–µ–Ω–¥–∞—Ç–µ–ª—å–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã**: –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ (—Å–ª–µ–¥—É—é—â–µ–µ –≤–∏–¥–µ–æ –Ω–∞ YouTube)
- **Healthcare**: –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –ª–µ—á–µ–Ω–∏–µ (adaptive clinical trials)

**–ö–æ–≥–¥–∞ –ø—Ä–∏–º–µ–Ω—è—Ç—å –Ω–µ–ª—å–∑—è:**
- ‚ùå –ù–µ—Ç —á–µ—Ç–∫–æ–π reward function (–∑–∞–¥–∞—á–∞ supervised learning)
- ‚ùå –°–æ—Å—Ç–æ—è–Ω–∏—è –Ω–µ Markov (–Ω—É–∂–Ω–∞ —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω–æ—Å—Ç—å ‚Äî POMDP)

### 3) –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —è–¥—Ä–æ

**–ú–∞—Ç–µ–º–∞—Ç–∏–∫–∞ MDP:**
–ü—Ä–æ—Ü–µ—Å—Å –ú–∞—Ä–∫–æ–≤–∞ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è –∫–æ—Ä—Ç–µ–∂–µ–º $(S, A, P, R, \gamma)$:
- **–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ø–µ—Ä–µ—Ö–æ–¥–∞:** $P(s' | s, a) = \mathbb{P}(S_{t+1} = s' | S_t = s, A_t = a)$
- **–î–∏—Å–∫–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –Ω–∞–≥—Ä–∞–¥–∞ (Return):**
$$
G_t = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}
$$
- **–¶–µ–Ω–Ω–æ—Å—Ç—å —Å–æ—Å—Ç–æ—è–Ω–∏—è (Value Function):**
$$
V^\pi(s) = \mathbb{E}_\pi [G_t | S_t = s]
$$
- **–¶–µ–Ω–Ω–æ—Å—Ç—å –¥–µ–π—Å—Ç–≤–∏—è (Q-function):**
$$
Q^\pi(s, a) = \mathbb{E}_\pi [G_t | S_t = s, A_t = a]
$$

> [!CAUTION]
> **Production Warning: Exploration-Exploitation Trade-off**  
> –°–∞–º–∞—è —á–∞—Å—Ç–∞—è –ø—Ä–æ–±–ª–µ–º–∞ –≤ RL ‚Äî –∞–≥–µ–Ω—Ç –Ω–∞—Ö–æ–¥–∏—Ç "—Ç–µ—Ä–ø–∏–º—É—é" —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –∏ –ø–µ—Ä–µ—Å—Ç–∞–µ—Ç –ø—Ä–æ–±–æ–≤–∞—Ç—å –Ω–æ–≤–æ–µ. –í –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–µ —ç—Ç–æ –º–æ–∂–µ—Ç –∑–Ω–∞—á–∏—Ç—å, —á—Ç–æ –≤–∞—à —Ä–æ–±–æ—Ç –≤—Å–µ–≥–¥–∞ —Ö–æ–¥–∏—Ç –ø–æ –æ–¥–Ω–æ–º—É –∏ —Ç–æ–º—É –∂–µ (–Ω–µ –∏–¥–µ–∞–ª—å–Ω–æ–º—É) –º–∞—Ä—à—Ä—É—Ç—É. –†–µ—à–∞–µ—Ç—Å—è —á–µ—Ä–µ–∑ $\epsilon$-greedy –∏–ª–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ **Entrophy Bonus** –≤ —Ñ—É–Ω–∫—Ü–∏—é –ø–æ—Ç–µ—Ä—å.

> [!TIP]
> **–ê–Ω–∞–ª–æ–≥–∏—è: –î—Ä–µ—Å—Å–∏—Ä–æ–≤–∫–∞ —Å–æ–±–∞–∫–∏**  
> –°–æ–±–∞–∫–∞ (–ê–≥–µ–Ω—Ç) –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ –∫–æ–º–Ω–∞—Ç–µ (State). –í—ã –¥–∞–µ—Ç–µ –∫–æ–º–∞–Ω–¥—É (Action). –ï—Å–ª–∏ –æ–Ω–∞ –≤—ã–ø–æ–ª–Ω—è–µ—Ç –µ–µ ‚Äî –ø–æ–ª—É—á–∞–µ—Ç —Å–∞—Ö–∞—Ä–æ–∫ (Reward), –µ—Å–ª–∏ –Ω–µ—Ç ‚Äî –Ω–∏—á–µ–≥–æ. –°–æ –≤—Ä–µ–º–µ–Ω–µ–º —Å–æ–±–∞–∫–∞ —Å—Ç—Ä–æ–∏—Ç –≤ –≥–æ–ª–æ–≤–µ –∫–∞—Ä—Ç—É: "–≤ —ç—Ç–æ–π —Å–∏—Ç—É–∞—Ü–∏–∏ —Ç–∞–∫–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ –ø—Ä–∏–Ω–æ—Å–∏—Ç –µ–¥—É" (Value Function). –í RL –º—ã –¥–µ–ª–∞–µ–º —Ä–æ–≤–Ω–æ —Ç–æ –∂–µ —Å–∞–º–æ–µ, —Ç–æ–ª—å–∫–æ —Å –º–∞—Ç–µ–º–∞—Ç–∏–∫–æ–π.

### 4) Middle-level –Ω—é–∞–Ω—Å—ã

**–ü—Ä–æ–±–ª–µ–º–∞ 1: Discount Factor Œ≥**

**$\gamma = 0$:**  
–ê–≥–µ–Ω—Ç "–±–ª–∏–∑–æ—Ä—É–∫", –º–∞–∫—Å–∏–º–∏–∑–∏—Ä—É–µ—Ç —Ç–æ–ª—å–∫–æ –º–≥–Ω–æ–≤–µ–Ω–Ω—É—é –Ω–∞–≥—Ä–∞–¥—É.

**$\gamma \to 1$:**  
–ê–≥–µ–Ω—Ç —Ü–µ–Ω–∏—Ç –±—É–¥—É—â–∏–µ –Ω–∞–≥—Ä–∞–¥—ã –ø–æ—á—Ç–∏ —Ç–∞–∫ –∂–µ, –∫–∞–∫ —Ç–µ–∫—É—â–∏–µ.

**–ó–∞—á–µ–º $\gamma < 1$:**
1. –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å (–±–µ—Å–∫–æ–Ω–µ—á–Ω—ã–µ —Å—É–º–º—ã)
2. –ù–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç—å –±—É–¥—É—â–µ–≥–æ (–ª—É—á—à–µ –ø–æ–ª—É—á–∏—Ç—å –Ω–∞–≥—Ä–∞–¥—É —Å–µ–π—á–∞—Å)
3. –ö–æ–Ω–µ—á–Ω—ã–π –≥–æ—Ä–∏–∑–æ–Ω—Ç –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö

**–≠–º–ø–∏—Ä–∏—á–µ—Å–∫–∏:**  
$\gamma = 0.9\text{--}0.99$ (—Å—Ç–∞–Ω–¥–∞—Ä—Ç).

**–ü—Ä–æ–±–ª–µ–º–∞ 2: Optimal Policy**

**Optimal state-value:**
$$
V^*(s) = \max_{\pi} V^{\pi}(s)
$$

**Optimal action-value:**
$$
Q^*(s, a) = \max_{\pi} Q^{\pi}(s, a)
$$

**–°–≤—è–∑—å:**
$$
V^*(s) = \max_{a} Q^*(s, a)
$$

**Optimal policy:**
$$
\pi^*(s) = \arg\max_{a} Q^*(s, a)
$$

–ñ–∞–¥–Ω–∞—è policy –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ Q*.

**–ü—Ä–æ–±–ª–µ–º–∞ 3: Exploration vs Exploitation**

**Exploitation:**  
–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª—É—á—à–µ–µ –∏–∑–≤–µ—Å—Ç–Ω–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ ($\arg\max_{a} Q(s, a)$).

**Exploration:**  
–ü—Ä–æ–±–æ–≤–∞—Ç—å –¥—Ä—É–≥–∏–µ –¥–µ–π—Å—Ç–≤–∏—è (–º–æ–∂–µ—Ç –Ω–∞–π—Ç–∏ —á—Ç–æ-—Ç–æ –ª—É—á—à–µ!).

**Trade-off:**  
–°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ exploitation ‚Üí –∑–∞—Å—Ç—Ä—è—Ç—å –≤ sub-optimal policy.  
–°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ exploration ‚Üí –Ω–∏–∫–æ–≥–¥–∞ –Ω–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –Ω–∞–π–¥–µ–Ω–Ω–æ–µ.

**–†–µ—à–µ–Ω–∏–µ:**
- **$\epsilon$-greedy**: —Å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é $\epsilon$ –≤—ã–±—Ä–∞—Ç—å —Å–ª—É—á–∞–π–Ω–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ, –∏–Ω–∞—á–µ $\arg\max Q$
- **Softmax (Boltzmann)**: –≤—ã–±–∏—Ä–∞—Ç—å –¥–µ–π—Å—Ç–≤–∏—è –ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª—å–Ω–æ $\exp(Q(s, a) / \tau)$
- **UCB (Upper Confidence Bound)**: –≤—ã–±–∏—Ä–∞—Ç—å –¥–µ–π—Å—Ç–≤–∏—è —Å —É—á–µ—Ç–æ–º –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏

### 5) –°—Ä–∞–≤–Ω–µ–Ω–∏–µ RL vs Supervised Learning

| –ê—Å–ø–µ–∫—Ç | Supervised Learning | Reinforcement Learning |
|--------|-------------------|----------------------|
| **–û–±—Ä–∞—Ç–Ω–∞—è —Å–≤—è–∑—å** | –ü—Ä–∞–≤–∏–ª—å–Ω—ã–µ –º–µ—Ç–∫–∏ (y) | –ù–∞–≥—Ä–∞–¥—ã (r) |
| **–î–∞–Ω–Ω—ã–µ** | –§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π dataset | –ê–≥–µ–Ω—Ç –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –¥–∞–Ω–Ω—ã–µ —á–µ—Ä–µ–∑ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ |
| **–¶–µ–ª—å** | –ü—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å y –¥–ª—è x | –ú–∞–∫—Å–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å cumulative reward |
| **Exploration** | –ù–µ—Ç | ‚úÖ –ö—Ä–∏—Ç–∏—á–Ω–æ |
| **Delayed rewards** | –ù–µ—Ç | ‚úÖ –î–∞ |

---

## 9.2 Bellman Equation (–£—Ä–∞–≤–Ω–µ–Ω–∏–µ –ë–µ–ª–ª–º–∞–Ω–∞)

### 1) –ò–Ω—Ç—É–∏—Ü–∏—è –∏ —Å—É—Ç—å

**–ü—Ä–æ—Å—Ç—ã–º–∏ —Å–ª–æ–≤–∞–º–∏ (ELI5):**  
–¶–µ–Ω–Ω–æ—Å—Ç—å —Å–æ—Å—Ç–æ—è–Ω–∏—è = –Ω–∞–≥—Ä–∞–¥–∞ –∑–∞ —Ç–µ–∫—É—â–∏–π —à–∞–≥ + —Ü–µ–Ω–Ω–æ—Å—Ç—å —Å–ª–µ–¥—É—é—â–µ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è (—Å —É—á–µ—Ç–æ–º discount).

–ù–∞–ø—Ä–∏–º–µ—Ä, —Ü–µ–Ω–Ω–æ—Å—Ç—å "–±—ã—Ç—å –Ω–∞ —Ä–∞–±–æ—Ç–µ" = –∑–∞—Ä–ø–ª–∞—Ç–∞ —Å–µ–≥–æ–¥–Ω—è + —Ü–µ–Ω–Ω–æ—Å—Ç—å "–±—ã—Ç—å –¥–æ–º–∞ –≤–µ—á–µ—Ä–æ–º" (–æ—Ç–¥—ã—Ö ‚Üí –∑–∞–≤—Ç—Ä–∞ —Å–Ω–æ–≤–∞ –Ω–∞ —Ä–∞–±–æ—Ç–µ —Å –Ω–æ–≤—ã–º–∏ —Å–∏–ª–∞–º–∏).

**–ê–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏:**  
Bellman Equation ‚Äî —ç—Ç–æ **—Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ–µ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ**, —Å–≤—è–∑—ã–≤–∞—é—â–µ–µ value function –≤ —Ç–µ–∫—É—â–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–∏ —Å value –≤ —Å–ª–µ–¥—É—é—â–∏—Ö —Å–æ—Å—Ç–æ—è–Ω–∏—è—Ö. –û—Å–Ω–æ–≤–∞ –¥–ª—è **Dynamic Programming** –∏ –º–Ω–æ–≥–∏—Ö RL –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤.

**–§–∏–∑–∏—á–µ—Å–∫–∏–π —Å–º—ã—Å–ª:**  
–û–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ –≤ s –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö —Ä–µ—à–µ–Ω–∏–π –≤ —Å–æ—Å—Ç–æ—è–Ω–∏—è—Ö, –¥–æ—Å—Ç–∏–∂–∏–º—ã—Ö –∏–∑ s.

### 2) –ó–∞—á–µ–º –∏ –∫–æ–≥–¥–∞ (Business Value)

**–ö–∞–∫—É—é –ø—Ä–æ–±–ª–µ–º—É —Ä–µ—à–∞–µ—Ç:**
- **–í—ã—á–∏—Å–ª–µ–Ω–∏–µ V^œÄ –∏ Q^œÄ**: –æ—Ü–µ–Ω–∫–∞ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
- **–ù–∞—Ö–æ–∂–¥–µ–Ω–∏–µ V* –∏ Q***: –æ–ø—Ç–∏–º–∞–ª—å–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è
- **–û—Å–Ω–æ–≤–∞ –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤**: Value Iteration, Policy Iteration, Q-learning, TD-learning

**–ì–¥–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è:**
- –í–µ–∑–¥–µ –≤ RL!

### 3) –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —è–¥—Ä–æ

**Bellman Expectation Equation (–¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω–æ–π policy œÄ):**

**–î–ª—è $V^\pi$:**
$$
V^\pi(s) = \sum_{a} \pi(a|s) \sum_{s'} P(s'|s, a) [R(s, a, s') + \gamma V^\pi(s')]
$$

**–†–∞–∑–±–æ—Ä:**
- œÄ(a|s): –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –≤—ã–±—Ä–∞—Ç—å a –≤ s
- P(s'|s, a): –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ø–µ—Ä–µ—Ö–æ–¥–∞ –≤ s'
- R + Œ≥¬∑$V^\pi$(s'): –º–≥–Ω–æ–≤–µ–Ω–Ω–∞—è –Ω–∞–≥—Ä–∞–¥–∞ + –¥–∏—Å–∫–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ü–µ–Ω–Ω–æ—Å—Ç—å —Å–ª–µ–¥—É—é—â–µ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è

**–î–ª—è $Q^\pi$:**
$$
Q^\pi(s, a) = \sum_{s'} P(s'|s, a) [R(s, a, s') + \gamma \sum_{a'} \pi(a'|s') Q^\pi(s', a')]
$$

**Bellman Optimality Equation (–¥–ª—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π policy):**

**–£—Ä–∞–≤–Ω–µ–Ω–∏–µ –ë–µ–ª–ª–º–∞–Ω–∞ (–û–ø—Ç–∏–º–∞–ª—å–Ω–æ—Å—Ç—å):**
$$
V^*(s) = \max_{a} \mathbb{E} [R_{t+1} + \gamma V^*(S_{t+1}) | S_t = s, A_t = a]
$$
$$
Q^*(s, a) = \mathbb{E} [R_{t+1} + \gamma \max_{a'} Q^*(S_{t+1}, a') | S_t = s, A_t = a]
$$

**Q-learning Update Rule (Temporal Difference):**
$$
Q(s, a) \leftarrow Q(s, a) + \alpha \underbrace{\left[ R + \gamma \max_{a'} Q(s', a') - Q(s, a) \right]}_{\text{TD Error}}
$$

> [!IMPORTANT]
> **Reward Engineering**  
> –°–∞–º–∞—è —Å–ª–æ–∂–Ω–∞—è —á–∞—Å—Ç—å RL –≤ –ø—Ä–æ–¥–∞–∫—à–µ–Ω–µ ‚Äî –ø—Ä–∏–¥—É–º–∞—Ç—å –Ω–∞–≥—Ä–∞–¥—É. –ï—Å–ª–∏ –≤—ã –¥–∞–¥–∏—Ç–µ —Ä–æ–±–æ—Ç—É –Ω–∞–≥—Ä–∞–¥—É –∑–∞ "–±—ã—Å—Ç—Ä–æ–µ –ø–µ—Ä–µ–¥–≤–∏–∂–µ–Ω–∏–µ", –æ–Ω –º–æ–∂–µ—Ç –Ω–∞—É—á–∏—Ç—å—Å—è –ø—Ä–æ—Å—Ç–æ –≤—Ä–∞—â–∞—Ç—å—Å—è –Ω–∞ –º–µ—Å—Ç–µ —Å –æ–≥—Ä–æ–º–Ω–æ–π —Å–∫–æ—Ä–æ—Å—Ç—å—é. –î–∏–∑–∞–π–Ω —Ñ—É–Ω–∫—Ü–∏–∏ –Ω–∞–≥—Ä–∞–¥—ã ‚Äî —ç—Ç–æ 80% —É—Å–ø–µ—Ö–∞ –≤ RL.

> [!TIP]
> **–ê–Ω–∞–ª–æ–≥–∏—è: –í–∏–¥–µ–æ–∏–≥—Ä–∞ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º**  
> –£—Ä–∞–≤–Ω–µ–Ω–∏–µ –ë–µ–ª–ª–º–∞–Ω–∞ ‚Äî —ç—Ç–æ –∫–∞–∫ –µ—Å–ª–∏ –±—ã –≤—ã —Å—Ç–æ—è–ª–∏ –Ω–∞ —Ä–∞–∑–≤–∏–ª–∫–µ –≤ –∏–≥—Ä–µ –∏ –∑–Ω–∞–ª–∏ —Ü–µ–Ω–Ω–æ—Å—Ç—å –∫–∞–∂–¥–æ–π —Å–ª–µ–¥—É—é—â–µ–π –∫–æ–º–Ω–∞—Ç—ã. –í—ã –ø—Ä–æ—Å—Ç–æ –≤—ã–±–∏—Ä–∞–µ—Ç–µ —Ç—É –¥–≤–µ—Ä—å, –∑–∞ –∫–æ—Ç–æ—Ä–æ–π "—Å—É–º–º–∞ —Å–æ–∫—Ä–æ–≤–∏—â –≤ –∫–æ–º–Ω–∞—Ç–µ + –ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∏–∑ –≤—Å–µ—Ö –¥–≤–µ—Ä–µ–π –∑–∞ –Ω–µ–π" –º–∞–∫—Å–∏–º–∞–ª–µ–Ω. Q-learning –ø–æ–∑–≤–æ–ª—è–µ—Ç –≤—ã—É—á–∏—Ç—å —ç—Ç–∏ —Ü–µ–Ω–Ω–æ—Å—Ç–∏ –±–µ–∑ –ø—Ä–æ—Ö–æ–∂–¥–µ–Ω–∏—è –∏–≥—Ä—ã –º–∏–ª–ª–∏–æ–Ω —Ä–∞–∑, –ø—Ä–æ—Å—Ç–æ –∑–∞–≥–ª—è–¥—ã–≤–∞—è –Ω–∞ –æ–¥–∏–Ω —à–∞–≥ –≤–ø–µ—Ä–µ–¥.

### 4) Middle-level –Ω—é–∞–Ω—Å—ã

**–ü—Ä–æ–±–ª–µ–º–∞ 1: –†–µ—à–µ–Ω–∏–µ Bellman Equations**

**Bellman Expectation:**  
–°–∏—Å—Ç–µ–º–∞ –ª–∏–Ω–µ–π–Ω—ã—Ö —É—Ä–∞–≤–Ω–µ–Ω–∏–π (–µ—Å–ª–∏ $S$ –∫–æ–Ω–µ—á–Ω–æ) ‚Üí –º–æ–∂–Ω–æ —Ä–µ—à–∏—Ç—å –Ω–∞–ø—Ä—è–º—É—é:
$$
\mathbf{V} = \mathbf{R} + \gamma \mathbf{P} \mathbf{V} \implies \mathbf{V} = (\mathbf{I} - \gamma \mathbf{P})^{-1} \mathbf{R}
$$

**–ü—Ä–æ–±–ª–µ–º–∞:** O(|S|¬≥) ‚Äî –Ω–µ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ—Ç—Å—è.

**Iterative –º–µ—Ç–æ–¥—ã:**
- **Value Iteration**
- **Policy Iteration**
- **Temporal Difference** (TD) learning

**Bellman Optimality:**  
–ù–µ–ª–∏–Ω–µ–π–Ω–æ–µ (–∏–∑-–∑–∞ max) ‚Üí –∏—Å–ø–æ–ª—å–∑—É–µ–º **Value Iteration** –∏–ª–∏ **Q-learning**.

**–ü—Ä–æ–±–ª–µ–º–∞ 2: Value Iteration**

**–ê–ª–≥–æ—Ä–∏—Ç–º:**

–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è: $V_0(s) = 0$ –¥–ª—è –≤—Å–µ—Ö $s$

–ü–æ–≤—Ç–æ—Ä—è—Ç—å:
$$
V_{k+1}(s) = \max_{a} \sum_{s'} P(s'|s, a) [R(s, a, s') + \gamma V_k(s')]
$$

–î–æ —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏: max_s |V_{k+1}(s) - V_k(s)| < Œµ

**–¢–µ–æ—Ä–µ–º–∞:**  
V_k ‚Üí V* –ø—Ä–∏ k ‚Üí ‚àû (contraction mapping).

**Policy Extraction:**
```
œÄ*(s) = argmax Œ£ P(s'|s, a) ¬∑ [R(s, a, s') + Œ≥ ¬∑ V*(s')]
        a      s'
```

**–ü—Ä–æ–±–ª–µ–º–∞ 3: Policy Iteration**

**–ê–ª–≥–æ—Ä–∏—Ç–º:**

1. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è: —Å–ª—É—á–∞–π–Ω–∞—è policy œÄ_0
2. –ü–æ–≤—Ç–æ—Ä—è—Ç—å:
   - **Policy Evaluation**: —Ä–µ—à–∏—Ç—å Bellman Expectation –¥–ª—è V^œÄ
   - **Policy Improvement**:
     ```
     œÄ_{k+1}(s) = argmax Q^œÄ_k(s, a)
                  a
     ```
3. –î–æ —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏: œÄ_k = œÄ_{k-1}

**–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å Value Iteration:**
- **Policy Iteration**: –º–µ–Ω—å—à–µ –∏—Ç–µ—Ä–∞—Ü–∏–π, –Ω–æ –∫–∞–∂–¥–∞—è –∏—Ç–µ—Ä–∞—Ü–∏—è –¥–æ—Ä–æ–∂–µ (–ø–æ–ª–Ω–∞—è policy evaluation)
- **Value Iteration**: –±–æ–ª—å—à–µ –∏—Ç–µ—Ä–∞—Ü–∏–π, –Ω–æ –∫–∞–∂–¥–∞—è –¥–µ—à–µ–≤–ª–µ

### 5) –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç–æ–¥–æ–≤ —Ä–µ—à–µ–Ω–∏—è MDP

| –ú–µ—Ç–æ–¥ | –¢—Ä–µ–±—É–µ—Ç –º–æ–¥–µ–ª—å P, R | –°—Ö–æ–¥–∏–º–æ—Å—Ç—å | –í—ã—á–∏—Å–ª–µ–Ω–∏—è per iteration | –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å |
|-------|-------------------|-----------|-------------------------|-------------------|
| **Value Iteration** | ‚úÖ –î–∞ | ‚úÖ –ì–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∞ | O(\|S\|¬≤¬∑\|A\|) | –ú–∞–ª—ã–µ MDP, –∏–∑–≤–µ—Å—Ç–Ω–∞ –º–æ–¥–µ–ª—å |
| **Policy Iteration** | ‚úÖ –î–∞ | ‚úÖ –ì–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∞ (–±—ã—Å—Ç—Ä–µ–µ VI) | O(\|S\|¬≥) + O(\|S\|¬≤¬∑\|A\|) | –ú–∞–ª—ã–µ MDP |
| **Q-learning** | ‚ùå –ù–µ—Ç (model-free) | ‚ö†Ô∏è –°—Ç–æ—Ö–∞—Å—Ç–∏—á–µ—Å–∫–∞—è | O(1) per step | –ë–æ–ª—å—à–∏–µ MDP, –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –º–æ–¥–µ–ª—å |

---

## 9.3 Q-learning

### 1) –ò–Ω—Ç—É–∏—Ü–∏—è –∏ —Å—É—Ç—å

**–ü—Ä–æ—Å—Ç—ã–º–∏ —Å–ª–æ–≤–∞–º–∏ (ELI5):**  
–¢—ã —É—á–∏—à—å—Å—è –∏–≥—Ä–∞—Ç—å –≤ –Ω–æ–≤—É—é –∏–≥—Ä—É, –Ω–µ –∑–Ω–∞—è –ø—Ä–∞–≤–∏–ª (–º–æ–¥–µ–ª–∏ —Å—Ä–µ–¥—ã). –¢—ã:
1. –ü—Ä–æ–±—É–µ—à—å –¥–µ–π—Å—Ç–≤–∏—è
2. –ü–æ–ª—É—á–∞–µ—à—å –Ω–∞–≥—Ä–∞–¥—ã
3. **–û–±–Ω–æ–≤–ª—è–µ—à—å –æ—Ü–µ–Ω–∫—É** Q(s, a): "–Ω–∞—Å–∫–æ–ª—å–∫–æ —Ö–æ—Ä–æ—à–æ —Å–¥–µ–ª–∞—Ç—å a –≤ s"
4. –°–æ –≤—Ä–µ–º–µ–Ω–µ–º Q ‚Üí Q* (–æ–ø—Ç–∏–º–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞)

Q-learning —É—á–∏—Ç—Å—è **–º–µ—Ç–æ–¥–æ–º –ø—Ä–æ–± –∏ –æ—à–∏–±–æ–∫** (trial and error).

**–ê–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏:**  
Q-learning ‚Äî —ç—Ç–æ **model-free, off-policy** –∞–ª–≥–æ—Ä–∏—Ç–º **Temporal Difference** learning, –∫–æ—Ç–æ—Ä—ã–π —É—á–∏—Ç—Å—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π action-value function Q* –ø—É—Ç–µ–º –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –æ—Ü–µ–Ω–æ–∫ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–ø—ã—Ç–∞ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å–æ —Å—Ä–µ–¥–æ–π.

**–§–∏–∑–∏—á–µ—Å–∫–∏–π —Å–º—ã—Å–ª:**  
Bootstrapping: –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–∫—É—â—É—é –æ—Ü–µ–Ω–∫—É Q –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —Å–∞–º–æ–π Q.

### 2) –ó–∞—á–µ–º –∏ –∫–æ–≥–¥–∞ (Business Value)

**–ö–∞–∫—É—é –ø—Ä–æ–±–ª–µ–º—É —Ä–µ—à–∞–µ—Ç:**
- **Model-free**: –Ω–µ –Ω—É–∂–Ω–æ –∑–Ω–∞—Ç—å P(s'|s, a) –∏ R(s, a, s')
- **Off-policy**: —É—á–∏—Ç—Å—è –æ—Ç –ª—é–±—ã—Ö –¥–∞–Ω–Ω—ã—Ö (–¥–∞–∂–µ –æ—Ç —Å–ª—É—á–∞–π–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π)
- **–ê—Å–∏–º–ø—Ç–æ—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ**: Q ‚Üí Q* (–ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ —É—Å–ª–æ–≤–∏–π)

**–ì–¥–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è:**
- **–ò–≥—Ä—ã**: Atari (—Å Deep Q-Learning)
- **–†–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∞**: –∫–æ–≥–¥–∞ –º–æ–¥–µ–ª—å —Å—Ä–µ–¥—ã –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–∞
- **–ê–≤—Ç–æ–Ω–æ–º–Ω—ã–µ –∞–≤—Ç–æ**: –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ —Å–∏–º—É–ª—è—Ç–æ—Ä–µ + —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö

**–ö–æ–≥–¥–∞ –ø—Ä–∏–º–µ–Ω—è—Ç—å –Ω–µ–ª—å–∑—è:**
- ‚ùå –ù–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–µ –¥–µ–π—Å—Ç–≤–∏—è –Ω–∞–ø—Ä—è–º—É—é (–Ω—É–∂–µ–Ω DQN —Å –∞–ø–ø—Ä–æ–∫—Å–∏–º–∞—Ü–∏–µ–π –∏–ª–∏ Actor-Critic)
- ‚ùå –û–≥—Ä–æ–º–Ω—ã–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ —Å–æ—Å—Ç–æ—è–Ω–∏–π –±–µ–∑ –∞–ø–ø—Ä–æ–∫—Å–∏–º–∞—Ü–∏–∏ (–Ω—É–∂–Ω–∞ –Ω–µ–π—Ä–æ—Å–µ—Ç—å ‚Üí DQN)

### 3) –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —è–¥—Ä–æ

**Q-learning Update Rule:**

–ü–æ—Å–ª–µ –ø–µ—Ä–µ—Ö–æ–¥–∞ (s, a, r, s'):
```
Q(s, a) ‚Üê Q(s, a) + Œ± ¬∑ [r + Œ≥ ¬∑ max Q(s', a') - Q(s, a)]
                                    a'
```

–≥–¥–µ:
- **Œ±** ‚Äî learning rate (—Ä–∞–∑–º–µ—Ä —à–∞–≥–∞)
- **r + Œ≥ ¬∑ max Q(s', a')** ‚Äî **TD target** (temporal difference target)
- **r + Œ≥ ¬∑ max Q(s', a') - Q(s, a)** ‚Äî **TD error** (—Ä–∞–∑–Ω–∏—Ü–∞ –º–µ–∂–¥—É –Ω–æ–≤–æ–π –æ—Ü–µ–Ω–∫–æ–π –∏ —Å—Ç–∞—Ä–æ–π)

**–†–∞–∑–±–æ—Ä:**
- –¢–µ–∫—É—â–∞—è –æ—Ü–µ–Ω–∫–∞: Q(s, a)
- –ù–æ–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è: r + Œ≥ ¬∑ max Q(s', a') (–º–≥–Ω–æ–≤–µ–Ω–Ω–∞—è –Ω–∞–≥—Ä–∞–¥–∞ + best Q –≤ —Å–ª–µ–¥—É—é—â–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–∏)
- –û–±–Ω–æ–≤–ª—è–µ–º Q –≤ —Å—Ç–æ—Ä–æ–Ω—É –Ω–æ–≤–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏

**Off-policy:**  
–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç **max** (–∂–∞–¥–Ω–æ–µ), –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –æ—Ç —Ç–æ–≥–æ, –∫–∞–∫–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ a —Ä–µ–∞–ª—å–Ω–æ –≤—ã–±—Ä–∞–Ω–æ (–º–æ–∂–µ—Ç –±—ã—Ç—å Œµ-greedy –¥–ª—è exploration).

**–ê–ª–≥–æ—Ä–∏—Ç–º Q-learning:**

1. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è: Q(s, a) = 0 –¥–ª—è –≤—Å–µ—Ö (s, a)
2. –î–ª—è –∫–∞–∂–¥–æ–≥–æ —ç–ø–∏–∑–æ–¥–∞:
   - –ù–∞—á–∞—Ç—å —Å s_0
   - –î–ª—è –∫–∞–∂–¥–æ–≥–æ —à–∞–≥–∞ t:
     - –í—ã–±—Ä–∞—Ç—å a –ø–æ Œµ-greedy –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ Q:
       ```
       a = {argmax Q(s, a'),  —Å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é 1 - Œµ;
            random,           —Å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é Œµ}
       ```
     - –í—ã–ø–æ–ª–Ω–∏—Ç—å a, –ø–æ–ª—É—á–∏—Ç—å r, s'
     - –û–±–Ω–æ–≤–∏—Ç—å Q:
       ```
       Q(s, a) ‚Üê Q(s, a) + Œ± ¬∑ [r + Œ≥ ¬∑ max Q(s', a') - Q(s, a)]
       ```
     - s ‚Üê s'
   - –î–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è —ç–ø–∏–∑–æ–¥–∞

### 4) Middle-level –Ω—é–∞–Ω—Å—ã

**–ü—Ä–æ–±–ª–µ–º–∞ 1: Convergence (—Å—Ö–æ–¥–∏–º–æ—Å—Ç—å)**

**–¢–µ–æ—Ä–µ–º–∞ (Watkins & Dayan, 1992):**  
Q-learning —Å—Ö–æ–¥–∏—Ç—Å—è –∫ Q* –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ —É—Å–ª–æ–≤–∏–π:
1. –í—Å–µ (s, a) –ø–∞—Ä—ã –ø–æ—Å–µ—â–∞—é—Ç—Å—è –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ —á–∞—Å—Ç–æ
2. Learning rate Œ±_t —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä—è–µ—Ç:
   ```
   Œ£ Œ±_t = ‚àû,  Œ£ Œ±_t¬≤ < ‚àû
   ```
   –ù–∞–ø—Ä–∏–º–µ—Ä: Œ±_t = 1/t^0.8

**–ù–∞ –ø—Ä–∞–∫—Ç–∏–∫–µ:**  
–ò—Å–ø–æ–ª—å–∑—É—é—Ç —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π Œ± = 0.01-0.1 ‚Üí —Å—Ç–æ—Ö–∞—Å—Ç–∏—á–µ—Å–∫–∞—è —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å –∫ –æ–∫—Ä–µ—Å—Ç–Ω–æ—Å—Ç–∏ Q*.

**–ü—Ä–æ–±–ª–µ–º–∞ 2: Exploration vs Exploitation**

**Œµ-greedy:**
- Œµ = 1.0 –≤ –Ω–∞—á–∞–ª–µ (–ø–æ–ª–Ω–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ)
- Œµ ‚Üí 0.1 –∏–ª–∏ 0.01 –∫ –∫–æ–Ω—Ü—É (–ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ exploitation)

**Decay schedule:**
```
Œµ_t = max(Œµ_min, Œµ_start ¬∑ decay^t)
```

**–ü—Ä–æ–±–ª–µ–º–∞ 3: Experience Replay (–¥–ª—è Deep Q-Learning)**

**–ü—Ä–æ–±–ª–µ–º–∞ vanilla Q-learning —Å –Ω–µ–π—Ä–æ—Å–µ—Ç—è–º–∏:**
- –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å–∏–ª—å–Ω–æ –∫–æ—Ä—Ä–µ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã ‚Üí –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ
- –ó–∞–±—ã–≤–∞–Ω–∏–µ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ –æ–ø—ã—Ç–∞ (catastrophic forgetting)

**–†–µ—à–µ–Ω–∏–µ ‚Äî Experience Replay:**

1. –°–æ—Ö—Ä–∞–Ω—è—Ç—å transitions (s, a, r, s') –≤ **replay buffer**
2. –û–±—É—á–∞—Ç—å —Å–µ—Ç—å –Ω–∞ **—Å–ª—É—á–∞–π–Ω—ã—Ö mini-batches** –∏–∑ buffer

**–≠—Ñ—Ñ–µ–∫—Ç:**
- –î–µ–∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
- –ü–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –æ–ø—ã—Ç–∞ (sample efficiency)

**–ü—Ä–æ–±–ª–µ–º–∞ 4: Q-table vs Function Approximation**

**Q-table (—Ç–∞–±–ª–∏—á–Ω—ã–π Q-learning):**
```
Q = –º–∞—Ç—Ä–∏—Ü–∞ |S| √ó |A|
```

**–ü—Ä–æ–±–ª–µ–º–∞:**  
–ù–µ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ—Ç—Å—è –Ω–∞ –±–æ–ª—å—à–∏–µ S (–Ω–∞–ø—Ä–∏–º–µ—Ä, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ Atari: 256^{84√ó84√ó4} —Å–æ—Å—Ç–æ—è–Ω–∏–π!).

**–†–µ—à–µ–Ω–∏–µ ‚Äî Deep Q-Network (DQN):**  
–ê–ø–ø—Ä–æ–∫—Å–∏–º–∏—Ä–æ–≤–∞—Ç—å Q(s, a) –Ω–µ–π—Ä–æ—Å–µ—Ç—å—é: Q(s, a; Œ∏).

**Update:**
```
Œ∏ ‚Üê Œ∏ + Œ± ¬∑ [r + Œ≥ ¬∑ max Q(s', a'; Œ∏^-) - Q(s, a; Œ∏)] ¬∑ ‚àá_Œ∏ Q(s, a; Œ∏)
                          a'
```

–≥–¥–µ Œ∏^- ‚Äî **target network** (frozen parameters, –æ–±–Ω–æ–≤–ª—è—é—Ç—Å—è –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏).

### 5) –°—Ä–∞–≤–Ω–µ–Ω–∏–µ RL –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤

| –ê–ª–≥–æ—Ä–∏—Ç–º | Model-free | On/Off-policy | Value/Policy-based | Continuous actions | –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å |
|----------|-----------|--------------|-------------------|-------------------|-------------------|
| **Q-learning** | ‚úÖ | Off-policy | Value-based (Q*) | ‚ùå | –î–∏—Å–∫—Ä–µ—Ç–Ω—ã–µ –¥–µ–π—Å—Ç–≤–∏—è, —Ç–∞–±–ª–∏—á–Ω—ã–π |
| **DQN** | ‚úÖ | Off-policy | Value-based | ‚ùå | –î–∏—Å–∫—Ä–µ—Ç–Ω—ã–µ –¥–µ–π—Å—Ç–≤–∏—è, –±–æ–ª—å—à–∏–µ S (Atari) |
| **SARSA** | ‚úÖ | On-policy | Value-based | ‚ùå | –î–∏—Å–∫—Ä–µ—Ç–Ω—ã–µ, –Ω—É–∂–µ–Ω on-policy |
| **Actor-Critic (A2C, PPO)** | ‚úÖ | On-policy | Policy-based + Value | ‚úÖ | –ù–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–µ –¥–µ–π—Å—Ç–≤–∏—è, —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å |
| **DDPG** | ‚úÖ | Off-policy | Actor-Critic | ‚úÖ | –ù–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–µ –¥–µ–π—Å—Ç–≤–∏—è, –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è policy |

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:**
- **–ú–∞–ª—ã–µ –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã–µ MDP**: —Ç–∞–±–ª–∏—á–Ω—ã–π Q-learning
- **–ë–æ–ª—å—à–∏–µ –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã–µ (Atari)**: DQN
- **–ù–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–µ –¥–µ–π—Å—Ç–≤–∏—è**: PPO, SAC, DDPG

---

---

## üìù –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –∫–æ–¥ (Q-learning)

### –ü—Ä–æ—Å—Ç–æ–π –ø—Ä–∏–º–µ—Ä: Grid World (–ü—Å–µ–≤–¥–æ–∫–æ–¥ –ª–æ–≥–∏–∫–∏)

```python
import numpy as np

# 1. –°—Ä–µ–¥–∞ (Environment)
# 0 1 2
# 3 4 5  (5 - –¶–µ–ª—å/Reward)
n_states = 6
n_actions = 4 # Up, Down, Left, Right
goal_state = 5

# 2. Q-Table (–¢–∞–±–ª–∏—Ü–∞ —Ü–µ–Ω–Ω–æ—Å—Ç–∏ –¥–µ–π—Å—Ç–≤–∏–π)
# –°—Ç—Ä–æ–∫–∏ = –°–æ—Å—Ç–æ—è–Ω–∏—è, –°—Ç–æ–ª–±—Ü—ã = –î–µ–π—Å—Ç–≤–∏—è
Q = np.zeros((n_states, n_actions))

# 3. –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã
gamma = 0.8    # Discount factor (–≤–∞–∂–Ω–æ—Å—Ç—å –±—É–¥—É—â–µ–≥–æ)
alpha = 0.1    # Learning rate (—Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è)
epsilon = 0.1  # Exploration rate (–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —Å–ª—É—á–∞–π–Ω–æ–≥–æ —à–∞–≥–∞)

# 4. Q-learning –∞–ª–≥–æ—Ä–∏—Ç–º (—É–ø—Ä–æ—â–µ–Ω–Ω–æ)
for episode in range(100):
    state = 0 # –°—Ç–∞—Ä—Ç
    
    while state != goal_state:
        # A. –í—ã–±–æ—Ä –¥–µ–π—Å—Ç–≤–∏—è (Epsilon-Greedy)
        if np.random.rand() < epsilon:
            action = np.random.randint(n_actions) # –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ (Exploration)
        else:
            action = np.argmax(Q[state])          # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ (Exploitation)
            
        # B. –®–∞–≥ —Å—Ä–µ–¥—ã
        # (–≤ —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ —ç—Ç–æ –¥–µ–ª–∞–µ—Ç env.step(action))
        if action == 3 and state == 0: next_state = 1 # –ü—Ä–∏–º–µ—Ä transitions
        else: next_state = state # –û—Å—Ç–∞–ª–∏—Å—å –Ω–∞ –º–µ—Å—Ç–µ (—Å—Ç–µ–Ω–∞)
        
        reward = 1 if next_state == goal_state else 0
        
        # C. –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ Q-function (–£—Ä–∞–≤–Ω–µ–Ω–∏–µ –ë–µ–ª–ª–º–∞–Ω–∞)
        # Q_new = Q_old + alpha * (Reward + gamma * max(Q_next) - Q_old)
        
        best_next_action = np.argmax(Q[next_state])
        td_target = reward + gamma * Q[next_state][best_next_action]
        td_error = td_target - Q[state][action]
        
        Q[state][action] = Q[state][action] + alpha * td_error
        
        state = next_state # –ü–µ—Ä–µ—Ö–æ–¥
```

### –¢–∞–±–ª–∏—Ü–∞: –û—Å–Ω–æ–≤–Ω—ã–µ –ø–æ–Ω—è—Ç–∏—è RL

| –¢–µ—Ä–º–∏–Ω | –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ | –ü—Ä–∏–º–µ—Ä –∏–∑ –∂–∏–∑–Ω–∏ (–®–∞—Ö–º–∞—Ç—ã) |
|--------|-------------|---------------------------|
| **Agent** | –¢–æ—Ç, –∫—Ç–æ –ø—Ä–∏–Ω–∏–º–∞–µ—Ç —Ä–µ—à–µ–Ω–∏—è | –®–∞—Ö–º–∞—Ç–∏—Å—Ç |
| **Environment** | –ú–∏—Ä, –≤ –∫–æ—Ç–æ—Ä–æ–º –∂–∏–≤–µ—Ç –∞–≥–µ–Ω—Ç | –®–∞—Ö–º–∞—Ç–Ω–∞—è –¥–æ—Å–∫–∞ –∏ –ø—Ä–∞–≤–∏–ª–∞ |
| **State (S)** | –¢–µ–∫—É—â–∞—è —Å–∏—Ç—É–∞—Ü–∏—è | –†–∞—Å–ø–æ–ª–æ–∂–µ–Ω–∏–µ —Ñ–∏–≥—É—Ä |
| **Action (A)** | –î–µ–π—Å—Ç–≤–∏–µ –∞–≥–µ–Ω—Ç–∞ | –•–æ–¥ –∫–æ–Ω–µ–º E2-E4 |
| **Reward (R)** | –ù–∞–≥—Ä–∞–¥–∞ –∑–∞ –¥–µ–π—Å—Ç–≤–∏–µ (+/-) | +1 (–ü–æ–±–µ–¥–∞), -1 (–ü–æ—Ä–∞–∂–µ–Ω–∏–µ), 0 (–ù–∏—á—å—è) |
| **Policy (œÄ)** | –°—Ç—Ä–∞—Ç–µ–≥–∏—è (–∫–∞—Ä—Ç–∞ –¥–µ–π—Å—Ç–≤–∏–π) | –ö–Ω–∏–≥–∞ –¥–µ–±—é—Ç–æ–≤ (–≤ —Ç–∞–∫–æ–π —Å–∏—Ç—É–∞—Ü–∏–∏ —Ö–æ–¥–∏ —Ç–∞–∫) |
| **Episode** | –û–¥–∏–Ω –ø—Ä–æ—Ö–æ–¥ –æ—Ç –Ω–∞—á–∞–ª–∞ –¥–æ –∫–æ–Ω—Ü–∞ | –û–¥–Ω–∞ –ø–∞—Ä—Ç–∏—è |

---

## üéØ Q&A –¥–ª—è —ç–∫–∑–∞–º–µ–Ω–∞

**Q1: –ß—Ç–æ —Ç–∞–∫–æ–µ Exploration vs Exploitation Trade-off?**
> –î–∏–ª–µ–º–º–∞ –∞–≥–µ–Ω—Ç–∞:
> - **Exploration (–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ):** –ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –Ω–æ–≤–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ, —á—Ç–æ–±—ã —É–∑–Ω–∞—Ç—å, –≤–¥—Ä—É–≥ –æ–Ω–æ –ª—É—á—à–µ (—Ä–∏—Å–∫ –ø—Ä–æ–∏–≥—Ä–∞—Ç—å, –Ω–æ —à–∞–Ω—Å –Ω–∞–π—Ç–∏ –∫–ª–∞–¥).
> - **Exploitation (–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ):** –î–µ–ª–∞—Ç—å —Ç–æ, —á—Ç–æ —É–∂–µ –ø—Ä–∏–Ω–æ—Å–∏—Ç –Ω–∞–≥—Ä–∞–¥—É (—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å, –Ω–æ –º–æ–∂–Ω–æ —É–ø—É—Å—Ç–∏—Ç—å –ª—É—á—à—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏).
> Epsilon-greedy —Ä–µ—à–∞–µ—Ç —ç—Ç–æ: —Å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é epsilon –∏—Å—Å–ª–µ–¥—É–µ–º, –∏–Ω–∞—á–µ ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º –ª—É—á—à–µ–µ.

**Q2: –ß—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç Discount Factor (Gamma)?**
> Gamma (–æ—Ç 0 –¥–æ 1) –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç, –Ω–∞—Å–∫–æ–ª—å–∫–æ –∞–≥–µ–Ω—Ç—É –≤–∞–∂–Ω–æ –±—É–¥—É—â–µ–µ.
> - **Gamma = 0:** –ê–≥–µ–Ω—Ç "–±–ª–∏–∑–æ—Ä—É–∫–∏–π", –≤–æ–ª–Ω—É–µ—Ç —Ç–æ–ª—å–∫–æ –Ω–∞–≥—Ä–∞–¥–∞ –ø—Ä—è–º–æ —Å–µ–π—á–∞—Å.
> - **Gamma = 0.99:** –ê–≥–µ–Ω—Ç "–¥–∞–ª—å–Ω–æ–≤–∏–¥–Ω—ã–π", –≥–æ—Ç–æ–≤ —Ç–µ—Ä–ø–µ—Ç—å —É–±—ã—Ç–∫–∏ —Å–µ–π—á–∞—Å —Ä–∞–¥–∏ –±–æ–ª—å—à–æ–π –Ω–∞–≥—Ä–∞–¥—ã –ø–æ—Ç–æ–º (—Å—Ç—Ä–∞—Ç–µ–≥–∏—è).

**Q3: –í —á–µ–º —Ä–∞–∑–Ω–∏—Ü–∞ –º–µ–∂–¥—É Model-Based –∏ Model-Free RL?**
> **Model-Based:** –ê–≥–µ–Ω—Ç —É—á–∏—Ç –º–æ–¥–µ–ª—å –º–∏—Ä–∞ (–µ—Å–ª–∏ —è —Å–¥–µ–ª–∞—é —à–∞–≥ –≤–ø–µ—Ä–µ–¥, –∫—É–¥–∞ —è –ø–æ–ø–∞–¥—É?). –ü—Ä–∏–º–µ—Ä: –ü–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –º–∞—Ä—à—Ä—É—Ç–∞ –≤ –≥–æ–ª–æ–≤–µ.
> **Model-Free (Q-learning):** –ê–≥–µ–Ω—Ç –Ω–µ –∑–Ω–∞–µ—Ç —Ñ–∏–∑–∏–∫—É –º–∏—Ä–∞, –æ–Ω –ø—Ä–æ—Å—Ç–æ –∑–∞–ø–æ–º–∏–Ω–∞–µ—Ç "–≤ —ç—Ç–æ–π —Å–∏—Ç—É–∞—Ü–∏–∏ –Ω–∞–∂–∞—Ç—å –∫–Ω–æ–ø–∫—É –ê ‚Äî —ç—Ç–æ —Ö–æ—Ä–æ—à–æ". –ü—Ä–∏–º–µ—Ä: –ï–∑–¥–∞ –Ω–∞ –≤–µ–ª–æ—Å–∏–ø–µ–¥–µ (–º—ã—à–µ—á–Ω–∞—è –ø–∞–º—è—Ç—å).

**Q4: –ß—Ç–æ —Ç–∞–∫–æ–µ Policy?**
> –≠—Ç–æ "–º–æ–∑–≥" –∞–≥–µ–Ω—Ç–∞. –§—É–Ω–∫—Ü–∏—è –∏–ª–∏ —Ç–∞–±–ª–∏—Ü–∞, –∫–æ—Ç–æ—Ä–∞—è –≥–æ–≤–æ—Ä–∏—Ç: "–ï—Å–ª–∏ —Ç—ã –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–∏ S, –¥–µ–ª–∞–π –¥–µ–π—Å—Ç–≤–∏–µ A". –¶–µ–ª—å RL ‚Äî –Ω–∞–π—Ç–∏ –æ–ø—Ç–∏–º–∞–ª—å–Ω—É—é Policy.

---

## –†–µ–∑—é–º–µ –º–æ–¥—É–ª—è

Reinforcement Learning ‚Äî **–æ–±—É—á–µ–Ω–∏–µ —á–µ—Ä–µ–∑ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ**:
- **MDP**: —Ñ–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è sequential decision making, Markov property, reward, discount factor
- **Bellman Equation**: —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ–µ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ –¥–ª—è value functions, –æ—Å–Ω–æ–≤–∞ DP –∏ RL
- **Q-learning**: model-free, off-policy TD learning, exploration/exploitation, DQN –¥–ª—è –±–æ–ª—å—à–∏—Ö –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤

RL ‚Äî —ç—Ç–æ –±—É–¥—É—â–µ–µ AI –¥–ª—è –∑–∞–¥–∞—á, –≥–¥–µ –Ω–µ—Ç —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –Ω–æ –µ—Å—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –ø—Ä–æ–± –∏ –æ—à–∏–±–æ–∫!

---

## –û–±—â–µ–µ —Ä–µ–∑—é–º–µ –≤—Å–µ—Ö 9 –º–æ–¥—É–ª–µ–π

–í—ã –∏–∑—É—á–∏–ª–∏ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ Machine Learning –æ—Ç –æ—Å–Ω–æ–≤ –¥–æ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö –º–µ—Ç–æ–¥–æ–≤:

1. **–û—Å–Ω–æ–≤—ã**: k-NN, OLS, Bias-Variance, CV ‚Äî —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç –ø–æ–Ω–∏–º–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π
2. **–õ–∏–Ω–µ–π–Ω—ã–µ –º–æ–¥–µ–ª–∏**: —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è, LogReg, SVM, MLE ‚Äî —Ä–∞–±–æ—á–∏–µ –ª–æ—à–∞–¥–∫–∏ –∏–Ω–¥—É—Å—Ç—Ä–∏–∏
3. **–ú–µ—Ç—Ä–∏–∫–∏**: –ø—Ä–∞–≤–∏–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –∑–∞–¥–∞—á
4. **–ë–∞–π–µ—Å**: –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω—ã–π –ø–æ–¥—Ö–æ–¥, generative vs discriminative
5. **–ê–Ω—Å–∞–º–±–ª–∏**: Random Forest, XGBoost ‚Äî state-of-the-art –¥–ª—è —Ç–∞–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
6. **–ù–µ–π—Ä–æ—Å–µ—Ç–∏**: backprop, –∞–∫—Ç–∏–≤–∞—Ü–∏–∏, –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è, –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä—ã ‚Äî –æ—Å–Ω–æ–≤–∞ deep learning
7. **Computer Vision**: —Å–≤–µ—Ä—Ç–∫–∏, pooling, dropout, batch norm ‚Äî —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ CNN
8. **Unsupervised**: –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è, PCA ‚Äî –ø–æ–∏—Å–∫ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –±–µ–∑ –º–µ—Ç–æ–∫
9. **RL**: MDP, Bellman, Q-learning ‚Äî –æ–±—É—á–µ–Ω–∏–µ —á–µ—Ä–µ–∑ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ

**–≠—Ç–∏ –∑–Ω–∞–Ω–∏—è –ø–æ–∫—Ä—ã–≤–∞—é—Ç 90% ML-–∑–∞–¥–∞—á –≤ –∏–Ω–¥—É—Å—Ç—Ä–∏–∏ –∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è—Ö. –£—Å–ø–µ—Ö–æ–≤ –Ω–∞ —ç–∫–∑–∞–º–µ–Ω–µ –∏ —Å–æ–±–µ—Å–µ–¥–æ–≤–∞–Ω–∏–∏!**
