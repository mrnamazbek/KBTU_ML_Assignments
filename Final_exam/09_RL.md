# Модуль 9: Reinforcement Learning (Обучение с подкреплением)

## 9.1 MDP (Markov Decision Process)

### 1) Интуиция и суть

**Простыми словами (ELI5):**  
Ты играешь в видеоигру:
- **Состояние (state)**: где ты находишься на карте, сколько здоровья, какие предметы
- **Действие (action)**: что ты делаешь (иди вперед, атакуй, возьми предмет)
- **Награда (reward)**: очки за убийство монстра (+10), урон от врага (-5)
- **Цель**: выбирать действия так, чтобы **максимизировать суммарные очки** за всю игру

MDP — это математическая модель такого процесса принятия решений.

**Академически:**  
MDP (Markov Decision Process) — это формальная модель **sequential decision making** в стохастической среде. Ключевое свойство — **Markov property**: будущее зависит только от текущего состояния, а не от истории.

**Физический смысл:**  
Агент взаимодействует со средой, выбирая действия для максимизации **cumulative reward**.

### 2) Зачем и когда (Business Value)

**Какую проблему решает:**
- **Optimal decision making**: как действовать оптимально в условиях неопределенности
- **Long-term planning**: учет отложенных наград (delayed rewards)
- **Trade-off exploration/exploitation**: исследовать новое vs использовать известное

**Где это индустриальный стандарт:**
- **Игры**: AlphaGo, шахматы, покер
- **Робототехника**: управление роботами, дронами
- **Финансы**: алгоритмическая торговля, управление портфелем
- **Рекомендательные системы**: последовательные рекомендации (следующее видео на YouTube)
- **Healthcare**: персонализированное лечение (adaptive clinical trials)

**Когда применять нельзя:**
- ❌ Нет четкой reward function (задача supervised learning)
- ❌ Состояния не Markov (нужна рекуррентность — POMDP)

### 3) Математическое ядро

**MDP определяется tuple (S, A, P, R, γ):**

- **S** — множество состояний (states)
- **A** — множество действий (actions)
- **P(s'|s, a)** — transition probability: вероятность перехода в s' из s при действии a
  ```
  P(s_{t+1} = s' | s_t = s, a_t = a)
  ```
- **R(s, a, s')** — reward function: награда за переход
- **γ ∈ [0, 1]** — discount factor: насколько ценим будущие награды

**Markov Property:**
```
P(s_{t+1}, r_{t+1} | s_t, a_t, s_{t-1}, a_{t-1}, ..., s_0, a_0) = P(s_{t+1}, r_{t+1} | s_t, a_t)
```

Будущее зависит только от текущего состояния и действия.

**Policy (стратегия):**
```
π(a|s) = P(a_t = a | s_t = s)
```

Вероятность выбрать действие a в состоянии s.

**Цель:**  
Найти оптимальную policy π*, максимизирующую **expected cumulative discounted reward**:
```
G_t = r_{t+1} + γ·r_{t+2} + γ²·r_{t+3} + ... = Σ γ^k · r_{t+k+1}
                                                 k=0
```

**Value Function:**

**State-value function V^π(s):**
```
V^π(s) = E_π[G_t | s_t = s]
```
Ожидаемая суммарная награда, начиная с s и следуя π.

**Action-value function Q^π(s, a):**
```
Q^π(s, a) = E_π[G_t | s_t = s, a_t = a]
```
Ожидаемая награда, начиная с s, сделав действие a, затем следуя π.

### 4) Middle-level нюансы

**Проблема 1: Discount Factor γ**

**γ = 0:**  
Агент "близорук", максимизирует только мгновенную награду.

**γ → 1:**  
Агент ценит будущие награды почти так же, как текущие.

**Зачем γ < 1:**
1. Математическая сходимость (бесконечные суммы)
2. Неопределенность будущего (лучше получить награду сейчас)
3. Конечный горизонт в реальных задачах

**Эмпирически:**  
γ = 0.9-0.99 (стандарт).

**Проблема 2: Optimal Policy**

**Optimal state-value:**
```
V*(s) = max V^π(s)
        π
```

**Optimal action-value:**
```
Q*(s, a) = max Q^π(s, a)
           π
```

**Связь:**
```
V*(s) = max Q*(s, a)
        a
```

**Optimal policy:**
```
π*(s) = argmax Q*(s, a)
        a
```

Жадная policy относительно Q*.

**Проблема 3: Exploration vs Exploitation**

**Exploitation:**  
Использовать лучшее известное действие (argmax Q).

**Exploration:**  
Пробовать другие действия (может найти что-то лучше!).

**Trade-off:**  
Слишком много exploitation → застрять в sub-optimal policy.  
Слишком много exploration → никогда не использовать найденное.

**Решение:**
- **ε-greedy**: с вероятностью ε выбрать случайное действие, иначе argmax Q
- **Softmax (Boltzmann)**: выбирать действия пропорционально exp(Q(s,a)/τ)
- **UCB (Upper Confidence Bound)**: выбирать действия с учетом неопределенности

### 5) Сравнение RL vs Supervised Learning

| Аспект | Supervised Learning | Reinforcement Learning |
|--------|-------------------|----------------------|
| **Обратная связь** | Правильные метки (y) | Награды (r) |
| **Данные** | Фиксированный dataset | Агент генерирует данные через взаимодействие |
| **Цель** | Предсказать y для x | Максимизировать cumulative reward |
| **Exploration** | Нет | ✅ Критично |
| **Delayed rewards** | Нет | ✅ Да |

---

## 9.2 Bellman Equation (Уравнение Беллмана)

### 1) Интуиция и суть

**Простыми словами (ELI5):**  
Ценность состояния = награда за текущий шаг + ценность следующего состояния (с учетом discount).

Например, ценность "быть на работе" = зарплата сегодня + ценность "быть дома вечером" (отдых → завтра снова на работе с новыми силами).

**Академически:**  
Bellman Equation — это **рекурсивное соотношение**, связывающее value function в текущем состоянии с value в следующих состояниях. Основа для **Dynamic Programming** и многих RL алгоритмов.

**Физический смысл:**  
Оптимальное решение в s зависит от оптимальных решений в состояниях, достижимых из s.

### 2) Зачем и когда (Business Value)

**Какую проблему решает:**
- **Вычисление V^π и Q^π**: оценка стратегии
- **Нахождение V* и Q***: оптимальная стратегия
- **Основа алгоритмов**: Value Iteration, Policy Iteration, Q-learning, TD-learning

**Где используется:**
- Везде в RL!

### 3) Математическое ядро

**Bellman Expectation Equation (для произвольной policy π):**

**Для V^π:**
```
V^π(s) = Σ π(a|s) · Σ P(s'|s, a) · [R(s, a, s') + γ · V^π(s')]
         a          s'
```

**Разбор:**
- π(a|s): вероятность выбрать a в s
- P(s'|s, a): вероятность перехода в s'
- R + γ·V^π(s'): мгновенная награда + дисконтированная ценность следующего состояния

**Для Q^π:**
```
Q^π(s, a) = Σ P(s'|s, a) · [R(s, a, s') + γ · Σ π(a'|s') · Q^π(s', a')]
            s'                                  a'
```

**Bellman Optimality Equation (для оптимальной policy):**

**Для V*:**
```
V*(s) = max Σ P(s'|s, a) · [R(s, a, s') + γ · V*(s')]
        a   s'
```

Вместо усреднения по π(a|s), просто берем **max** по действиям.

**Для Q*:**
```
Q*(s, a) = Σ P(s'|s, a) · [R(s, a, s') + γ · max Q*(s', a')]
           s'                                  a'
```

**Связь V* и Q*:**
```
V*(s) = max Q*(s, a)
        a
```

### 4) Middle-level нюансы

**Проблема 1: Решение Bellman Equations**

**Bellman Expectation:**  
Система линейных уравнений (если S конечно) → можно решить напрямую:
```
V = R + γ · P · V
V = (I - γP)^(-1) · R
```

**Проблема:** O(|S|³) — не масштабируется.

**Iterative методы:**
- **Value Iteration**
- **Policy Iteration**
- **Temporal Difference** (TD) learning

**Bellman Optimality:**  
Нелинейное (из-за max) → используем **Value Iteration** или **Q-learning**.

**Проблема 2: Value Iteration**

**Алгоритм:**

Инициализация: V_0(s) = 0 для всех s

Повторять:
```
V_{k+1}(s) = max Σ P(s'|s, a) · [R(s, a, s') + γ · V_k(s')]
             a   s'
```

До сходимости: max_s |V_{k+1}(s) - V_k(s)| < ε

**Теорема:**  
V_k → V* при k → ∞ (contraction mapping).

**Policy Extraction:**
```
π*(s) = argmax Σ P(s'|s, a) · [R(s, a, s') + γ · V*(s')]
        a      s'
```

**Проблема 3: Policy Iteration**

**Алгоритм:**

1. Инициализация: случайная policy π_0
2. Повторять:
   - **Policy Evaluation**: решить Bellman Expectation для V^π
   - **Policy Improvement**:
     ```
     π_{k+1}(s) = argmax Q^π_k(s, a)
                  a
     ```
3. До сходимости: π_k = π_{k-1}

**Сравнение с Value Iteration:**
- **Policy Iteration**: меньше итераций, но каждая итерация дороже (полная policy evaluation)
- **Value Iteration**: больше итераций, но каждая дешевле

### 5) Сравнение методов решения MDP

| Метод | Требует модель P, R | Сходимость | Вычисления per iteration | Когда использовать |
|-------|-------------------|-----------|-------------------------|-------------------|
| **Value Iteration** | ✅ Да | ✅ Гарантирована | O(\|S\|²·\|A\|) | Малые MDP, известна модель |
| **Policy Iteration** | ✅ Да | ✅ Гарантирована (быстрее VI) | O(\|S\|³) + O(\|S\|²·\|A\|) | Малые MDP |
| **Q-learning** | ❌ Нет (model-free) | ⚠️ Стохастическая | O(1) per step | Большие MDP, неизвестная модель |

---

## 9.3 Q-learning

### 1) Интуиция и суть

**Простыми словами (ELI5):**  
Ты учишься играть в новую игру, не зная правил (модели среды). Ты:
1. Пробуешь действия
2. Получаешь награды
3. **Обновляешь оценку** Q(s, a): "насколько хорошо сделать a в s"
4. Со временем Q → Q* (оптимальная оценка)

Q-learning учится **методом проб и ошибок** (trial and error).

**Академически:**  
Q-learning — это **model-free, off-policy** алгоритм **Temporal Difference** learning, который учится оптимальной action-value function Q* путем итеративного обновления оценок на основе опыта взаимодействия со средой.

**Физический смысл:**  
Bootstrapping: используем текущую оценку Q для обновления самой Q.

### 2) Зачем и когда (Business Value)

**Какую проблему решает:**
- **Model-free**: не нужно знать P(s'|s, a) и R(s, a, s')
- **Off-policy**: учится от любых данных (даже от случайных действий)
- **Асимптотически оптимально**: Q → Q* (при выполнении условий)

**Где используется:**
- **Игры**: Atari (с Deep Q-Learning)
- **Робототехника**: когда модель среды неизвестна
- **Автономные авто**: обучение на симуляторе + реальных данных

**Когда применять нельзя:**
- ❌ Непрерывные действия напрямую (нужен DQN с аппроксимацией или Actor-Critic)
- ❌ Огромные пространства состояний без аппроксимации (нужна нейросеть → DQN)

### 3) Математическое ядро

**Q-learning Update Rule:**

После перехода (s, a, r, s'):
```
Q(s, a) ← Q(s, a) + α · [r + γ · max Q(s', a') - Q(s, a)]
                                    a'
```

где:
- **α** — learning rate (размер шага)
- **r + γ · max Q(s', a')** — **TD target** (temporal difference target)
- **r + γ · max Q(s', a') - Q(s, a)** — **TD error** (разница между новой оценкой и старой)

**Разбор:**
- Текущая оценка: Q(s, a)
- Новая информация: r + γ · max Q(s', a') (мгновенная награда + best Q в следующем состоянии)
- Обновляем Q в сторону новой информации

**Off-policy:**  
Обновление использует **max** (жадное), независимо от того, какое действие a реально выбрано (может быть ε-greedy для exploration).

**Алгоритм Q-learning:**

1. Инициализация: Q(s, a) = 0 для всех (s, a)
2. Для каждого эпизода:
   - Начать с s_0
   - Для каждого шага t:
     - Выбрать a по ε-greedy относительно Q:
       ```
       a = {argmax Q(s, a'),  с вероятностью 1 - ε;
            random,           с вероятностью ε}
       ```
     - Выполнить a, получить r, s'
     - Обновить Q:
       ```
       Q(s, a) ← Q(s, a) + α · [r + γ · max Q(s', a') - Q(s, a)]
       ```
     - s ← s'
   - До завершения эпизода

### 4) Middle-level нюансы

**Проблема 1: Convergence (сходимость)**

**Теорема (Watkins & Dayan, 1992):**  
Q-learning сходится к Q* при выполнении условий:
1. Все (s, a) пары посещаются бесконечно часто
2. Learning rate α_t удовлетворяет:
   ```
   Σ α_t = ∞,  Σ α_t² < ∞
   ```
   Например: α_t = 1/t^0.8

**На практике:**  
Используют фиксированный α = 0.01-0.1 → стохастическая сходимость к окрестности Q*.

**Проблема 2: Exploration vs Exploitation**

**ε-greedy:**
- ε = 1.0 в начале (полное исследование)
- ε → 0.1 или 0.01 к концу (преимущественно exploitation)

**Decay schedule:**
```
ε_t = max(ε_min, ε_start · decay^t)
```

**Проблема 3: Experience Replay (для Deep Q-Learning)**

**Проблема vanilla Q-learning с нейросетями:**
- Последовательные состояния сильно коррелированны → нестабильное обучение
- Забывание предыдущего опыта (catastrophic forgetting)

**Решение — Experience Replay:**

1. Сохранять transitions (s, a, r, s') в **replay buffer**
2. Обучать сеть на **случайных mini-batches** из buffer

**Эффект:**
- Декорреляция данных
- Переиспользование опыта (sample efficiency)

**Проблема 4: Q-table vs Function Approximation**

**Q-table (табличный Q-learning):**
```
Q = матрица |S| × |A|
```

**Проблема:**  
Не масштабируется на большие S (например, изображения в Atari: 256^{84×84×4} состояний!).

**Решение — Deep Q-Network (DQN):**  
Аппроксимировать Q(s, a) нейросетью: Q(s, a; θ).

**Update:**
```
θ ← θ + α · [r + γ · max Q(s', a'; θ^-) - Q(s, a; θ)] · ∇_θ Q(s, a; θ)
                          a'
```

где θ^- — **target network** (frozen parameters, обновляются периодически).

### 5) Сравнение RL алгоритмов

| Алгоритм | Model-free | On/Off-policy | Value/Policy-based | Continuous actions | Когда использовать |
|----------|-----------|--------------|-------------------|-------------------|-------------------|
| **Q-learning** | ✅ | Off-policy | Value-based (Q*) | ❌ | Дискретные действия, табличный |
| **DQN** | ✅ | Off-policy | Value-based | ❌ | Дискретные действия, большие S (Atari) |
| **SARSA** | ✅ | On-policy | Value-based | ❌ | Дискретные, нужен on-policy |
| **Actor-Critic (A2C, PPO)** | ✅ | On-policy | Policy-based + Value | ✅ | Непрерывные действия, стабильность |
| **DDPG** | ✅ | Off-policy | Actor-Critic | ✅ | Непрерывные действия, детерминированная policy |

**Рекомендации:**
- **Малые дискретные MDP**: табличный Q-learning
- **Большие дискретные (Atari)**: DQN
- **Непрерывные действия**: PPO, SAC, DDPG

---

## Резюме модуля

Reinforcement Learning — **обучение через взаимодействие**:
- **MDP**: формализация sequential decision making, Markov property, reward, discount factor
- **Bellman Equation**: рекурсивное соотношение для value functions, основа DP и RL
- **Q-learning**: model-free, off-policy TD learning, exploration/exploitation, DQN для больших пространств

RL — это будущее AI для задач, где нет размеченных данных, но есть возможность проб и ошибок!

---

## Общее резюме всех 9 модулей

Вы изучили фундаментальные концепции Machine Learning от основ до продвинутых методов:

1. **Основы**: k-NN, OLS, Bias-Variance, CV — фундамент понимания моделей
2. **Линейные модели**: регуляризация, LogReg, SVM, MLE — рабочие лошадки индустрии
3. **Метрики**: правильная оценка моделей для разных задач
4. **Байес**: вероятностный подход, generative vs discriminative
5. **Ансамбли**: Random Forest, XGBoost — state-of-the-art для табличных данных
6. **Нейросети**: backprop, активации, инициализация, оптимизаторы — основа deep learning
7. **Computer Vision**: свертки, pooling, dropout, batch norm — современные CNN
8. **Unsupervised**: кластеризация, PCA — поиск структуры без меток
9. **RL**: MDP, Bellman, Q-learning — обучение через взаимодействие

**Эти знания покрывают 90% ML-задач в индустрии и исследованиях. Успехов на экзамене и собеседовании!**
