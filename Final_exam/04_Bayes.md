# Модуль 4: Байесовские методы

## 4.1 Теорема Байеса в Machine Learning

### 1) Интуиция и суть

**Простыми словами (ELI5):**  
Ты врач и видишь симптом (кашель). Как узнать вероятность болезни (грипп)?
- **До теста** у тебя есть базовая вероятность (prior): "грипп сейчас у 5% населения"
- **После наблюдения симптома** (кашель) ты обновляешь вероятность (posterior): "при гриппе кашель в 80% случаев, а при обычной простуде только в 20%"

Теорема Байеса — это **математическое правило обновления убеждений** на основе новой информации.

**Академически:**  
Теорема Байеса устанавливает связь между **априорной вероятностью (prior)** гипотезы, **правдоподобием (likelihood)** данных при этой гипотезе и **апостериорной вероятностью (posterior)** гипотезы после наблюдения данных.

**Физический смысл:**  
Это способ **логического вывода в условиях неопределенности**: как рационально изменить свои убеждения при получении новых данных.

### 2) Зачем и когда (Business Value)

**Какую проблему решает:**
- **Классификация** с вероятностным выводом
- **Малые данные**: использование априорных знаний (domain expertise)
- **Онлайн обучение**: постепенное обновление модели при поступлении новых данных
- **Интерпретация**: явная модель зависимостей P(X|Y)

**Где используется:**
- **Медицина**: диагностика на основе симптомов
- **Spam filtering**: классификация писем (Naive Bayes — классика!)
- **Рекомендательные системы**: Bayesian Personalized Ranking
- **A/B тестирование**: Bayesian A/B test вместо t-test
- **NLP**: language modeling, text classification

**Когда применять нельзя:**
- ❌ Нужна высокая точность при больших данных (deep learning лучше)
- ❌ Сложные зависимости между признаками (Naive Bayes предполагает независимость!)

### 3) Математическое ядро

**Теорема Байеса:**
```
P(H|D) = P(D|H) · P(H) / P(D)
```

где:
- **H** — гипотеза (hypothesis), например, класс y
- **D** — данные (data), например, признаки x
- **P(H|D)** — **posterior** (апостериорная): вероятность гипотезы после наблюдения данных
- **P(D|H)** — **likelihood** (правдоподобие): вероятность данных при данной гипотезе
- **P(H)** — **prior** (априорная): вероятность гипотезы до наблюдения данных
- **P(D)** — **evidence** (свидетельство, нормировочная константа)

**Полная формула через закон полной вероятности:**
```
P(H|D) = P(D|H) · P(H) / [Σ P(D|H_i) · P(H_i)]
```

**Для** классификации:**
```
P(y|x) = P(x|y) · P(y) / P(x)
```

**Решающее правило (MAP — Maximum A Posteriori):**
```
ŷ = argmax P(y|x) = argmax P(x|y) · P(y)
      y               y
```
P(x) не зависит от y, поэтому убираем.

### 4) Middle-level нюансы

**Проблема 1: Prior vs Likelihood**

**Prior P(y):**  
Базовая вероятность класса (до наблюдения признаков).

**Оценка из данных:**
```
P(y = k) = n_k / n
```

**Likelihood P(x|y):**  
Вероятность признаков при данном классе.

**Пример:**

```
Email spam classification:
P(spam) = 0.3  (30% писем — спам)
P("viagra" | spam) = 0.8
P("viagra" | not_spam) = 0.01

Видим слово "viagra":
P(spam | "viagra") ∝ 0.8 · 0.3 = 0.24
P(not_spam | "viagra") ∝ 0.01 · 0.7 = 0.007
→ Скорее всего spam!
```

**Проблема 2: Generative vs Discriminative**

**Generative models (Bayesian approach):**  
Моделируют **совместное распределение** P(x, y) = P(x|y) · P(y).  
Могут генерировать новые примеры x из класса y.

**Примеры:**
- Naive Bayes
- Hidden Markov Models (HMM)
- Gaussian Mixture Models (GMM)

**Discriminative models:**  
Моделируют только **условное распределение** P(y|x) напрямую.  
Не могут генерировать x.

**Примеры:**
- Logistic Regression
- SVM
- Neural Networks

**Сравнение:**

| Тип | Что моделирует | Нужно для обучения | Преимущества | Недостатки |
|-----|---------------|-------------------|-------------|------------|
| **Generative** | P(x\|y) и P(y) | Больше параметров | ✅ Малые данные<br>✅ Может генерировать | ❌ Делает предположения о P(x\|y) |
| **Discriminative** | P(y\|x) напрямую | Меньше параметров | ✅ Обычно точнее при больших данных | ❌ Нужно больше данных |

**Проблема 3: Выбор Prior (Bayesian Inference)**

**Uninformative (non-informative) prior:**  
Не делаем предположений, все гипотезы равновероятны.

**Пример:**
- Uniform prior: P(θ) = const

**Informative prior:**  
Используем domain knowledge.

**Пример (регрессия с Bayesian Ridge):**
- Prior на веса: w ∼ N(0, σ²)
- Это эквивалентно L2 регуляризации!

**Проблема 4: Computational Issues**

**Числитель вычислим:**
```
P(x|y) · P(y)
```

**Знаменатель P(x) сложен:**
```
P(x) = Σ P(x|y) · P(y)
```
Требует суммирования по всем классам (ОК для малого числа классов).

**Для непрерывных x:**  
P(x) — интеграл, часто аналитически невычислим → используются приближенные методы:
- **MCMC (Markov Chain Monte Carlo)**
- **Variational Inference**

### 5) Сравнение Bayesian vs Frequentist

| Подход | Параметры | Неопределенность | Малые данные | Интерпретация |
|--------|-----------|-----------------|-------------|---------------|
| **Bayesian** | Случайные величины с распределением | ✅ Явно моделируется (posterior) | ✅ Можно использовать prior | "Вероятность гипотезы" |
| **Frequentist (MLE)** | Фиксированные (но неизвестные) | ⚠️ Через confidence intervals | ❌ Нужно много данных | "Частота при повторениях" |

---

## 4.2 Naive Bayes Classifier

### 1) Интуиция и суть

**Простыми словами (ELI5):**  
Ты определяешь, описывает ли текст "спорт" или "политику". Смотришь на каждое слово **независимо**: "мяч" → скорее спорт, "выборы" → скорее политика. Naive Bayes **наивно** предполагает, что слова независимы друг от друга (что неверно, но часто работает!).

**Академически:**  
Naive Bayes — это **generative probabilistic classifier**, основанный на теореме Байеса с **сильным (наивным) предположением о условной независимости признаков** при данном классе.

**Физический смысл:**  
Упрощение P(x|y) = P(x_1, x_2, ..., x_d | y) до произведения P(x_1|y) · P(x_2|y) · ... · P(x_d|y).

### 2) Зачем и когда (Business Value)

**Какую проблему решает:**
- **Быстрая классификация** (обучение и инференс за O(n·d))
- **Малые данные**: работает даже при n << d
- **Text classification**: естественный выбор для bag-of-words

**Где это индустриальный стандарт:**
- **Spam filtering**: классика (простой, быстрый, эффективный)
- **Sentiment analysis**: тональность текста (positive/negative)
- **Document classification**: категоризация новостей, статей
- **Medical diagnosis**: симптомы → болезнь

**Когда применять нельзя:**
- ❌ Признаки сильно коррелированы (нарушается assumption независимости)
- ❌ Нужна высокая точность при больших данных (используйте нейросети)

### 3) Математическое ядро

**Теорема Байеса для классификации:**
```
P(y|x) ∝ P(x|y) · P(y)
```

**Naive Bayes assumption (условная независимость):**
```
P(x|y) = P(x_1, x_2, ..., x_d | y) = Π P(x_j | y)
                                     j=1..d
```

**Решающее правило:**
```
ŷ = argmax [P(y) · Π P(x_j | y)]
      y           j
```

**Log-форма (для численной стабильности):**
```
ŷ = argmax [log P(y) + Σ log P(x_j | y)]
      y                j
```

**Типы Naive Bayes (в зависимости от распределения P(x_j|y)):**

1. **Gaussian Naive Bayes (для непрерывных признаков):**
   ```
   P(x_j | y) = (1/√(2πσ²_jy)) · exp(-(x_j - μ_jy)² / (2σ²_jy))
   ```
   Признак x_j при классе y ∼ N(μ_jy, σ²_jy).

2. **Multinomial Naive Bayes (для счетных данных, text):**
   ```
   P(x_j | y) = θ_jy = count(x_j, y) / Σ count(x_k, y)
   ```
   Вероятность слова x_j в документах класса y.

3. **Bernoulli Naive Bayes (для бинарных признаков):**
   ```
   P(x_j | y) = p_jy^(x_j) · (1 - p_jy)^(1 - x_j)
   ```
   x_j ∈ {0, 1} (слово есть/нет).

### 4) Middle-level нюансы

**Проблема 1: Почему "наивный"?**

**Assumption:**  
P(x_1, x_2 | y) = P(x_1 | y) · P(x_2 | y)

**Реальность (пример text):**  
Слова "горячий" и "собака" могут быть независимы, но в контексте "хот-дог" они коррелированны!

**Почему все равно работает:**
- Нам не нужны **точные вероятности**, только **правильный порядок** (ранжирование классов)
- Даже при нарушении assumption, модель часто дает **правильные предсказания**

**Проблема 2: Laplace Smoothing (сглаживание)**

**Проблема zero-frequency:**

```
Text: "buy viagra now"
Training: слово "viagra" никогда не встречалось в "not_spam".
→ P("viagra" | not_spam) = 0
→ P("buy viagra now" | not_spam) = 0  (произведение!)
→ Модель всегда предскажет spam, даже если остальные слова указывают на not_spam!
```

**Решение — Laplace Smoothing (add-α smoothing):**
```
P(x_j | y) = (count(x_j, y) + α) / (Σ count(x_k, y) + α · d)
```

Обычно α = 1 (add-one smoothing).

**Эффект:**
- Все P(x_j | y) > 0 (нет нулей)
- При больших данных α/n → 0, smoothing не влияет
- При малых данных smoothing критично!

**Проблема 3: Вероятности слишком малы (Numerical Underflow)**

**Проблема:**
```
P(y|x) ∝ P(y) · Π P(x_j | y)
```
Произведение множества малых вероятностей (например, 0.001^100) → underflow (≈ 0 в компьютере).

**Решение — Log-probabilities:**
```
log P(y|x) ∝ log P(y) + Σ log P(x_j | y)
```

Сумма логарифмов вместо произведения → численная стабильность!

**Проблема 4: Калибровка вероятностей**

Naive Bayes выдает **некалиброванные вероятности** (слишком уверенные или неуверенные).

**Причина:**  
Assumption независимости нарушается → P(y|x) не истинная вероятность.

**Решение:**
- **Platt Scaling**: обучить логистическую регрессию на выходах Naive Bayes
- Использовать Naive Bayes только для **ранжирования**, не для вероятностей

### 5) Сравнение вариантов Naive Bayes

| Вариант | Тип признаков | Пример задачи | Формула P(x_j\|y) |
|---------|--------------|--------------|------------------|
| **Gaussian NB** | Непрерывные | Классификация по физическим параметрам | N(μ, σ²) |
| **Multinomial NB** | Счетные (частоты) | Text classification (bag-of-words) | Multinomial distribution |
| **Bernoulli NB** | Бинарные {0,1} | Text (слово есть/нет) | Bernoulli(p) |

---

## 4.3 Generative vs Discriminative Models

### 1) Интуиция и суть

**Простыми словами (ELI5):**  
Ты учишься отличать кошек от собак.

- **Generative**: сначала изучаешь, "как выглядят кошки" и "как выглядят собаки" отдельно. Потом, увидев животное, сравниваешь: "больше похоже на кошку или собаку?"
- **Discriminative**: сразу учишься находить **границу**: "уши торчат → кошка, уши висят → собака". Не моделируешь, как они выглядят, только разницу.

**Академически:**  
**Generative models** моделируют совместное распределение P(x, y), затем применяют теорему Байеса для P(y|x).  
**Discriminative models** моделируют условное распределение P(y|x) напрямую.

### 2) Зачем и когда (Business Value)

**Generative:**
- ✅ **Малые данные**: можно использовать prior P(y)
- ✅ **Генерация**: можно сэмплировать новые x из P(x|y)
- ✅ **Интерпретация**: явная модель зависимостей

**Discriminative:**
- ✅ **Большие данные**: обычно точнее  
- ✅ **Меньше параметров**: не моделируем P(x|y), только границу
- ✅ **Гибкость**: можно моделировать сложные нелинейные границы

**Где используется:**

**Generative:**
- Naive Bayes (text classification)
- Hidden Markov Models (speech recognition)
- Generative Adversarial Networks (GANs) — генерация изображений

**Discriminative:**
- Logistic Regression
- SVM
- Neural Networks
- Random Forest

### 3) Математическое ядро

**Generative:**
```
P(y|x) = P(x, y) / P(x) = P(x|y) · P(y) / P(x)
```

**Обучение:**  
Оценить P(x|y) и P(y) из данных.

**Классификация:**
```
ŷ = argmax P(x|y) · P(y)
      y
```

**Discriminative:**
```
P(y|x) напрямую
```

**Обучение:**  
Максимизировать P(y|x) (например, через LogLoss).

**Классификация:**
```
ŷ = argmax P(y|x)
      y
```

### 4) Middle-level нюансы

**Когда Generative лучше:**

1. **Малые данные**: prior P(y) помогает
2. **Несбалансированные данные**: явная модель P(y) учитывает дисбаланс
3. **Нужна генерация**: например, синтез изображений

**Когда Discriminative лучше:**

1. **Большие данные**: больше гибкости, меньше assumptions
2. **Сложные зависимости**: не нужно моделировать P(x|y), только границу

**Пример:**  
Naive Bayes (generative) vs Logistic Regression (discriminative) на text classification:
- Малые данные (< 1000): Naive Bayes часто лучше
- Большие данные (> 10,000): Logistic Regression обычно точнее

### 5) Сравнение

| Аспект | Generative | Discriminative |
|--------|-----------|---------------|
| **Что моделирует** | P(x, y) = P(x\|y) · P(y) | P(y\|x) |
| **Параметры** | Больше (нужно смоделировать P(x\|y)) | Меньше |
| **Данные** | ✅ Малые данные | ✅ Большие данные |
| **Assumptions** | ⚠️ Сильные (о распределении P(x\|y)) | ✅ Меньше assumptions |
| **Генерация** | ✅ Может генерировать x | ❌ Нет |
| **Примеры** | Naive Bayes, GMM, HMM | Logistic Regression, SVM, NN |

---

## Резюме модуля

Байесовские методы — это **вероятностный подход к ML**:
- **Теорема Байеса**: обновление убеждений при новых данных, prior + likelihood → posterior
- **Naive Bayes**: быстрый классификатор с assumption независимости, Laplace smoothing
- **Generative vs Discriminative**: P(x, y) vs P(y|x), малые данные vs большие данные, генерация vs граница

Эти методы — основа для понимания вероятностного ML и современных генеративных моделей (VAE, GANs)!
