# –ú–æ–¥—É–ª—å 4: –ë–∞–π–µ—Å–æ–≤—Å–∫–∏–µ –º–µ—Ç–æ–¥—ã

> **–ó–∞—á–µ–º —ç—Ç–æ—Ç –º–æ–¥—É–ª—å?**  
> –ë–∞–π–µ—Å–æ–≤—Å–∫–∏–µ –º–µ—Ç–æ–¥—ã ‚Äî —ç—Ç–æ **"–∑–¥—Ä–∞–≤—ã–π —Å–º—ã—Å–ª, –¥–æ–≤–µ–¥–µ–Ω–Ω—ã–π –¥–æ –∏—Å—á–∏—Å–ª–µ–Ω–∏—è"** (–ü—å–µ—Ä-–°–∏–º–æ–Ω –õ–∞–ø–ª–∞—Å).
> - **–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω–æ–µ –º—ã—à–ª–µ–Ω–∏–µ**: –≤ –æ—Ç–ª–∏—á–∏–µ –æ—Ç –º–Ω–æ–≥–∏—Ö –º–æ–¥–µ–ª–µ–π, –ë–∞–π–µ—Å –¥–∞–µ—Ç –Ω–µ –ø—Ä–æ—Å—Ç–æ –æ—Ç–≤–µ—Ç "–¥–∞/–Ω–µ—Ç", –∞ "–Ω–∞—Å–∫–æ–ª—å–∫–æ —è –≤ —ç—Ç–æ–º —É–≤–µ—Ä–µ–Ω".
> - **–†–∞–±–æ—Ç–∞ —Å –∞–ø—Ä–∏–æ—Ä–Ω—ã–º–∏ –∑–Ω–∞–Ω–∏—è–º–∏**: –º—ã –º–æ–∂–µ–º —É—á–∏—Ç—ã–≤–∞—Ç—å –æ–ø—ã—Ç —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ (Priors) –¥–æ —Ç–æ–≥–æ, –∫–∞–∫ —É–≤–∏–¥–∏–º –¥–∞–Ω–Ω—ã–µ.
> - **–§—É–Ω–¥–∞–º–µ–Ω—Ç –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –ò–ò**: –ø–æ–Ω–∏–º–∞–Ω–∏–µ —Ç–æ–≥–æ, –∫–∞–∫ –º–æ–¥–µ–ª—å "–ø–æ—Ä–æ–∂–¥–∞–µ—Ç" –¥–∞–Ω–Ω—ã–µ (Generative), –≤–µ–¥–µ—Ç –∫ –ø–æ–Ω–∏–º–∞–Ω–∏—é —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∏ LLMs.

---

## 4.1 –¢–µ–æ—Ä–µ–º–∞ –ë–∞–π–µ—Å–∞ –≤ Machine Learning

### 1) –ò–Ω—Ç—É–∏—Ü–∏—è –∏ —Å—É—Ç—å

**–ü—Ä–æ—Å—Ç—ã–º–∏ —Å–ª–æ–≤–∞–º–∏ (ELI5):**  
–¢—ã –≤—Ä–∞—á –∏ –≤–∏–¥–∏—à—å —Å–∏–º–ø—Ç–æ–º (–∫–∞—à–µ–ª—å). –ö–∞–∫ —É–∑–Ω–∞—Ç—å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –±–æ–ª–µ–∑–Ω–∏ (–≥—Ä–∏–ø–ø)?
- **–î–æ —Ç–µ—Å—Ç–∞** —É —Ç–µ–±—è –µ—Å—Ç—å –±–∞–∑–æ–≤–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å (prior): "–≥—Ä–∏–ø–ø —Å–µ–π—á–∞—Å —É 5% –Ω–∞—Å–µ–ª–µ–Ω–∏—è"
- **–ü–æ—Å–ª–µ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è —Å–∏–º–ø—Ç–æ–º–∞** (–∫–∞—à–µ–ª—å) —Ç—ã –æ–±–Ω–æ–≤–ª—è–µ—à—å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å (posterior): "–ø—Ä–∏ –≥—Ä–∏–ø–ø–µ –∫–∞—à–µ–ª—å –≤ 80% —Å–ª—É—á–∞–µ–≤, –∞ –ø—Ä–∏ –æ–±—ã—á–Ω–æ–π –ø—Ä–æ—Å—Ç—É–¥–µ —Ç–æ–ª—å–∫–æ –≤ 20%"

–¢–µ–æ—Ä–µ–º–∞ –ë–∞–π–µ—Å–∞ ‚Äî —ç—Ç–æ **–º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –ø—Ä–∞–≤–∏–ª–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —É–±–µ–∂–¥–µ–Ω–∏–π** –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–æ–≤–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏.

**–ê–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏:**  
–¢–µ–æ—Ä–µ–º–∞ –ë–∞–π–µ—Å–∞ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç —Å–≤—è–∑—å –º–µ–∂–¥—É **–∞–ø—Ä–∏–æ—Ä–Ω–æ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é (prior)** –≥–∏–ø–æ—Ç–µ–∑—ã, **–ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–∏–µ–º (likelihood)** –¥–∞–Ω–Ω—ã—Ö –ø—Ä–∏ —ç—Ç–æ–π –≥–∏–ø–æ—Ç–µ–∑–µ –∏ **–∞–ø–æ—Å—Ç–µ—Ä–∏–æ—Ä–Ω–æ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é (posterior)** –≥–∏–ø–æ—Ç–µ–∑—ã –ø–æ—Å–ª–µ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö.

**–§–∏–∑–∏—á–µ—Å–∫–∏–π —Å–º—ã—Å–ª:**  
–≠—Ç–æ —Å–ø–æ—Å–æ–± **–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –≤—ã–≤–æ–¥–∞ –≤ —É—Å–ª–æ–≤–∏—è—Ö –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏**: –∫–∞–∫ —Ä–∞—Ü–∏–æ–Ω–∞–ª—å–Ω–æ –∏–∑–º–µ–Ω–∏—Ç—å —Å–≤–æ–∏ —É–±–µ–∂–¥–µ–Ω–∏—è –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö.

### 2) –ó–∞—á–µ–º –∏ –∫–æ–≥–¥–∞ (Business Value)

**–ö–∞–∫—É—é –ø—Ä–æ–±–ª–µ–º—É —Ä–µ—à–∞–µ—Ç:**
- **–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è** —Å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω—ã–º –≤—ã–≤–æ–¥–æ–º
- **–ú–∞–ª—ã–µ –¥–∞–Ω–Ω—ã–µ**: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∞–ø—Ä–∏–æ—Ä–Ω—ã—Ö –∑–Ω–∞–Ω–∏–π (domain expertise)
- **–û–Ω–ª–∞–π–Ω –æ–±—É—á–µ–Ω–∏–µ**: –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –ø—Ä–∏ –ø–æ—Å—Ç—É–ø–ª–µ–Ω–∏–∏ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
- **–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è**: —è–≤–Ω–∞—è –º–æ–¥–µ–ª—å –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π P(X|Y)

**–ì–¥–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è:**
- **–ú–µ–¥–∏—Ü–∏–Ω–∞**: –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–∏–º–ø—Ç–æ–º–æ–≤
- **Spam filtering**: –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –ø–∏—Å–µ–º (Naive Bayes ‚Äî –∫–ª–∞—Å—Å–∏–∫–∞!)
- **–†–µ–∫–æ–º–µ–Ω–¥–∞—Ç–µ–ª—å–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã**: Bayesian Personalized Ranking
- **A/B —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ**: Bayesian A/B test –≤–º–µ—Å—Ç–æ t-test
- **NLP**: language modeling, text classification

**–ö–æ–≥–¥–∞ –ø—Ä–∏–º–µ–Ω—è—Ç—å –Ω–µ–ª—å–∑—è:**
- ‚ùå –ù—É–∂–Ω–∞ –≤—ã—Å–æ–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –ø—Ä–∏ –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö (deep learning –ª—É—á—à–µ)
- ‚ùå –°–ª–æ–∂–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –º–µ–∂–¥—É –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏ (Naive Bayes –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å!)

### 3) –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —è–¥—Ä–æ

**–¢–µ–æ—Ä–µ–º–∞ –ë–∞–π–µ—Å–∞:**
$$
P(H|D) = \frac{P(D|H) \cdot P(H)}{P(D)}
$$

–≥–¥–µ:
- **H** ‚Äî –≥–∏–ø–æ—Ç–µ–∑–∞ (hypothesis), –Ω–∞–ø—Ä–∏–º–µ—Ä, –∫–ª–∞—Å—Å $y$
- **D** ‚Äî –¥–∞–Ω–Ω—ã–µ (data), –Ω–∞–ø—Ä–∏–º–µ—Ä, –ø—Ä–∏–∑–Ω–∞–∫–∏ $\mathbf{x}$
- **P(H|D)** ‚Äî **posterior** (–∞–ø–æ—Å—Ç–µ—Ä–∏–æ—Ä–Ω–∞—è): –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –≥–∏–ø–æ—Ç–µ–∑—ã –ø–æ—Å–ª–µ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö
- **P(D|H)** ‚Äî **likelihood** (–ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–∏–µ): –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö –ø—Ä–∏ –¥–∞–Ω–Ω–æ–π –≥–∏–ø–æ—Ç–µ–∑–µ
- **P(H)** ‚Äî **prior** (–∞–ø—Ä–∏–æ—Ä–Ω–∞—è): –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –≥–∏–ø–æ—Ç–µ–∑—ã –¥–æ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö
- **P(D)** ‚Äî **evidence** (—Å–≤–∏–¥–µ—Ç–µ–ª—å—Å—Ç–≤–æ, –Ω–æ—Ä–º–∏—Ä–æ–≤–æ—á–Ω–∞—è –∫–æ–Ω—Å—Ç–∞–Ω—Ç–∞)

**–ü–æ–ª–Ω–∞—è —Ñ–æ—Ä–º—É–ª–∞ —á–µ—Ä–µ–∑ –∑–∞–∫–æ–Ω –ø–æ–ª–Ω–æ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏:**
$$
P(H|D) = \frac{P(D|H) \cdot P(H)}{\sum_{i} P(D|H_i) \cdot P(H_i)}
$$

**–î–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏:**
$$
P(y|\mathbf{x}) = \frac{P(\mathbf{x}|y) \cdot P(y)}{P(\mathbf{x})}
$$

**–†–µ—à–∞—é—â–µ–µ –ø—Ä–∞–≤–∏–ª–æ (MAP ‚Äî Maximum A Posteriori):**
$$
\hat{y} = \arg\max_{y} P(y|\mathbf{x}) = \arg\max_{y} P(\mathbf{x}|y) \cdot P(y)
$$

> [!TIP]
> **–ê–Ω–∞–ª–æ–≥–∏—è: –®–µ—Ä–ª–æ–∫ –•–æ–ª–º—Å –∏ –∫–∞–ø–ª–∏ –¥–æ–∂–¥—è**  
> –í—ã –ø—Ä–æ—Å—ã–ø–∞–µ—Ç–µ—Å—å, —à—Ç–æ—Ä—ã –∑–∞–∫—Ä—ã—Ç—ã, –Ω–æ –≤—ã —Å–ª—ã—à–∏—Ç–µ —à—É–º.
> - **Prior**: –í—ã –∂–∏–≤–µ—Ç–µ –≤ –õ–æ–Ω–¥–æ–Ω–µ (–≤—ã—Å–æ–∫–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –¥–æ–∂–¥—è) –∏–ª–∏ –≤ –°–∞—Ö–∞—Ä–µ (–Ω–∏–∑–∫–∞—è).
> - **Likelihood**: –®—É–º –ø–æ—Ö–æ–∂ –Ω–∞ –∫–∞–ø–ª–∏ –≤–æ–¥—ã. –ï—Å–ª–∏ —ç—Ç–æ –¥–æ–∂–¥—å, –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —Ç–∞–∫–æ–≥–æ —à—É–º–∞ 99%. –ï—Å–ª–∏ —ç—Ç–æ —Å–æ—Å–µ–¥ –º–æ–µ—Ç –º–∞—à–∏–Ω—É, –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å 5%.
> - **Posterior**: –û–±—ä–µ–¥–∏–Ω–∏–≤ —à—É–º –∏ –∑–Ω–∞–Ω–∏–µ –æ —Ç–æ–º, –≥–¥–µ –≤—ã –Ω–∞—Ö–æ–¥–∏—Ç–µ—Å—å, –≤—ã –¥–µ–ª–∞–µ—Ç–µ –≤—ã–≤–æ–¥: "–°–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ, –∏–¥–µ—Ç –¥–æ–∂–¥—å".

> [!CAUTION]
> **Production Warning: –ü—Ä–æ–∫–ª—è—Ç–∏–µ –∑–Ω–∞–º–µ–Ω–∞—Ç–µ–ª—è (Evidence)**  
> –í —Ä–µ–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö —Å —Å–æ—Ç–Ω—è–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∑–Ω–∞–º–µ–Ω–∞—Ç–µ–ª—å $P(D)$ ‚Äî —ç—Ç–æ —Å–ª–æ–∂–Ω–µ–π—à–∏–π –∏–Ω—Ç–µ–≥—Ä–∞–ª –ø–æ –≤—Å–µ–º –≤–æ–∑–º–æ–∂–Ω—ã–º –∫–æ–º–±–∏–Ω–∞—Ü–∏—è–º –¥–∞–Ω–Ω—ã—Ö. –í Naive Bayes –º—ã –µ–≥–æ –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º, –Ω–æ –≤ —Å–µ—Ä—å–µ–∑–Ω—ã—Ö –ë–∞–π–µ—Å–æ–≤—Å–∫–∏—Ö —Å–µ—Ç—è—Ö –ø—Ä–∏—Ö–æ–¥–∏—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–ª–æ–∂–Ω—ã–µ –∞–ø–ø—Ä–æ–∫—Å–∏–º–∞—Ü–∏–∏ —Ç–∏–ø–∞ **MCMC (Markov Chain Monte Carlo)**. –ï—Å–ª–∏ –º–æ–¥–µ–ª—å "–∑–∞–≤–∏—Å–ª–∞" –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ ‚Äî –≤–æ–∑–º–æ–∂–Ω–æ, –æ–Ω–∞ –ø—ã—Ç–∞–µ—Ç—Å—è —á–µ—Å—Ç–Ω–æ –ø–æ—Å—á–∏—Ç–∞—Ç—å —ç—Ç–æ—Ç –∏–Ω—Ç–µ–≥—Ä–∞–ª.

### 4) Middle-level –Ω—é–∞–Ω—Å—ã

**–ü—Ä–æ–±–ª–µ–º–∞ 1: Prior vs Likelihood**

**Prior P(y):**  
–ë–∞–∑–æ–≤–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –∫–ª–∞—Å—Å–∞ (–¥–æ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤).

**–û—Ü–µ–Ω–∫–∞ –∏–∑ –¥–∞–Ω–Ω—ã—Ö:**
```
P(y = k) = n_k / n
```

**Likelihood P(x|y):**  
–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø—Ä–∏ –¥–∞–Ω–Ω–æ–º –∫–ª–∞—Å—Å–µ.

**–ü—Ä–∏–º–µ—Ä:**

```
Email spam classification:
P(spam) = 0.3  (30% –ø–∏—Å–µ–º ‚Äî —Å–ø–∞–º)
P("viagra" | spam) = 0.8
P("viagra" | not_spam) = 0.01

–í–∏–¥–∏–º —Å–ª–æ–≤–æ "viagra":
P(spam | "viagra") ‚àù 0.8 ¬∑ 0.3 = 0.24
P(not_spam | "viagra") ‚àù 0.01 ¬∑ 0.7 = 0.007
‚Üí –°–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ spam!
```

**–ü—Ä–æ–±–ª–µ–º–∞ 2: Generative vs Discriminative**

**Generative models (Bayesian approach):**  
–ú–æ–¥–µ–ª–∏—Ä—É—é—Ç **—Å–æ–≤–º–µ—Å—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ** P(x, y) = P(x|y) ¬∑ P(y).  
–ú–æ–≥—É—Ç –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –Ω–æ–≤—ã–µ –ø—Ä–∏–º–µ—Ä—ã x –∏–∑ –∫–ª–∞—Å—Å–∞ y.

**–ü—Ä–∏–º–µ—Ä—ã:**
- Naive Bayes
- Hidden Markov Models (HMM)
- Gaussian Mixture Models (GMM)

**Discriminative models:**  
–ú–æ–¥–µ–ª–∏—Ä—É—é—Ç —Ç–æ–ª—å–∫–æ **—É—Å–ª–æ–≤–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ** P(y|x) –Ω–∞–ø—Ä—è–º—É—é.  
–ù–µ –º–æ–≥—É—Ç –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å x.

**–ü—Ä–∏–º–µ—Ä—ã:**
- Logistic Regression
- SVM
- Neural Networks

**–°—Ä–∞–≤–Ω–µ–Ω–∏–µ:**

| –¢–∏–ø | –ß—Ç–æ –º–æ–¥–µ–ª–∏—Ä—É–µ—Ç | –ù—É–∂–Ω–æ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è | –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ | –ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏ |
|-----|---------------|-------------------|-------------|------------|
| **Generative** | P(x\|y) –∏ P(y) | –ë–æ–ª—å—à–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ | ‚úÖ –ú–∞–ª—ã–µ –¥–∞–Ω–Ω—ã–µ<br>‚úÖ –ú–æ–∂–µ—Ç –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å | ‚ùå –î–µ–ª–∞–µ—Ç –ø—Ä–µ–¥–ø–æ–ª–æ–∂–µ–Ω–∏—è –æ P(x\|y) |
| **Discriminative** | P(y\|x) –Ω–∞–ø—Ä—è–º—É—é | –ú–µ–Ω—å—à–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ | ‚úÖ –û–±—ã—á–Ω–æ —Ç–æ—á–Ω–µ–µ –ø—Ä–∏ –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö | ‚ùå –ù—É–∂–Ω–æ –±–æ–ª—å—à–µ –¥–∞–Ω–Ω—ã—Ö |

**–ü—Ä–æ–±–ª–µ–º–∞ 3: –í—ã–±–æ—Ä Prior (Bayesian Inference)**

**Uninformative (non-informative) prior:**  
–ù–µ –¥–µ–ª–∞–µ–º –ø—Ä–µ–¥–ø–æ–ª–æ–∂–µ–Ω–∏–π, –≤—Å–µ –≥–∏–ø–æ—Ç–µ–∑—ã —Ä–∞–≤–Ω–æ–≤–µ—Ä–æ—è—Ç–Ω—ã.

**–ü—Ä–∏–º–µ—Ä:**
- Uniform prior: P(Œ∏) = const

**Informative prior:**  
–ò—Å–ø–æ–ª—å–∑—É–µ–º domain knowledge.

**–ü—Ä–∏–º–µ—Ä (—Ä–µ–≥—Ä–µ—Å—Å–∏—è —Å Bayesian Ridge):**
- Prior –Ω–∞ –≤–µ—Å–∞: w ‚àº N(0, œÉ¬≤)
- –≠—Ç–æ —ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç–Ω–æ L2 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏!

**–ü—Ä–æ–±–ª–µ–º–∞ 4: Computational Issues**

**–ß–∏—Å–ª–∏—Ç–µ–ª—å –≤—ã—á–∏—Å–ª–∏–º:**
```
P(x|y) ¬∑ P(y)
```

**–ó–Ω–∞–º–µ–Ω–∞—Ç–µ–ª—å P(x) —Å–ª–æ–∂–µ–Ω:**
```
P(x) = Œ£ P(x|y) ¬∑ P(y)
```
–¢—Ä–µ–±—É–µ—Ç —Å—É–º–º–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ –≤—Å–µ–º –∫–ª–∞—Å—Å–∞–º (–û–ö –¥–ª—è –º–∞–ª–æ–≥–æ —á–∏—Å–ª–∞ –∫–ª–∞—Å—Å–æ–≤).

**–î–ª—è –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã—Ö x:**  
P(x) ‚Äî –∏–Ω—Ç–µ–≥—Ä–∞–ª, —á–∞—Å—Ç–æ –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏ –Ω–µ–≤—ã—á–∏—Å–ª–∏–º ‚Üí –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –ø—Ä–∏–±–ª–∏–∂–µ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã:
- **MCMC (Markov Chain Monte Carlo)**
- **Variational Inference**

### 5) –°—Ä–∞–≤–Ω–µ–Ω–∏–µ Bayesian vs Frequentist

| –ü–æ–¥—Ö–æ–¥ | –ü–∞—Ä–∞–º–µ—Ç—Ä—ã | –ù–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç—å | –ú–∞–ª—ã–µ –¥–∞–Ω–Ω—ã–µ | –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è |
|--------|-----------|-----------------|-------------|---------------|
| **Bayesian** | –°–ª—É—á–∞–π–Ω—ã–µ –≤–µ–ª–∏—á–∏–Ω—ã —Å —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º | ‚úÖ –Ø–≤–Ω–æ –º–æ–¥–µ–ª–∏—Ä—É–µ—Ç—Å—è (posterior) | ‚úÖ –ú–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å prior | "–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –≥–∏–ø–æ—Ç–µ–∑—ã" |
| **Frequentist (MLE)** | –§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ (–Ω–æ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–µ) | ‚ö†Ô∏è –ß–µ—Ä–µ–∑ confidence intervals | ‚ùå –ù—É–∂–Ω–æ –º–Ω–æ–≥–æ –¥–∞–Ω–Ω—ã—Ö | "–ß–∞—Å—Ç–æ—Ç–∞ –ø—Ä–∏ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è—Ö" |

---

## 4.2 Naive Bayes Classifier

### 1) –ò–Ω—Ç—É–∏—Ü–∏—è –∏ —Å—É—Ç—å

**–ü—Ä–æ—Å—Ç—ã–º–∏ —Å–ª–æ–≤–∞–º–∏ (ELI5):**  
–¢—ã –æ–ø—Ä–µ–¥–µ–ª—è–µ—à—å, –æ–ø–∏—Å—ã–≤–∞–µ—Ç –ª–∏ —Ç–µ–∫—Å—Ç "—Å–ø–æ—Ä—Ç" –∏–ª–∏ "–ø–æ–ª–∏—Ç–∏–∫—É". –°–º–æ—Ç—Ä–∏—à—å –Ω–∞ –∫–∞–∂–¥–æ–µ —Å–ª–æ–≤–æ **–Ω–µ–∑–∞–≤–∏—Å–∏–º–æ**: "–º—è—á" ‚Üí —Å–∫–æ—Ä–µ–µ —Å–ø–æ—Ä—Ç, "–≤—ã–±–æ—Ä—ã" ‚Üí —Å–∫–æ—Ä–µ–µ –ø–æ–ª–∏—Ç–∏–∫–∞. Naive Bayes **–Ω–∞–∏–≤–Ω–æ** –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç, —á—Ç–æ —Å–ª–æ–≤–∞ –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã –¥—Ä—É–≥ –æ—Ç –¥—Ä—É–≥–∞ (—á—Ç–æ –Ω–µ–≤–µ—Ä–Ω–æ, –Ω–æ —á–∞—Å—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç!).

**–ê–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏:**  
Naive Bayes ‚Äî —ç—Ç–æ **generative probabilistic classifier**, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ —Ç–µ–æ—Ä–µ–º–µ –ë–∞–π–µ—Å–∞ —Å **—Å–∏–ª—å–Ω—ã–º (–Ω–∞–∏–≤–Ω—ã–º) –ø—Ä–µ–¥–ø–æ–ª–æ–∂–µ–Ω–∏–µ–º –æ —É—Å–ª–æ–≤–Ω–æ–π –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤** –ø—Ä–∏ –¥–∞–Ω–Ω–æ–º –∫–ª–∞—Å—Å–µ.

**–§–∏–∑–∏—á–µ—Å–∫–∏–π —Å–º—ã—Å–ª:**  
–£–ø—Ä–æ—â–µ–Ω–∏–µ P(x|y) = P(x_1, x_2, ..., x_d | y) –¥–æ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è P(x_1|y) ¬∑ P(x_2|y) ¬∑ ... ¬∑ P(x_d|y).

### 2) –ó–∞—á–µ–º –∏ –∫–æ–≥–¥–∞ (Business Value)

**–ö–∞–∫—É—é –ø—Ä–æ–±–ª–µ–º—É —Ä–µ—à–∞–µ—Ç:**
- **–ë—ã—Å—Ç—Ä–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è** (–æ–±—É—á–µ–Ω–∏–µ –∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å –∑–∞ O(n¬∑d))
- **–ú–∞–ª—ã–µ –¥–∞–Ω–Ω—ã–µ**: —Ä–∞–±–æ—Ç–∞–µ—Ç –¥–∞–∂–µ –ø—Ä–∏ n << d
- **Text classification**: –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –≤—ã–±–æ—Ä –¥–ª—è bag-of-words

**–ì–¥–µ —ç—Ç–æ –∏–Ω–¥—É—Å—Ç—Ä–∏–∞–ª—å–Ω—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç:**
- **Spam filtering**: –∫–ª–∞—Å—Å–∏–∫–∞ (–ø—Ä–æ—Å—Ç–æ–π, –±—ã—Å—Ç—Ä—ã–π, —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π)
- **Sentiment analysis**: —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å —Ç–µ–∫—Å—Ç–∞ (positive/negative)
- **Document classification**: –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è –Ω–æ–≤–æ—Å—Ç–µ–π, —Å—Ç–∞—Ç–µ–π
- **Medical diagnosis**: —Å–∏–º–ø—Ç–æ–º—ã ‚Üí –±–æ–ª–µ–∑–Ω—å

**–ö–æ–≥–¥–∞ –ø—Ä–∏–º–µ–Ω—è—Ç—å –Ω–µ–ª—å–∑—è:**
- ‚ùå –ü—Ä–∏–∑–Ω–∞–∫–∏ —Å–∏–ª—å–Ω–æ –∫–æ—Ä—Ä–µ–ª–∏—Ä–æ–≤–∞–Ω—ã (–Ω–∞—Ä—É—à–∞–µ—Ç—Å—è assumption –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏)
- ‚ùå –ù—É–∂–Ω–∞ –≤—ã—Å–æ–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –ø—Ä–∏ –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö (–∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏)

### 3) –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —è–¥—Ä–æ

**–¢–µ–æ—Ä–µ–º–∞ –ë–∞–π–µ—Å–∞ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏:**
```
P(y|x) ‚àù P(x|y) ¬∑ P(y)
```

**Naive Bayes assumption (—É—Å–ª–æ–≤–Ω–∞—è –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å):**
$$
P(\mathbf{x}|y) = P(x_1, x_2, \dots, x_d | y) = \prod_{j=1}^{d} P(x_j | y)
$$

**–†–µ—à–∞—é—â–µ–µ –ø—Ä–∞–≤–∏–ª–æ:**
$$
\hat{y} = \arg\max_{y} \left[ P(y) \cdot \prod_{j=1}^{d} P(x_j | y) \right]
$$

**Log-—Ñ–æ—Ä–º–∞ (–¥–ª—è —á–∏—Å–ª–µ–Ω–Ω–æ–π —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏):**
$$
\hat{y} = \arg\max_{y} \left[ \log P(y) + \sum_{j=1}^{d} \log P(x_j | y) \right]
$$

**–¢–∏–ø—ã Naive Bayes:**

1. **Gaussian Naive Bayes:**
$$
P(x_j | y) = \frac{1}{\sqrt{2\pi\sigma^2_{jy}}} \exp\left(-\frac{(x_j - \mu_{jy})^2}{2\sigma^2_{jy}}\right)
$$

2. **Multinomial Naive Bayes (Laplace Smoothing):**
$$
P(x_j | y) = \frac{\text{count}(x_j, y) + \alpha}{\sum_{k} \text{count}(x_k, y) + \alpha \cdot d}
$$

> [!WARNING]
> **Zero Frequency Problem & Laplace Smoothing**  
> –ï—Å–ª–∏ —Å–ª–æ–≤–∞ "–ë–∏—Ç–∫–æ–∏–Ω" –Ω–µ –±—ã–ª–æ –≤ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–µ —Å–ø–∞–º–∞, –µ–≥–æ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –±—É–¥–µ—Ç 0. –ò–∑-–∑–∞ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è –≤—Å—ë –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å—Ç–∞–Ω–µ—Ç 0. **$\alpha$ (–æ–±—ã—á–Ω–æ 1.0)** ‚Äî —ç—Ç–æ "–ø—Å–µ–≤–¥–æ-–æ—Ç—Å—á–µ—Ç", –∫–æ—Ç–æ—Ä—ã–π –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç, —á—Ç–æ –Ω–∏ –æ–¥–Ω–∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –Ω–µ —Å—Ç–∞–Ω–µ—Ç —Å—Ç—Ä–æ–≥–æ –Ω—É–ª–µ–º. –í `sklearn` —ç—Ç–æ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è `alpha`.

> [!TIP]
> **–ê–Ω–∞–ª–æ–≥–∏—è: –ú—É–¥—Ä–æ—Å—Ç—å —Ç–æ–ª–ø—ã –∏–∑ –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã—Ö —ç–∫—Å–ø–µ—Ä—Ç–æ–≤**  
> –ü—Ä–µ–¥—Å—Ç–∞–≤—å—Ç–µ, —á—Ç–æ –≤—ã —Å–ø—Ä–∞—à–∏–≤–∞–µ—Ç–µ —Å–æ–≤–µ—Ç–∞ —É 10 —ç–∫—Å–ø–µ—Ä—Ç–æ–≤. Naive Bayes –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç, —á—Ç–æ —ç–∫—Å–ø–µ—Ä—Ç—ã –Ω–µ –æ–±—â–∞—é—Ç—Å—è –¥—Ä—É–≥ —Å –¥—Ä—É–≥–æ–º. –ö–∞–∂–¥—ã–π –¥–∞–µ—Ç —Å–≤–æ—é –æ—Ü–µ–Ω–∫—É –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ. –≠—Ç–æ "–Ω–∞–∏–≤–Ω–æ", –ø–æ—Ç–æ–º—É —á—Ç–æ —ç–∫—Å–ø–µ—Ä—Ç—ã –º–æ–≥—É—Ç –±—ã—Ç—å –∫–æ–ª–ª–µ–≥–∞–º–∏, –Ω–æ –µ—Å–ª–∏ –∏—Ö –º–Ω–æ–≥–æ, –∏—Ö –æ–±—â–∞—è "–º—É–¥—Ä–æ—Å—Ç—å" –ø–µ—Ä–µ–≤–µ—à–∏–≤–∞–µ—Ç –∏—Ö –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ —Å–≤—è–∑–∏.

### 4) Middle-level –Ω—é–∞–Ω—Å—ã

**–ü—Ä–æ–±–ª–µ–º–∞ 1: –ü–æ—á–µ–º—É "–Ω–∞–∏–≤–Ω—ã–π"?**

**Assumption:**  
P(x_1, x_2 | y) = P(x_1 | y) ¬∑ P(x_2 | y)

**–†–µ–∞–ª—å–Ω–æ—Å—Ç—å (–ø—Ä–∏–º–µ—Ä text):**  
–°–ª–æ–≤–∞ "–≥–æ—Ä—è—á–∏–π" –∏ "—Å–æ–±–∞–∫–∞" –º–æ–≥—É—Ç –±—ã—Ç—å –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã, –Ω–æ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ "—Ö–æ—Ç-–¥–æ–≥" –æ–Ω–∏ –∫–æ—Ä—Ä–µ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã!

**–ü–æ—á–µ–º—É –≤—Å–µ —Ä–∞–≤–Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç:**
- –ù–∞–º –Ω–µ –Ω—É–∂–Ω—ã **—Ç–æ—á–Ω—ã–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏**, —Ç–æ–ª—å–∫–æ **–ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø–æ—Ä—è–¥–æ–∫** (—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤)
- –î–∞–∂–µ –ø—Ä–∏ –Ω–∞—Ä—É—à–µ–Ω–∏–∏ assumption, –º–æ–¥–µ–ª—å —á–∞—Å—Ç–æ –¥–∞–µ—Ç **–ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è**

**–ü—Ä–æ–±–ª–µ–º–∞ 2: Laplace Smoothing (—Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ)**

**–ü—Ä–æ–±–ª–µ–º–∞ zero-frequency:**

```
Text: "buy viagra now"
Training: —Å–ª–æ–≤–æ "viagra" –Ω–∏–∫–æ–≥–¥–∞ –Ω–µ –≤—Å—Ç—Ä–µ—á–∞–ª–æ—Å—å –≤ "not_spam".
‚Üí P("viagra" | not_spam) = 0
‚Üí P("buy viagra now" | not_spam) = 0  (–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ!)
‚Üí –ú–æ–¥–µ–ª—å –≤—Å–µ–≥–¥–∞ –ø—Ä–µ–¥—Å–∫–∞–∂–µ—Ç spam, –¥–∞–∂–µ –µ—Å–ª–∏ –æ—Å—Ç–∞–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞ —É–∫–∞–∑—ã–≤–∞—é—Ç –Ω–∞ not_spam!
```

**–†–µ—à–µ–Ω–∏–µ ‚Äî Laplace Smoothing (add-Œ± smoothing):**
```
P(x_j | y) = (count(x_j, y) + Œ±) / (Œ£ count(x_k, y) + Œ± ¬∑ d)
```

–û–±—ã—á–Ω–æ Œ± = 1 (add-one smoothing).

**–≠—Ñ—Ñ–µ–∫—Ç:**
- –í—Å–µ P(x_j | y) > 0 (–Ω–µ—Ç –Ω—É–ª–µ–π)
- –ü—Ä–∏ –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö Œ±/n ‚Üí 0, smoothing –Ω–µ –≤–ª–∏—è–µ—Ç
- –ü—Ä–∏ –º–∞–ª—ã—Ö –¥–∞–Ω–Ω—ã—Ö smoothing –∫—Ä–∏—Ç–∏—á–Ω–æ!

**–ü—Ä–æ–±–ª–µ–º–∞ 3: –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ —Å–ª–∏—à–∫–æ–º –º–∞–ª—ã (Numerical Underflow)**

**–ü—Ä–æ–±–ª–µ–º–∞:**
```
P(y|x) ‚àù P(y) ¬∑ Œ† P(x_j | y)
```
–ü—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ –º–Ω–æ–∂–µ—Å—Ç–≤–∞ –º–∞–ª—ã—Ö –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π (–Ω–∞–ø—Ä–∏–º–µ—Ä, 0.001^100) ‚Üí underflow (‚âà 0 –≤ –∫–æ–º–ø—å—é—Ç–µ—Ä–µ).

**–†–µ—à–µ–Ω–∏–µ ‚Äî Log-probabilities:**
```
log P(y|x) ‚àù log P(y) + Œ£ log P(x_j | y)
```

–°—É–º–º–∞ –ª–æ–≥–∞—Ä–∏—Ñ–º–æ–≤ –≤–º–µ—Å—Ç–æ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è ‚Üí —á–∏—Å–ª–µ–Ω–Ω–∞—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å!

**–ü—Ä–æ–±–ª–µ–º–∞ 4: –ö–∞–ª–∏–±—Ä–æ–≤–∫–∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π**

Naive Bayes –≤—ã–¥–∞–µ—Ç **–Ω–µ–∫–∞–ª–∏–±—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏** (—Å–ª–∏—à–∫–æ–º —É–≤–µ—Ä–µ–Ω–Ω—ã–µ –∏–ª–∏ –Ω–µ—É–≤–µ—Ä–µ–Ω–Ω—ã–µ).

**–ü—Ä–∏—á–∏–Ω–∞:**  
Assumption –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –Ω–∞—Ä—É—à–∞–µ—Ç—Å—è ‚Üí P(y|x) –Ω–µ –∏—Å—Ç–∏–Ω–Ω–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å.

**–†–µ—à–µ–Ω–∏–µ:**
- **Platt Scaling**: –æ–±—É—á–∏—Ç—å –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫—É—é —Ä–µ–≥—Ä–µ—Å—Å–∏—é –Ω–∞ –≤—ã—Ö–æ–¥–∞—Ö Naive Bayes
- –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Naive Bayes —Ç–æ–ª—å–∫–æ –¥–ª—è **—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è**, –Ω–µ –¥–ª—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π

### 5) –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ Naive Bayes

| –í–∞—Ä–∏–∞–Ω—Ç | –¢–∏–ø –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ | –ü—Ä–∏–º–µ—Ä –∑–∞–¥–∞—á–∏ | –§–æ—Ä–º—É–ª–∞ P(x_j\|y) |
|---------|--------------|--------------|------------------|
| **Gaussian NB** | –ù–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–µ | –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –ø–æ —Ñ–∏–∑–∏—á–µ—Å–∫–∏–º –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º | N(Œº, œÉ¬≤) |
| **Multinomial NB** | –°—á–µ—Ç–Ω—ã–µ (—á–∞—Å—Ç–æ—Ç—ã) | Text classification (bag-of-words) | Multinomial distribution |
| **Bernoulli NB** | –ë–∏–Ω–∞—Ä–Ω—ã–µ {0,1} | Text (—Å–ª–æ–≤–æ –µ—Å—Ç—å/–Ω–µ—Ç) | Bernoulli(p) |

---

## 4.3 Generative vs Discriminative Models

### 1) –ò–Ω—Ç—É–∏—Ü–∏—è –∏ —Å—É—Ç—å

**–ü—Ä–æ—Å—Ç—ã–º–∏ —Å–ª–æ–≤–∞–º–∏ (ELI5):**  
–¢—ã —É—á–∏—à—å—Å—è –æ—Ç–ª–∏—á–∞—Ç—å –∫–æ—à–µ–∫ –æ—Ç —Å–æ–±–∞–∫.

- **Generative**: —Å–Ω–∞—á–∞–ª–∞ –∏–∑—É—á–∞–µ—à—å, "–∫–∞–∫ –≤—ã–≥–ª—è–¥—è—Ç –∫–æ—à–∫–∏" –∏ "–∫–∞–∫ –≤—ã–≥–ª—è–¥—è—Ç —Å–æ–±–∞–∫–∏" –æ—Ç–¥–µ–ª—å–Ω–æ. –ü–æ—Ç–æ–º, —É–≤–∏–¥–µ–≤ –∂–∏–≤–æ—Ç–Ω–æ–µ, —Å—Ä–∞–≤–Ω–∏–≤–∞–µ—à—å: "–±–æ–ª—å—à–µ –ø–æ—Ö–æ–∂–µ –Ω–∞ –∫–æ—à–∫—É –∏–ª–∏ —Å–æ–±–∞–∫—É?"
- **Discriminative**: —Å—Ä–∞–∑—É —É—á–∏—à—å—Å—è –Ω–∞—Ö–æ–¥–∏—Ç—å **–≥—Ä–∞–Ω–∏—Ü—É**: "—É—à–∏ —Ç–æ—Ä—á–∞—Ç ‚Üí –∫–æ—à–∫–∞, —É—à–∏ –≤–∏—Å—è—Ç ‚Üí —Å–æ–±–∞–∫–∞". –ù–µ –º–æ–¥–µ–ª–∏—Ä—É–µ—à—å, –∫–∞–∫ –æ–Ω–∏ –≤—ã–≥–ª—è–¥—è—Ç, —Ç–æ–ª—å–∫–æ —Ä–∞–∑–Ω–∏—Ü—É.

**–ê–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏:**  
**Generative models** –º–æ–¥–µ–ª–∏—Ä—É—é—Ç —Å–æ–≤–º–µ—Å—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ P(x, y), –∑–∞—Ç–µ–º –ø—Ä–∏–º–µ–Ω—è—é—Ç —Ç–µ–æ—Ä–µ–º—É –ë–∞–π–µ—Å–∞ –¥–ª—è P(y|x).  
**Discriminative models** –º–æ–¥–µ–ª–∏—Ä—É—é—Ç —É—Å–ª–æ–≤–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ P(y|x) –Ω–∞–ø—Ä—è–º—É—é.

### 2) –ó–∞—á–µ–º –∏ –∫–æ–≥–¥–∞ (Business Value)

**Generative:**
- ‚úÖ **–ú–∞–ª—ã–µ –¥–∞–Ω–Ω—ã–µ**: –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å prior P(y)
- ‚úÖ **–ì–µ–Ω–µ—Ä–∞—Ü–∏—è**: –º–æ–∂–Ω–æ —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞—Ç—å –Ω–æ–≤—ã–µ x –∏–∑ P(x|y)
- ‚úÖ **–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è**: —è–≤–Ω–∞—è –º–æ–¥–µ–ª—å –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π

**Discriminative:**
- ‚úÖ **–ë–æ–ª—å—à–∏–µ –¥–∞–Ω–Ω—ã–µ**: –æ–±—ã—á–Ω–æ —Ç–æ—á–Ω–µ–µ  
- ‚úÖ **–ú–µ–Ω—å—à–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤**: –Ω–µ –º–æ–¥–µ–ª–∏—Ä—É–µ–º P(x|y), —Ç–æ–ª—å–∫–æ –≥—Ä–∞–Ω–∏—Ü—É
- ‚úÖ **–ì–∏–±–∫–æ—Å—Ç—å**: –º–æ–∂–Ω–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞—Ç—å —Å–ª–æ–∂–Ω—ã–µ –Ω–µ–ª–∏–Ω–µ–π–Ω—ã–µ –≥—Ä–∞–Ω–∏—Ü—ã

**–ì–¥–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è:**

**Generative:**
- Naive Bayes (text classification)
- Hidden Markov Models (speech recognition)
- Generative Adversarial Networks (GANs) ‚Äî –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π

**Discriminative:**
- Logistic Regression
- SVM
- Neural Networks
- Random Forest

### 3) –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —è–¥—Ä–æ

**Generative:**
```
P(y|x) = P(x, y) / P(x) = P(x|y) ¬∑ P(y) / P(x)
```

**–û–±—É—á–µ–Ω–∏–µ:**  
–û—Ü–µ–Ω–∏—Ç—å P(x|y) –∏ P(y) –∏–∑ –¥–∞–Ω–Ω—ã—Ö.

**–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è:**
```
≈∑ = argmax P(x|y) ¬∑ P(y)
      y
```

**Discriminative:**
```
P(y|x) –Ω–∞–ø—Ä—è–º—É—é
```

**–û–±—É—á–µ–Ω–∏–µ:**  
–ú–∞–∫—Å–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å P(y|x) (–Ω–∞–ø—Ä–∏–º–µ—Ä, —á–µ—Ä–µ–∑ LogLoss).

**–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è:**
```
≈∑ = argmax P(y|x)
      y
```

### 4) Middle-level –Ω—é–∞–Ω—Å—ã

**–ö–æ–≥–¥–∞ Generative –ª—É—á—à–µ:**

1. **–ú–∞–ª—ã–µ –¥–∞–Ω–Ω—ã–µ**: prior P(y) –ø–æ–º–æ–≥–∞–µ—Ç
2. **–ù–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ**: —è–≤–Ω–∞—è –º–æ–¥–µ–ª—å P(y) —É—á–∏—Ç—ã–≤–∞–µ—Ç –¥–∏—Å–±–∞–ª–∞–Ω—Å
3. **–ù—É–∂–Ω–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è**: –Ω–∞–ø—Ä–∏–º–µ—Ä, —Å–∏–Ω—Ç–µ–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π

**–ö–æ–≥–¥–∞ Discriminative –ª—É—á—à–µ:**

1. **–ë–æ–ª—å—à–∏–µ –¥–∞–Ω–Ω—ã–µ**: –±–æ–ª—å—à–µ –≥–∏–±–∫–æ—Å—Ç–∏, –º–µ–Ω—å—à–µ assumptions
2. **–°–ª–æ–∂–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏**: –Ω–µ –Ω—É–∂–Ω–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞—Ç—å P(x|y), —Ç–æ–ª—å–∫–æ –≥—Ä–∞–Ω–∏—Ü—É

**–ü—Ä–∏–º–µ—Ä:**  
Naive Bayes (generative) vs Logistic Regression (discriminative) –Ω–∞ text classification:
- –ú–∞–ª—ã–µ –¥–∞–Ω–Ω—ã–µ (< 1000): Naive Bayes —á–∞—Å—Ç–æ –ª—É—á—à–µ
- –ë–æ–ª—å—à–∏–µ –¥–∞–Ω–Ω—ã–µ (> 10,000): Logistic Regression –æ–±—ã—á–Ω–æ —Ç–æ—á–Ω–µ–µ

### 5) –°—Ä–∞–≤–Ω–µ–Ω–∏–µ

| –ê—Å–ø–µ–∫—Ç | Generative | Discriminative |
|--------|-----------|---------------|
| **–ß—Ç–æ –º–æ–¥–µ–ª–∏—Ä—É–µ—Ç** | P(x, y) = P(x\|y) ¬∑ P(y) | P(y\|x) |
| **–ü–∞—Ä–∞–º–µ—Ç—Ä—ã** | –ë–æ–ª—å—à–µ (–Ω—É–∂–Ω–æ —Å–º–æ–¥–µ–ª–∏—Ä–æ–≤–∞—Ç—å P(x\|y)) | –ú–µ–Ω—å—à–µ |
| **–î–∞–Ω–Ω—ã–µ** | ‚úÖ –ú–∞–ª—ã–µ –¥–∞–Ω–Ω—ã–µ | ‚úÖ –ë–æ–ª—å—à–∏–µ –¥–∞–Ω–Ω—ã–µ |
| **Assumptions** | ‚ö†Ô∏è –°–∏–ª—å–Ω—ã–µ (–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–∏ P(x\|y)) | ‚úÖ –ú–µ–Ω—å—à–µ assumptions |
| **–ì–µ–Ω–µ—Ä–∞—Ü–∏—è** | ‚úÖ –ú–æ–∂–µ—Ç –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å x | ‚ùå –ù–µ—Ç |
| **–ü—Ä–∏–º–µ—Ä—ã** | Naive Bayes, GMM, HMM | Logistic Regression, SVM, NN |

---

---

## üìù –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –∫–æ–¥ (sklearn)

### Naive Bayes –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞

```python
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# –î–∞–Ω–Ω—ã–µ: —Ç–µ–∫—Å—Ç—ã –∏ –º–µ—Ç–∫–∏ (spam/ham)
texts = ["win money now", "hi how are you", "free offer", "meeting at 5"]
labels = [1, 0, 1, 0]  # 1=spam, 0=ham

X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.25, random_state=42)

# Pipeline: –¢–µ–∫—Å—Ç ‚Üí –í–µ–∫—Ç–æ—Ä–∞ (Count) ‚Üí MultinomialNB
pipeline = Pipeline([
    ('vectorizer', CountVectorizer()),  # –ü—Ä–µ–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞ –≤ —á–∞—Å—Ç–æ—Ç—ã (bag-of-words)
    ('nb', MultinomialNB(alpha=1.0))    # alpha=1.0 –¥–ª—è Laplace Smoothing
])

pipeline.fit(X_train, y_train)
print("Accuracy:", pipeline.score(X_test, y_test))

# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–æ–≤–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
new_emails = ["free money for you", "project updates"]
print("Predictions:", pipeline.predict(new_emails))
# –í—ã–≤–æ–¥: [1, 0] (—Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ)
```

### GaussianNB –¥–ª—è –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö

```python
# –î–∞–Ω–Ω—ã–µ: —Ä–æ—Å—Ç, –≤–µ—Å, —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ (–Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏)
from sklearn.datasets import make_classification
X, y = make_classification(n_features=3, n_informative=3, n_redundant=0)

model_gnb = GaussianNB()
model_gnb.fit(X, y)

print(f"GaussianNB Accuracy: {model_gnb.score(X, y):.3f}")
```

### –¢–∞–±–ª–∏—Ü–∞ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ Naive Bayes

| –ö–ª–∞—Å—Å –≤ sklearn | –¢–∏–ø –¥–∞–Ω–Ω—ã—Ö | –ó–∞–¥–∞—á–∞ | –ü–∞—Ä–∞–º–µ—Ç—Ä—ã |
|-----------------|------------|--------|-----------|
| **MultinomialNB** | –°—á–µ—Ç–Ω—ã–µ (—Ü–µ–ª—ã–µ —á–∏—Å–ª–∞) | Text Classification (Bag-of-Words) | `alpha` (—Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ) |
| **BernoulliNB** | –ë–∏–Ω–∞—Ä–Ω—ã–µ (0/1) | Text (–µ—Å—Ç—å —Å–ª–æ–≤–æ –∏–ª–∏ –Ω–µ—Ç) | `alpha`, `binarize` |
| **GaussianNB** | –ù–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–µ | –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –ø–æ —Ñ–∏–∑. –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º | `var_smoothing` (—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å) |
| **ComplementNB** | –°—á–µ—Ç–Ω—ã–µ | –î–∏—Å–±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤ (—Ç–µ–∫—Å—Ç—ã) | `alpha` |

---

## üéØ Q&A –¥–ª—è —ç–∫–∑–∞–º–µ–Ω–∞

**Q1: –ü–æ—á–µ–º—É Naive Bayes –Ω–∞–∑—ã–≤–∞—é—Ç "–Ω–∞–∏–≤–Ω—ã–º"?**
> –ü–æ—Ç–æ–º—É —á—Ç–æ –æ–Ω –¥–µ–ª–∞–µ—Ç –æ—á–µ–Ω—å —Å–∏–ª—å–Ω–æ–µ (–∏ —á–∞—Å—Ç–æ –Ω–µ–≤–µ—Ä–Ω–æ–µ) –ø—Ä–µ–¥–ø–æ–ª–æ–∂–µ–Ω–∏–µ: –≤—Å–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ **—É—Å–ª–æ–≤–Ω–æ –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã** –ø—Ä–∏ –¥–∞–Ω–Ω–æ–º –∫–ª–∞—Å—Å–µ. –ù–∞–ø—Ä–∏–º–µ—Ä, –æ–Ω —Å—á–∏—Ç–∞–µ—Ç, —á—Ç–æ –Ω–∞–ª–∏—á–∏–µ —Å–ª–æ–≤–∞ "New" –Ω–µ –≤–ª–∏—è–µ—Ç –Ω–∞ –Ω–∞–ª–∏—á–∏–µ —Å–ª–æ–≤–∞ "York", –∑–Ω–∞—è, —á—Ç–æ —Ç–µ–∫—Å—Ç –ø—Ä–æ –≥–æ—Ä–æ–¥–∞. –ù–∞ –ø—Ä–∞–∫—Ç–∏–∫–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –µ—Å—Ç—å, –Ω–æ –º–æ–¥–µ–ª—å –≤—Å–µ —Ä–∞–≤–Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç —Ö–æ—Ä–æ—à–æ.

**Q2: –ß—Ç–æ —Ç–∞–∫–æ–µ –ø—Ä–æ–±–ª–µ–º–∞ –Ω—É–ª–µ–≤–æ–π —á–∞—Å—Ç–æ—Ç—ã (Zero Frequency) –∏ –∫–∞–∫ –µ—ë —Ä–µ—à–∏—Ç—å?**
> –ï—Å–ª–∏ —Å–ª–æ–≤–æ "viagra" –Ω–∏–∫–æ–≥–¥–∞ –Ω–µ –≤—Å—Ç—Ä–µ—á–∞–ª–æ—Å—å –≤ —Ö–æ—Ä–æ—à–∏—Ö –ø–∏—Å—å–º–∞—Ö (ham) –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è, —Ç–æ P("viagra" | ham) = 0. –¢–æ–≥–¥–∞ –≤—Å—ë –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π P(text | ham) —Å—Ç–∞–Ω–µ—Ç 0, –∏ –º–æ–¥–µ–ª—å –Ω–∏–∫–æ–≥–¥–∞ –Ω–µ –ø—Ä–µ–¥—Å–∫–∞–∂–µ—Ç ham.
> **–†–µ—à–µ–Ω–∏–µ:** Laplace Smoothing (Add-one smoothing). –î–æ–±–∞–≤–ª—è–µ–º +1 –∫–æ –≤—Å–µ–º —Å—á–µ—Ç—á–∏–∫–∞–º, —á—Ç–æ–±—ã –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –±—ã–ª–∏ > 0.

**Q3: –í —á–µ–º —Ä–∞–∑–Ω–∏—Ü–∞ –º–µ–∂–¥—É Generative –∏ Discriminative –º–æ–¥–µ–ª—è–º–∏?**
> **Generative (Naive Bayes):** –º–æ–¥–µ–ª–∏—Ä—É–µ—Ç, –∫–∞–∫ –≥–µ–Ω–µ—Ä–∏—Ä—É—é—Ç—Å—è –¥–∞–Ω–Ω—ã–µ (P(x|y)) –∏ –∑–Ω–∞–µ—Ç –∞–ø—Ä–∏–æ—Ä–Ω—ã–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –∫–ª–∞—Å—Å–æ–≤ P(y). –ú–æ–∂–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –Ω–æ–≤—ã–π –ø—Ä–∏–º–µ—Ä —Å–ø–∞–º–∞.
> **Discriminative (Logistic Regression):** –º–æ–¥–µ–ª–∏—Ä—É–µ—Ç —Ç–æ–ª—å–∫–æ –≥—Ä–∞–Ω–∏—Ü—É –º–µ–∂–¥—É –∫–ª–∞—Å—Å–∞–º–∏ (P(y|x)). –ù–µ –∑–Ω–∞–µ—Ç, "–∫–∞–∫ –≤—ã–≥–ª—è–¥–∏—Ç —Å–ø–∞–º", –∑–Ω–∞–µ—Ç —Ç–æ–ª—å–∫–æ, –∫–∞–∫ –æ—Ç–ª–∏—á–∏—Ç—å –µ–≥–æ –æ—Ç –Ω–µ-—Å–ø–∞–º–∞. –û–±—ã—á–Ω–æ —Ç–æ—á–Ω–µ–µ –ø—Ä–∏ –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö.

**Q4: –ö–æ–≥–¥–∞ –ª—É—á—à–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Naive Bayes –≤–º–µ—Å—Ç–æ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏?**
> 1. –û—á–µ–Ω—å **–º–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö** (–Ω–µ–π—Ä–æ—Å–µ—Ç—å –ø–µ—Ä–µ–æ–±—É—á–∏—Ç—Å—è).
> 2. –ù—É–∂–Ω–æ **–æ—á–µ–Ω—å –±—ã—Å—Ç—Ä–æ–µ** –æ–±—É—á–µ–Ω–∏–µ –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ (Real-time).
> 3. –ù—É–∂–Ω–∞ –ø—Ä–æ—Å—Ç–∞—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è (–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ —Å–ª–æ–≤).
> 4. –ö–∞–∫ —Å–∏–ª—å–Ω—ã–π **baseline** –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤.

---

## –†–µ–∑—é–º–µ –º–æ–¥—É–ª—è

–ë–∞–π–µ—Å–æ–≤—Å–∫–∏–µ –º–µ—Ç–æ–¥—ã ‚Äî —ç—Ç–æ **–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ ML**:
- **–¢–µ–æ—Ä–µ–º–∞ –ë–∞–π–µ—Å–∞**: –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —É–±–µ–∂–¥–µ–Ω–∏–π –ø—Ä–∏ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö, prior + likelihood ‚Üí posterior
- **Naive Bayes**: –±—ã—Å—Ç—Ä—ã–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Å assumption –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏, Laplace smoothing
- **Generative vs Discriminative**: P(x, y) vs P(y|x), –º–∞–ª—ã–µ –¥–∞–Ω–Ω—ã–µ vs –±–æ–ª—å—à–∏–µ –¥–∞–Ω–Ω—ã–µ, –≥–µ–Ω–µ—Ä–∞—Ü–∏—è vs –≥—Ä–∞–Ω–∏—Ü–∞

–≠—Ç–∏ –º–µ—Ç–æ–¥—ã ‚Äî –æ—Å–Ω–æ–≤–∞ –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω–æ–≥–æ ML –∏ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π (VAE, GANs)!
