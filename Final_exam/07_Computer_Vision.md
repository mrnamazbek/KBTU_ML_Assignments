# Модуль 7: Computer Vision (Сверточные нейронные сети)

## 7.1 Свертка (Convolution)

### 1) Интуиция и суть

**Простыми словами (ELI5):**  
Представь, что у тебя есть фотография, и ты проводишь по ней маленькой лупой (фильтром 3×3). В каждой позиции лупа ищет определенный паттерн (например, вертикальную линию, угол, текстуру). Результат — карта, где записано "насколько сильно паттерн виден в каждом месте".

**Академически:**  
Свертка (convolution) — это операция применения **фильтра (kernel)** к входному изображению, скользя по нему с определенным шагом (stride) и вычисляя взвешенную сумму пикселей. Результат — **feature map**, показывающая наличие определенного признака в разных частях изображения.

**Физический смысл:**  
Фильтр = **детектор паттерна**. Сверточная сеть учится оптимальным фильтрам для задачи.

### 2) Зачем и когда (Business Value)

**Какую проблему решает:**
- **Локальные паттерны**: край, угол, текстура (не нужно смотреть на все изображение сразу)
- **Translation Invariance**: паттерн распознается в любой части изображения
- **Parameter Sharing**: один фильтр на все изображение → меньше параметров, чем fully connected

**Где это индустриальный стандарт:**
- **Computer Vision**: классификация изображений (ImageNet), объектов detection (YOLO), сегментация
- **Медицина**: анализ рентгеновских снимков, МРТ
- **Автономные авто**: распознавание объектов, дорожных знаков
- **Безопасность**: распознавание лиц

### 3) Математическое ядро

**2D Convolution:**

Входное изображение: **I** (H × W)  
Фильтр (kernel): **K** (k × k), обычно k = 3 или 5

**Операция свертки:**
```
(I * K)[i, j] = Σ Σ I[i+m, j+n] · K[m, n]
                m n
```

**Feature map:**  
Результат применения фильтра ко всем позициям изображения.

**Пример (edge detection):**

**Вертикальный фильтр Собеля:**
```
K = [[-1, 0, 1],
     [-2, 0, 2],
     [-1, 0, 1]]
```

Этот фильтр выделяет **вертикальные края** (резкие изменения яркости слева-направо).

**Параметры свертки:**

1. **Stride (шаг)**: на сколько пикселей смещать фильтр
   - stride = 1: каждый пиксель
   - stride = 2: каждый второй пиксель → уменьшение размера

2. **Padding (дополнение)**: добавление нулей по краям
   - 'valid': без padding → размер уменьшается
   - 'same': с padding → размер сохраняется

**Размер выходной feature map:**
```
H_out = (H_in - k + 2·padding) / stride + 1
W_out = (W_in - k + 2·padding) / stride + 1
```

**Пример:**
```
Input: 28×28
Kernel: 3×3
Stride: 1
Padding: 0 (valid)
→ Output: (28 - 3 + 0)/1 + 1 = 26×26
```

**Channels (каналы):**

**Цветное изображение:** 3 канала (RGB)  
**Фильтр:** тоже 3 канала (по одному на каждый цвет)  
**Свертка:** суммируем по всем каналам:
```
Output[i, j] = Σ Σ Σ I[i+m, j+n, c] · K[m, n, c]
               m n c
```

**Множество фильтров:**  
Один сверточный слой обычно содержит **N фильтров** → **N feature maps**.

**Output:** (H_out, W_out, N)

### 4) Middle-level нюансы

**Проблема 1: Зачем свертка, а не Fully Connected?**

**Fully Connected для изображения 224×224×3:**
```
Число параметров = 224·224·3 · N_hidden ≈ 150,000 · N_hidden
```

При N_hidden = 1000 → 150 млн параметров только в первом слое!

**Свертка (фильтр 3×3×3, 64 фильтра):**
```
Число параметров = 3·3·3·64 = 1,728
```

**Преимущества свертки:**
- ✅ **Parameter Sharing**: один фильтр на все изображение
- ✅ **Sparse Connectivity**: нейрон смотрит только на локальную область (receptive field)
- ✅ **Translation Invariance**: паттерн распознается везде

**Проблема 2: Receptive Field (рецептивное поле)**

**Receptive field** — это область входного изображения, которая влияет на данный нейрон.

**Один сверточный слой (3×3):**  
Receptive field = 3×3

**Два сверточных слоя (каждый 3×3):**
```
Layer 1: 3×3
Layer 2: каждый пиксель layer 2 "видит" 3×3 из layer 1
→ Receptive field на входе = 5×5
```

**Глубокая сеть:**  
Receptive field растет с глубиной → глубокие слои "видят" большие области изображения.

**Проблема 3: Число параметров в сверточном слое**

**Формула:**
```
# параметров = k · k · C_in · C_out + C_out
              |____________|   |_____|
               веса фильтров    biases
```

где:
- k — размер фильтра
- C_in — число входных каналов
- C_out — число выходных каналов (фильтров)

**Пример:**
```
Input: 32×32×3
Conv layer: 64 фильтров 3×3
→ Параметры: 3·3·3·64 + 64 = 1,792
```

### 5) Сравнение Convolution vs Fully Connected

| Аспект | Convolution | Fully Connected |
|--------|------------|-----------------|
| **Параметры** | ✅ Мало (parameter sharing) | ❌ Много |
| **Locality** | ✅ Локальные признаки | ❌ Глобальные связи |
| **Translation Invariance** | ✅ Да | ❌ Нет |
| **Когда использовать** | Изображения, пространственные данные | Векторы признаков, последний слой классификации |

---

## 7.2 Pooling (Пулинг)

### 1) Интуиция и суть

**Простыми словами (ELI5):**  
После того как фильтр нашел паттерн в изображении (например, "глаз кота виден здесь"), нам не нужны **точные координаты** пикселя. Достаточно знать: "глаз где-то в этой области". Pooling **упрощает** feature map, сохраняя важную информацию, но уменьшая размер.

**Академически:**  
Pooling — это операция **downsampling** (уменьшения размерности) feature map путем агрегирования значений в небольших окнах (обычно 2×2). Основная цель — **уменьшение вычислений** и достижение **небольшой translation invariance**.

### 2) Зачем и когда (Business Value)

**Какую проблему решает:**
- **Размерность**: уменьшает размер feature maps → меньше вычислений
- **Overfitting**: уменьшение параметров
- **Translation Invariance**: малые сдвиги объекта не меняют результат

### 3) Математическое ядро

**Max Pooling (наиболее популярный):**

Окно 2×2, stride = 2:
```
Input:  [[1, 3],    Output: 3  (max из [1, 3, 2, 4])
         [2, 4]]
```

**Применение ко всей feature map:**
```
Input: 4×4
Output: 2×2  (размер уменьшился в 2 раза по каждой оси)
```

**Average Pooling:**
```
Output: среднее значение в окне
```

**Global Average Pooling:**
```
Input: H×W×C
Output: 1×1×C  (среднее по всей spatial dimension для каждого канала)
```

Используется в конце сети (вместо flatten + FC).

### 4) Middle-level нюансы

**Проблема 1: Max vs Average Pooling**

| Тип | Когда использовать | Эффект |
|-----|-------------------|--------|
| **Max Pooling** | Default | Сохраняет **сильные активации** (яркие features) |
| **Average Pooling** | Когда важна общая информация | Сглаживает, меньше overfitting |
| **Global Average Pooling** | Вместо FC в конце | Меньше параметров, меньше overfitting |

**Проблема 2: Pooling убивает точную локализацию**

**Последствие:** сложно использовать для задач, где нужны точные координаты (semantic segmentation).

**Решение:**  
Современные архитектуры (U-Net, Feature Pyramid Networks) используют **skip connections** для восстановления пространственной информации.

**Проблема 3: Современные альтернативы**

**Strided Convolution:**  
Вместо pooling используем свертку с stride = 2 → уменьшение размера без отдельного слоя pooling.

**Преимущество:**  
Сеть **учится** оптимальному downsampling (а pooling — фиксированная операция).

### 5) Сравнение

| Операция | Размер | Параметры | Инвариантность | Когда использовать |
|----------|--------|----------|---------------|-------------------|
| **Max Pooling** | Уменьшается | 0 | ✅ Небольшая | Классификация |
| **Average Pooling** | Уменьшается | 0 | ✅ Небольшая | Сглаживание |
| **Strided Conv** | Уменьшается | Есть (учатся) | ⚠️ Зависит от весов | Современные архитектуры |

---

## 7.3 Dropout

### 1) Интуиция и суть

**Простыми словами (ELI5):**  
Представь команду, где один игрок очень силен, и остальные на него полагаются. Если он заболеет, команда проиграет. Dropout — это тренировка, где **случайные игроки пропускают каждую игру**. Команда учится работать без любого отдельного игрока → все становятся сильнее, команда робастнее.

**Академически:**  
Dropout — это регуляризационная техника, при которой во время обучения **случайно "выключаются"** (обнуляются) нейроны с вероятностью p. Это предотвращает **co-adaptation** (взаимную зависимость) нейронов и снижает overfitting.

**Физический смысл:**  
Dropout = **неявное ансамблирование**: каждая итерация обучает разную подсеть → финальная сеть = среднее по экспоненциальному числу подсетей.

### 2) Зачем и когда (Business Value)

**Какую проблему решает:**
- **Overfitting**: особенно в fully connected слоях
- **Co-adaptation**: нейроны учатся работать независимо

**Где используется:**
- **Fully connected слои** в конце CNN
- **RNN, LSTM** (dropout между слоями)
- ❌ Редко в сверточных слоях (spatial dropout вместо обычного)

### 3) Математическое ядро

**Training (обучение):**

Для каждого нейрона h_i:
1. Сэмплировать r_i ∼ Bernoulli(p)
2. Выключить нейрон:
   ```
   h_i = {h_i, если r_i = 0 (вероятность 1-p);
          0,   если r_i = 1 (вероятность p)}
   ```

**Среднее:**  
E[h_i] = (1 - p) · h_i

**Inference (тестирование):**

**Вариант 1 (scaling at test):**  
Использовать все нейроны, но умножить на (1 - p):
```
h_i = (1 - p) · h_i
```

**Вариант 2 (inverted dropout, стандарт в современных фреймворках):**  
Во время обучения сразу масштабировать:
```
h_i = h_i / (1 - p)  (при r_i = 0)
```
Тогда при inference ничего не меняем.

### 4) Middle-level нюансы

**Проблема 1: Dropout как ансамбль**

**Интерпретация:**  
Если обучаем сеть с dropout p = 0.5 в слое из n нейронов, каждая итерация обучает одну из 2^n возможных подсетей.

**Inference:**  
Используем **среднее** по всем подсетям (математически: используем все нейроны с весами, умноженными на (1-p)).

**Проблема 2: Выбор p (dropout rate)**

**Эмпирические правила:**
- **Fully connected**: p = 0.5 (стандарт)
- **Convolutional**: p = 0.2-0.3 (меньше, так как меньше параметров)
- **Input layer**: p = 0.1-0.2 (осторожно, можем потерять данные)

**Подбор:** через cross-validation.

**Проблема 3: Spatial Dropout для CNN**

**Обычный dropout в CNN:**  
Выключает отдельные пиксели в feature map → соседние пиксели коррелированны, информация все равно просачивается.

**Spatial Dropout:**  
Выключает **целые feature maps** (все пиксели одного канала).

**Когда использовать:**  
В сверточных слоях.

**Проблема 4: Dropout vs Batch Normalization**

**Batch Normalization** также действует как регуляризатор (добавляет шум через батч-статистики).

**Современная практика:**  
С Batch Norm dropout менее критичен (иногда можно обойтись без него).

### 5) Сравнение регуляризации

| Метод | Где применять | Эффект | Вычисления |
|-------|--------------|--------|-----------|
| **Dropout** | FC слои | Снижает overfitting, ансамблирование | Почти бесплатно |
| **Batch Norm** | После Conv/FC | Нормализация, регуляризация | Немного медленнее |
| **L2 Regularization** | Веса | Shrinkage | Бесплатно (добавка к loss) |

---

## 7.4 Batch Normalization

### 1) Интуиция и суть

**Простыми словами (ELI5):**  
Представь конвейер на фабрике, где каждый рабочий получает детали разного размера (то огромные, то крошечные). Сложно работать! Batch Norm — это **стандартизация деталей** после каждого этапа: приводим их к одинаковому масштабу → рабочий может работать эффективнее.

**Академически:**  
Batch Normalization нормализует активации слоя (приводит к среднему 0, дисперсии 1) **по текущему batch**, затем применяет **learnable affine transformation** (γ, β). Стабилизирует обучение, позволяет использовать большие learning rates.

**Физический смысл:**  
Решает проблему **internal covariate shift** — изменения распределения активаций во время обучения.

### 2) Зачем и когда (Business Value)

**Какую проблему решает:**
- **Стабильность обучения**: активации не взрываются и не затухают
- **Большие learning rates**: можно обучаться быстрее
- **Регуляризация**: небольшой эффект снижения overfitting

**Где используется:**
- **Почти везде в современных CNN**: после Conv слоев (до или после активации)
- **Transformers, ResNets**

### 3) Математическое ядро

**Для batch из m примеров:**

**1. Вычислить среднее и дисперсию по batch:**
```
μ_B = (1/m) Σ x_i
σ²_B = (1/m) Σ (x_i - μ_B)²
```

**2. Нормализовать:**
```
x̂_i = (x_i - μ_B) / √(σ²_B + ε)
```
ε — малое число для численной стабильности (10^(-5)).

**3. Learnable affine transformation:**
```
y_i = γ · x̂_i + β
```

**γ, β** — learnable параметры (позволяют сети "отменить" нормализацию, если нужно).

**Training vs Inference:**

**Training:**  
Используем статистики текущего batch (μ_B, σ²_B).

**Inference:**  
Используем **running mean/variance** (экспоненциальное среднее по всем batch во время обучения):
```
μ_running = α · μ_running + (1 - α) · μ_B
σ²_running = α · σ²_running + (1 - α) · σ²_B
```

### 4) Middle-level нюансы

**Проблема 1: Где ставить Batch Norm?**

**Вариант 1 (оригинальная статья):**
```
Conv → Batch Norm → ReLU
```

**Вариант 2 (современная практика):**
```
Conv → ReLU → Batch Norm
```

**Эмпирически:** оба работают, вариант 2 иногда чуть лучше.

**Проблема 2: Малые batch sizes**

**Проблема:**  
Если batch size мал (например, 4), статистики μ_B, σ²_B шумные → Batch Norm работает плохо.

**Решение:**
- **Group Normalization**: нормализация по группам каналов (не зависит от batch size)
- **Layer Normalization**: нормализация по всем каналам одного примера (используется в Transformers)

**Проблема 3: Batch Norm как регуляризатор**

**Эффект:**  
Batch Norm добавляет **шум** (каждый пример нормализуется относительно batch, а не сам по себе).

**Следствие:**  
Небольшая регуляризация → можно уменьшить dropout.

### 5) Сравнение нормализаций

| Метод | По чему нормализация | Зависит от batch size | Когда использовать |
|-------|---------------------|---------------------|-------------------|
| **Batch Norm** | По batch (N) | ✅ Да | CNN (большие batch) |
| **Layer Norm** | По features (C, H, W) | ❌ Нет | Transformers, RNN |
| **Instance Norm** | По каждому примеру отдельно | ❌ Нет | Style transfer |
| **Group Norm** | По группам каналов | ❌ Нет | Малые batch sizes |

---

## Резюме модуля

Computer Vision с CNN — это **иерархия признаков**:
- **Convolution**: локальные паттерны, parameter sharing, translation invariance
- **Pooling**: downsampling, инвариантность к малым сдвигам
- **Dropout**: регуляризация через ансамблирование подсетей
- **Batch Normalization**: стабилизация обучения, internal covariate shift

Эти блоки — основа всех современных CNN (ResNet, EfficientNet, Vision Transformers)!
