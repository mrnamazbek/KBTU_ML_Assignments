# –ú–æ–¥—É–ª—å 5: –ê–Ω—Å–∞–º–±–ª–∏ (Ensemble Methods)

> **–ó–∞—á–µ–º —ç—Ç–æ—Ç –º–æ–¥—É–ª—å?**  
> –ê–Ω—Å–∞–º–±–ª–∏ ‚Äî —ç—Ç–æ **State-of-the-art** –¥–ª—è —Ç–∞–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –ï—Å–ª–∏ –≤—ã —Ä–∞–±–æ—Ç–∞–µ—Ç–µ —Å Excel-–ø–æ–¥–æ–±–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ (–Ω–µ –∫–∞—Ä—Ç–∏–Ω–∫–∏/—Ç–µ–∫—Å—Ç), –∞–Ω—Å–∞–º–±–ª–∏ ‚Äî –≤–∞—à –≥–ª–∞–≤–Ω—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç.
> - **–¢–æ—á–Ω–æ—Å—Ç—å**: –æ–¥–∏–Ω –≤ –ø–æ–ª–µ –Ω–µ –≤–æ–∏–Ω. –°–æ—Ç–Ω—è "—Å–ª–∞–±—ã—Ö" –º–æ–¥–µ–ª–µ–π –ø–æ—á—Ç–∏ –≤—Å–µ–≥–¥–∞ –ø–æ–±–µ–∂–¥–∞–µ—Ç –æ–¥–Ω—É "—Å–∏–ª—å–Ω—É—é".
> - **–£—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å**: —É—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ –º–Ω–µ–Ω–∏–π —Ä–∞–∑–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∑–∞—â–∏—â–∞–µ—Ç –æ—Ç —Å–ª—É—á–∞–π–Ω—ã—Ö –≤—ã–±—Ä–æ—Å–æ–≤ –∏ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è.
> - **SOTA**: –ø–æ–±–µ–¥–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è –Ω–∞ Kaggle –≤ 90% —Å–ª—É—á–∞–µ–≤ –∏—Å–ø–æ–ª—å–∑—É—é—Ç XGBoost, LightGBM –∏–ª–∏ CatBoost.

---

## 5.1 Bagging (Bootstrap Aggregating)

### 1) –ò–Ω—Ç—É–∏—Ü–∏—è –∏ —Å—É—Ç—å

**–ü—Ä–æ—Å—Ç—ã–º–∏ —Å–ª–æ–≤–∞–º–∏ (ELI5):**  
–£ —Ç–µ–±—è –µ—Å—Ç—å —Å–ª–æ–∂–Ω–∞—è –∑–∞–¥–∞—á–∞ –∏ 100 –¥—Ä—É–∑–µ–π. –í–º–µ—Å—Ç–æ —Ç–æ–≥–æ —á—Ç–æ–±—ã –æ–¥–∏–Ω –¥—Ä—É–≥ —Ä–µ—à–∞–ª –∑–∞–¥–∞—á—É (–º–æ–∂–µ—Ç –æ—à–∏–±–∏—Ç—å—Å—è!), —Ç—ã –¥–∞–µ—à—å –∑–∞–¥–∞—á—É **–∫–∞–∂–¥–æ–º—É**, –Ω–æ –¥–∞–µ—à—å –∏–º —Å–ª–µ–≥–∫–∞ **—Ä–∞–∑–Ω—ã–µ –≤–µ—Ä—Å–∏–∏ —É—Å–ª–æ–≤–∏—è** (bootstrap samples). –ü–æ—Ç–æ–º **—É—Å—Ä–µ–¥–Ω—è–µ—à—å** –∏—Ö –æ—Ç–≤–µ—Ç—ã. –ö–æ–ª–ª–µ–∫—Ç–∏–≤–Ω—ã–π —Ä–∞–∑—É–º –æ–±—ã—á–Ω–æ —Ç–æ—á–Ω–µ–µ!

**–ê–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏:**  
Bagging ‚Äî —ç—Ç–æ **ensemble –º–µ—Ç–æ–¥**, –∫–æ—Ç–æ—Ä—ã–π –æ–±—É—á–∞–µ—Ç –º–Ω–æ–∂–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–µ–π –Ω–∞ **bootstrap samples** (—Å–ª—É—á–∞–π–Ω—ã–µ –≤—ã–±–æ—Ä–∫–∏ —Å –≤–æ–∑–≤—Ä–∞—â–µ–Ω–∏–µ–º –∏–∑ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö) –∏ **—É—Å—Ä–µ–¥–Ω—è–µ—Ç** –∏—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è. –û—Å–Ω–æ–≤–Ω–∞—è —Ü–µ–ª—å ‚Äî **—Å–Ω–∏–∂–µ–Ω–∏–µ variance**.

**–§–∏–∑–∏—á–µ—Å–∫–∏–π —Å–º—ã—Å–ª:**  
–£—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã—Ö –º–æ–¥–µ–ª–µ–π —Å–Ω–∏–∂–∞–µ—Ç –¥–∏—Å–ø–µ—Ä—Å–∏—é: Var(average) = Var(model) / n_models.

### 2) –ó–∞—á–µ–º –∏ –∫–æ–≥–¥–∞ (Business Value)

**–ö–∞–∫—É—é –ø—Ä–æ–±–ª–µ–º—É —Ä–µ—à–∞–µ—Ç:**
- **–ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ** (overfitting): —Å–Ω–∏–∂–∞–µ—Ç variance –≤—ã—Å–æ–∫–æ–¥–∏—Å–ø–µ—Ä—Å–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
- **–ù–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å** –º–æ–¥–µ–ª–µ–π (–Ω–∞–ø—Ä–∏–º–µ—Ä, decision trees –º–µ–Ω—è—é—Ç—Å—è –ø—Ä–∏ –º–∞–ª–æ–º –∏–∑–º–µ–Ω–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö)
- **–£–ª—É—á—à–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏** –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏—è –±–∞–∑–æ–≤–æ–≥–æ –∞–ª–≥–æ—Ä–∏—Ç–º–∞

**–ì–¥–µ —ç—Ç–æ –∏–Ω–¥—É—Å—Ç—Ä–∏–∞–ª—å–Ω—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç:**
- **Random Forest** (bagging + random feature selection) ‚Äî –≤–µ–∑–¥–µ!
- **Healthcare**: –∞–Ω—Å–∞–º–±–ª–∏ –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
- **–§–∏–Ω–∞–Ω—Å—ã**: –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä—ã–Ω–∫–æ–≤
- **Kaggle**: bagging ‚Äî –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç

**–ö–æ–≥–¥–∞ –ø—Ä–∏–º–µ–Ω—è—Ç—å –Ω–µ–ª—å–∑—è:**
- ‚ùå –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å –∏–º–µ–µ—Ç low variance (–Ω–∞–ø—Ä–∏–º–µ—Ä, linear regression) ‚Äî bagging –Ω–µ –ø–æ–º–æ–∂–µ—Ç
- ‚ùå –í—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã –æ–≥—Ä–∞–Ω–∏—á–µ–Ω—ã (–Ω—É–∂–Ω–æ –æ–±—É—á–∞—Ç—å –º–Ω–æ–≥–æ –º–æ–¥–µ–ª–µ–π)

### 3) –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —è–¥—Ä–æ

**–ê–ª–≥–æ—Ä–∏—Ç–º Bagging:**

1. –î–ª—è $b = 1$ –¥–æ $B$ (—á–∏—Å–ª–æ –º–æ–¥–µ–ª–µ–π):
   - –°—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å **Bootstrap sample** $D_b$ (–≤—ã–±–æ—Ä–∫–∞ $n$ –æ–±—ä–µ–∫—Ç–æ–≤ —Å –≤–æ–∑–≤—Ä–∞—â–µ–Ω–∏–µ–º).
   - –û–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å $f_b$ –Ω–∞ $D_b$.

2. **–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ (—Ä–µ–≥—Ä–µ—Å—Å–∏—è):**
$$
\hat{y} = \frac{1}{B} \sum_{b=1}^{B} f_b(\mathbf{x})
$$

3. **–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ (–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è):**
$$
\hat{y} = \text{mode}\{f_b(\mathbf{x})\} \quad \text{(–≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏–µ –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–∞)}
$$

**Out-of-Bag (OOB):**  
–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —Ç–æ–≥–æ, —á—Ç–æ –æ–±—ä–µ–∫—Ç **–Ω–µ –ø–æ–ø–∞–¥–µ—Ç** –≤ bootstrap sample –ø—Ä–∏ $n \to \infty$:
$$
P(\text{not selected}) = \left(1 - \frac{1}{n}\right)^n \to \frac{1}{e} \approx 0.368
$$
–û–∫–æ–ª–æ 37% –¥–∞–Ω–Ω—ã—Ö –æ—Å—Ç–∞—é—Ç—Å—è "—á–∏—Å—Ç—ã–º–∏" –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–µ—Ä–µ–≤–∞ ‚Äî –∏—Ö –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–ª—è **–±–µ—Å–ø–ª–∞—Ç–Ω–æ–π –≤–∞–ª–∏–¥–∞—Ü–∏–∏**.

**–°–Ω–∏–∂–µ–Ω–∏–µ variance (–º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏):**  
–ï—Å–ª–∏ –º–æ–¥–µ–ª–∏ $f_b$ –∏–º–µ—é—Ç –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—é $\rho$ –∏ –¥–∏—Å–ø–µ—Ä—Å–∏—é $\sigma^2$:
$$
\text{Var}(\text{average}) = \rho\sigma^2 + \frac{1-\rho}{B}\sigma^2
$$

**–í—ã–≤–æ–¥:**  
- –ï—Å–ª–∏ $\rho = 0$ (–Ω–µ–∑–∞–≤–∏—Å–∏–º—ã): $\text{Var} = \sigma^2 / B$.
- –ï—Å–ª–∏ $\rho = 1$ (–ø–æ–ª–Ω–∞—è –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å): $\text{Var} = \sigma^2$ (—É—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ –Ω–µ –ø–æ–º–æ–≥–∞–µ—Ç).
- Bagging (Random Forest) —Ä–∞–±–æ—Ç–∞–µ—Ç –∑–∞ —Å—á–µ—Ç **–¥–µ–∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏** –¥–µ—Ä–µ–≤—å–µ–≤ (—É–º–µ–Ω—å—à–µ–Ω–∏—è $\rho$).

> [!TIP]
> **–ê–Ω–∞–ª–æ–≥–∏—è: –ö–æ–Ω—Å–∏–ª–∏—É–º –≤—Ä–∞—á–µ–π**  
> –û–¥–∏–Ω –≤—Ä–∞—á –º–æ–∂–µ—Ç –æ—à–∏–±–∏—Ç—å—Å—è –∏–∑-–∑–∞ —É—Å—Ç–∞–ª–æ—Å—Ç–∏ –∏–ª–∏ —Å—É–±—ä–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–ø—ã—Ç–∞. –ï—Å–ª–∏ —Å–æ–±—Ä–∞—Ç—å 100 –≤—Ä–∞—á–µ–π –∏ –ø–æ–ø—Ä–æ—Å–∏—Ç—å –∫–∞–∂–¥–æ–≥–æ –ø–æ—Å—Ç–∞–≤–∏—Ç—å –¥–∏–∞–≥–Ω–æ–∑, –æ—Å–Ω–æ–≤—ã–≤–∞—è—Å—å –Ω–∞ *—Ä–∞–∑–Ω—ã—Ö* –∞–Ω–∞–ª–∏–∑–∞—Ö –ø–∞—Ü–∏–µ–Ω—Ç–∞, –∏ –∑–∞—Ç–µ–º –ø—Ä–æ–≥–æ–ª–æ—Å–æ–≤–∞—Ç—å ‚Äî –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –æ—à–∏–±–∫–∏ —É–ø–∞–¥–µ—Ç –≤ —Ä–∞–∑—ã. –≠—Ç–æ –∏ –µ—Å—Ç—å Bagging.

> [!CAUTION]
> **Production Warning: OOB Error vs Cross-Validation**  
> OOB –æ—Ü–µ–Ω–∫–∞ –≤ Random Forest ‚Äî –æ—Ç–ª–∏—á–Ω—ã–π —Å–ø–æ—Å–æ–± –±—ã—Å—Ç—Ä–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏. –ù–æ –µ—Å–ª–∏ —É –≤–∞—Å –≤ –¥–∞–Ω–Ω—ã—Ö –µ—Å—Ç—å **–≤—Ä–µ–º–µ–Ω–Ω–∞—è –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å** (Time Series), OOB –±—É–¥–µ—Ç –¥–∞–≤–∞—Ç—å —Å–ª–∏—à–∫–æ–º –æ–ø—Ç–∏–º–∏—Å—Ç–∏—á–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç (leakage). –í —ç—Ç–æ–º —Å–ª—É—á–∞–µ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ç–æ–ª—å–∫–æ `TimeSeriesSplit`.

### 4) Middle-level –Ω—é–∞–Ω—Å—ã

**–ü—Ä–æ–±–ª–µ–º–∞ 1: –ü–æ—á–µ–º—É —Å–Ω–∏–∂–∞–µ—Ç—Å—è variance?**

**Bias-Variance –¥–ª—è Bagging:**
- **Bias**: –ø—Ä–∏–º–µ—Ä–Ω–æ —Ç–∞–∫–æ–π –∂–µ, –∫–∞–∫ —É –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏ (—É—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ –Ω–µ —Å–Ω–∏–∂–∞–µ—Ç bias)
- **Variance**: —Å–∏–ª—å–Ω–æ —Å–Ω–∏–∂–∞–µ—Ç—Å—è (—É—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ!)

**–°–ª–µ–¥—Å—Ç–≤–∏–µ:**  
Bagging —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–µ–Ω –¥–ª—è **low-bias, high-variance –º–æ–¥–µ–ª–µ–π**:
- ‚úÖ **Decision Trees** (–≥–ª—É–±–æ–∫–∏–µ) ‚Äî –∏–¥–µ–∞–ª—å–Ω–æ!
- ‚úÖ Neural Networks (—Å–ª–æ–∂–Ω—ã–µ)
- ‚ùå Linear Regression (—É–∂–µ low variance) ‚Äî –±–µ—Å–ø–æ–ª–µ–∑–Ω–æ

**–ü—Ä–æ–±–ª–µ–º–∞ 2: Out-of-Bag (OOB) Error**

**–ò–¥–µ—è:**  
‚âà 37% –æ–±—ä–µ–∫—Ç–æ–≤ –Ω–µ –ø–æ–ø–∞–¥–∞—é—Ç –≤ –∫–∞–∂–¥—ã–π bootstrap sample.  
–ò—Å–ø–æ–ª—å–∑—É–µ–º –∏—Ö –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–∏ **–±–µ–∑ –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ validation set**!

**OOB Prediction –¥–ª—è –æ–±—ä–µ–∫—Ç–∞ $\mathbf{x}_i$:**
$$
\hat{y}_{OOB}(\mathbf{x}_i) = \text{average}\{f_b(\mathbf{x}_i) : \mathbf{x}_i \notin D_b\}
$$

**OOB Error:**
$$
\text{OOB\_Error} = \frac{1}{n} \sum_{i=1}^{n} L(y_i, \hat{y}_{OOB}(\mathbf{x}_i))
$$

**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ:**  
"–ë–µ—Å–ø–ª–∞—Ç–Ω–∞—è" –≤–∞–ª–∏–¥–∞—Ü–∏—è (–Ω–µ –Ω—É–∂–µ–Ω –æ—Ç–¥–µ–ª—å–Ω—ã–π validation set), –ø–æ—á—Ç–∏ —ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç–Ω–∞ cross-validation!

**–ü—Ä–æ–±–ª–µ–º–∞ 3: –ß–∏—Å–ª–æ –º–æ–¥–µ–ª–µ–π B**

**–¢–µ–æ—Ä–∏—è:**  
–ß–µ–º –±–æ–ª—å—à–µ B, —Ç–µ–º –º–µ–Ω—å—à–µ variance. –ù–æ **—É–±—ã–≤–∞—é—â–∞—è –æ—Ç–¥–∞—á–∞** (diminishing returns).

**–ü—Ä–∞–∫—Ç–∏–∫–∞:**
- B = 50-100: —É–∂–µ —Ö–æ—Ä–æ—à–æ  
- B = 500-1000: —Å—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è Random Forest  
- B > 1000: —Ä–µ–¥–∫–æ –Ω—É–∂–Ω–æ (–º–∞–ª–æ –ø—Ä–∏—Ä–æ—Å—Ç–∞)

**–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥:**  
–ü–æ—Å—Ç—Ä–æ–π—Ç–µ –≥—Ä–∞—Ñ–∏–∫ OOB Error vs B ‚Üí –Ω–∞–π–¥–∏—Ç–µ "–∫–æ–ª–µ–Ω–æ" (plateau).

**–ü—Ä–æ–±–ª–µ–º–∞ 4: –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å**

Bagging –∞–Ω—Å–∞–º–±–ª—å –∏–∑ 100 –¥–µ—Ä–µ–≤—å–µ–≤ ‚Üí —Å–ª–æ–∂–Ω–æ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä–æ–≤–∞—Ç—å.

**–†–µ—à–µ–Ω–∏–µ:**
- **Feature Importance**: —É—Å—Ä–µ–¥–Ω–∏—Ç—å importance –ø–æ –≤—Å–µ–º –º–æ–¥–µ–ª—è–º
- **Partial Dependence Plots**: –≤–ª–∏—è–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∞ –Ω–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ

### 5) –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞–º–∏

| –ú–µ—Ç–æ–¥ | –¶–µ–ª—å | –ö–∞–∫ —Å–Ω–∏–∂–∞–µ—Ç –æ—à–∏–±–∫—É | –ë–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ | –ü–∞—Ä–∞–ª–ª–µ–ª–∏–∑–∞—Ü–∏—è |
|-------|------|-------------------|---------------|----------------|
| **Bagging** | –°–Ω–∏–∂–µ–Ω–∏–µ variance | –£—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã—Ö –º–æ–¥–µ–ª–µ–π | High variance (deep trees) | ‚úÖ –õ–µ–≥–∫–æ (–º–æ–¥–µ–ª–∏ –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã) |
| **Boosting** | –°–Ω–∏–∂–µ–Ω–∏–µ bias | –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–∞—è –∫–æ—Ä—Ä–µ–∫—Ü–∏—è –æ—à–∏–±–æ–∫ | Low variance (shallow trees) | ‚ùå –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ |
| **Stacking** | –ö–æ–º–±–∏–Ω–∞—Ü–∏—è | –ú–µ—Ç–∞-–º–æ–¥–µ–ª—å —É—á–∏—Ç—Å—è –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞—Ç—å | –†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ | ‚ö†Ô∏è –ß–∞—Å—Ç–∏—á–Ω–æ |

---

## 5.2 Random Forest

### 1) –ò–Ω—Ç—É–∏—Ü–∏—è –∏ —Å—É—Ç—å

**–ü—Ä–æ—Å—Ç—ã–º–∏ —Å–ª–æ–≤–∞–º–∏ (ELI5):**  
–ü—Ä–µ–¥—Å—Ç–∞–≤—å, —á—Ç–æ —É —Ç–µ–±—è –µ—Å—Ç—å –∫–æ–º–∞–Ω–¥–∞ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ (–¥–µ—Ä–µ–≤—å–µ–≤), –∏ –∫–∞–∂–¥—ã–π —ç–∫—Å–ø–µ—Ä—Ç:
1. –°–º–æ—Ç—Ä–∏—Ç –Ω–∞ **—Å–ª—É—á–∞–π–Ω—É—é —á–∞—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö** (bootstrap)
2. –†–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç —Ç–æ–ª—å–∫–æ **—Å–ª—É—á–∞–π–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏** –ø—Ä–∏ –∫–∞–∂–¥–æ–º —Ä–µ—à–µ–Ω–∏–∏

–ü–æ—Ç–æ–º –≤—Å–µ –≥–æ–ª–æ—Å—É—é—Ç ‚Üí **–∫–æ–ª–ª–µ–∫—Ç–∏–≤–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ**. –≠—Ç–æ Random Forest!

**–ê–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏:**  
Random Forest = **Bagging** + **Random Feature Selection**. –û–±—É—á–∞–µ—Ç—Å—è –∞–Ω—Å–∞–º–±–ª—å —Ä–µ—à–∞—é—â–∏—Ö –¥–µ—Ä–µ–≤—å–µ–≤, –≥–¥–µ –∫–∞–∂–¥–æ–µ –¥–µ—Ä–µ–≤–æ:
- –û–±—É—á–∞–µ—Ç—Å—è –Ω–∞ bootstrap sample
- –ü—Ä–∏ –∫–∞–∂–¥–æ–º split —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç —Ç–æ–ª—å–∫–æ —Å–ª—É—á–∞–π–Ω–æ–µ –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤

**–§–∏–∑–∏—á–µ—Å–∫–∏–π —Å–º—ã—Å–ª:**  
–°–ª—É—á–∞–π–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–µ–ª–∞—é—Ç –¥–µ—Ä–µ–≤—å—è **–º–µ–Ω–µ–µ –∫–æ—Ä—Ä–µ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏** ‚Üí —É—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ —Å–Ω–∏–∂–∞–µ—Ç variance.

### 2) –ó–∞—á–µ–º –∏ –∫–æ–≥–¥–∞ (Business Value)

**–ö–∞–∫—É—é –ø—Ä–æ–±–ª–µ–º—É —Ä–µ—à–∞–µ—Ç:**
- **–ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ** decision trees
- **–í—ã—Å–æ–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å** "–∏–∑ –∫–æ—Ä–æ–±–∫–∏" (–º–∞–ª–æ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤)
- **Feature Importance** –¥–ª—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏
- **–†–∞–±–æ—Ç–∞–µ—Ç** —Å –º–∞–ª—ã–º–∏ –∏ –±–æ–ª—å—à–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏

**–ì–¥–µ —ç—Ç–æ –∏–Ω–¥—É—Å—Ç—Ä–∏–∞–ª—å–Ω—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç (—Ä–∞–±–æ—á–∞—è –ª–æ—à–∞–¥–∫–∞!):**
- **–í—Å—ë!** –¢–∞–±–ª–∏—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ: Healthcare, Finance, E-commerce, Insurance
- **Kaggle**: top baseline –º–æ–¥–µ–ª—å
- **–ë–∞–Ω–∫–∏**: –∫—Ä–µ–¥–∏—Ç–Ω—ã–π —Å–∫–æ—Ä–∏–Ω–≥
- **–ú–µ–¥–∏—Ü–∏–Ω–∞**: –ø—Ä–æ–≥–Ω–æ–∑ –∑–∞–±–æ–ª–µ–≤–∞–Ω–∏–π

**–ö–æ–≥–¥–∞ –ø—Ä–∏–º–µ–Ω—è—Ç—å –Ω–µ–ª—å–∑—è:**
- ‚ùå –≠–∫—Å—Ç—Ä–∞–ø–æ–ª—è—Ü–∏—è (–¥–µ—Ä–µ–≤—å—è –Ω–µ –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—é—Ç –∑–∞ –ø—Ä–µ–¥–µ–ª–∞–º–∏ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö)
- ‚ùå –û—á–µ–Ω—å –≤—ã—Å–æ–∫–æ—Ä–∞–∑–º–µ—Ä–Ω—ã–µ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (text, images) ‚Äî –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ linear models –∏–ª–∏ deep learning

### 3) –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —è–¥—Ä–æ

**–ê–ª–≥–æ—Ä–∏—Ç–º Random Forest:**
–î–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–µ—Ä–µ–≤–∞ $t = 1$ –¥–æ $T$:
1. –°—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å bootstrap –≤—ã–±–æ—Ä–∫—É $D_t \subset D$.
2. –ü–æ—Å—Ç—Ä–æ–∏—Ç—å –¥–µ—Ä–µ–≤–æ: –Ω–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ –≤—ã–±–∏—Ä–∞—Ç—å –ª—É—á—à–∏–π —Å–ø–ª–∏—Ç —Ç–æ–ª—å–∫–æ –∏–∑ **m —Å–ª—É—á–∞–π–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤** (–æ–±—ã—á–Ω–æ $m = \sqrt{p}$).

**–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ (soft voting):**
$$
P(y = c | \mathbf{x}) = \frac{1}{T} \sum_{t=1}^{T} I\{\text{tree}_t(\mathbf{x}) = c\}
$$

> [!IMPORTANT]
> **Random Forest Hyperparameters:**
> - `n_estimators`: –ß–∏—Å–ª–æ –¥–µ—Ä–µ–≤—å–µ–≤. –ë–æ–ª—å—à–µ = –ª—É—á—à–µ (–¥–æ –ø–ª–∞—Ç–æ). –ù–µ –ø–µ—Ä–µ–æ–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å!
> - `max_features`: –ì–ª–∞–≤–Ω—ã–π —Ä—ã—á–∞–≥ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è. –ß–µ–º –º–µ–Ω—å—à–µ, —Ç–µ–º –±–æ–ª–µ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã (–¥–µ–∫–æ—Ä—Ä–µ–ª–∏—Ä–æ–≤–∞–Ω—ã) –¥–µ—Ä–µ–≤—å—è.
> - `max_depth`: –ì–ª—É–±–∏–Ω–∞. –î–ª—è RF –æ–±—ã—á–Ω–æ —Å—Ç–∞–≤—è—Ç `None` –∏–ª–∏ –æ—á–µ–Ω—å –≥–ª—É–±–æ–∫–∏–µ –¥–µ—Ä–µ–≤—å—è, —Ç–∞–∫ –∫–∞–∫ –∞–Ω—Å–∞–º–±–ª—å –±–æ—Ä–µ—Ç—Å—è —Å –∏—Ö –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ–º.

> [!TIP]
> **–ê–Ω–∞–ª–æ–≥–∏—è: –°—É–¥ –ø—Ä–∏—Å—è–∂–Ω—ã—Ö —Å —É–∑–∫–æ–π —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–µ–π**  
> –ü—Ä–µ–¥—Å—Ç–∞–≤—å—Ç–µ, —á—Ç–æ –≤ —Å—É–¥–µ 12 –ø—Ä–∏—Å—è–∂–Ω—ã—Ö, –Ω–æ –∫–∞–∂–¥–æ–º—É —Ä–∞–∑—Ä–µ—à–∏–ª–∏ —Å–º–æ—Ç—Ä–µ—Ç—å —Ç–æ–ª—å–∫–æ –Ω–∞ 3 —Å–ª—É—á–∞–π–Ω—ã–µ —É–ª–∏–∫–∏ –∏–∑ 10. –ï—Å–ª–∏ –∫–∞–∂–¥—ã–π –≤—ã–Ω–µ—Å–µ—Ç —Ä–µ—à–µ–Ω–∏–µ, –∏ –º—ã –∏—Ö —É—Å—Ä–µ–¥–Ω–∏–º, –∏—Ç–æ–≥–æ–≤—ã–π –≤–µ—Ä–¥–∏–∫—Ç –±—É–¥–µ—Ç –≥–æ—Ä–∞–∑–¥–æ –æ–±—ä–µ–∫—Ç–∏–≤–Ω–µ–µ, —á–µ–º –µ—Å–ª–∏ –±—ã –≤—Å–µ 12 —Å–º–æ—Ç—Ä–µ–ª–∏ –Ω–∞ –æ–¥–Ω—É –∏ —Ç—É –∂–µ —Å–∞–º—É—é —è—Ä–∫—É—é (–Ω–æ, –≤–æ–∑–º–æ–∂–Ω–æ, –æ–±–º–∞–Ω—á–∏–≤—É—é) —É–ª–∏–∫—É.

### 4) Middle-level –Ω—é–∞–Ω—Å—ã

**–ü—Ä–æ–±–ª–µ–º–∞ 1: –ó–∞—á–µ–º Random Feature Selection?**

**–ë–µ–∑ random features (–æ–±—ã—á–Ω—ã–π bagging):**  
–î–µ—Ä–µ–≤—å—è **–∫–æ—Ä—Ä–µ–ª–∏—Ä–æ–≤–∞–Ω—ã**: –≤—Å–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç –æ–¥–∏–Ω –∏ —Ç–æ—Ç –∂–µ —Å–∏–ª—å–Ω—ã–π –ø—Ä–∏–∑–Ω–∞–∫ –Ω–∞ –ø–µ—Ä–≤–æ–º split.

**–ü—Ä–∏–º–µ—Ä:**
```
–î–∞–Ω–Ω—ã–µ: –ø—Ä–æ–≥–Ω–æ–∑ —Ü–µ–Ω—ã –¥–æ–º–∞
–ü—Ä–∏–∑–Ω–∞–∫–∏: –ø–ª–æ—â–∞–¥—å, —Ä–∞–π–æ–Ω, –≥–æ–¥ –ø–æ—Å—Ç—Ä–æ–π–∫–∏, ...
–í—Å–µ –¥–µ—Ä–µ–≤—å—è —Å–¥–µ–ª–∞—é—Ç –ø–µ—Ä–≤—ã–π split –ø–æ "–ø–ª–æ—â–∞–¥—å" (—Å–∞–º—ã–π –≤–∞–∂–Ω—ã–π)
‚Üí –¥–µ—Ä–µ–≤—å—è –ø–æ—Ö–æ–∂–∏ ‚Üí —É—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ –º–µ–Ω–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ
```

**–° random features:**  
–î–µ—Ä–µ–≤—å—è **—Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã** (decorrelated) ‚Üí —É—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ —Å–Ω–∏–∂–∞–µ—Ç variance –ª—É—á—à–µ!

**–ú–∞—Ç–µ–º–∞—Ç–∏–∫–∞:**

$$
\text{Var}(\text{average}) = \rho \sigma^2 + \frac{1 - \rho}{T} \sigma^2
$$

Random features ‚Üí œÅ ‚Üì ‚Üí variance ‚Üì‚Üì

**–ü—Ä–æ–±–ª–µ–º–∞ 2: Feature Importance**

**–ö–∞–∫ –∏–∑–º–µ—Ä—è–µ—Ç—Å—è:**

**Impurity-based (Gini importance):**
$$
\text{Importance}(X_j) = \sum_{t=1}^{T} \sum_{\text{splits on } X_j} \Delta \text{Impurity}
$$

–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç—Å—è –Ω–∞ [0, 1], —Å—É–º–º–∞ = 1.

**Permutation Importance (–±–æ–ª–µ–µ –Ω–∞–¥–µ–∂–Ω—ã–π):**
1. –ò–∑–º–µ—Ä–∏—Ç—å OOB error
2. –ü–µ—Ä–µ–º–µ—à–∞—Ç—å (permute) –ø—Ä–∏–∑–Ω–∞–∫ X_j
3. –ò–∑–º–µ—Ä–∏—Ç—å –Ω–æ–≤—ã–π OOB error
4. Importance = —É–≤–µ–ª–∏—á–µ–Ω–∏–µ –æ—à–∏–±–∫–∏

**–ü—Ä–æ–±–ª–µ–º—ã Gini importance:**
- –°–º–µ—â–µ–Ω –∫ **high-cardinality** –ø—Ä–∏–∑–Ω–∞–∫–∞–º (–º–Ω–æ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π)
- –°–º–µ—â–µ–Ω –∫ **–Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–º** –ø—Ä–∏–∑–Ω–∞–∫–∞–º

**–†–µ—à–µ–Ω–∏–µ:**  
–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ **Permutation Importance** –¥–ª—è critical decisions.

**–ü—Ä–æ–±–ª–µ–º–∞ 3: –ß–∏—Å–ª–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ m**

**m —Å–ª–∏—à–∫–æ–º –º–∞–ª:**  
–î–µ—Ä–µ–≤—å—è —Å–ª–∞–±—ã–µ (–Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏) ‚Üí high bias.

**m —Å–ª–∏—à–∫–æ–º –≤–µ–ª–∏–∫:**  
–î–µ—Ä–µ–≤—å—è –∫–æ—Ä—Ä–µ–ª–∏—Ä–æ–≤–∞–Ω—ã ‚Üí bagging –º–µ–Ω–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–µ–Ω.

**–≠–º–ø–∏—Ä–∏—á–µ—Å–∫–∏–µ –ø—Ä–∞–≤–∏–ª–∞:**
- **–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è**: $m = \sqrt{p}$
- **–†–µ–≥—Ä–µ—Å—Å–∏—è**: $m = p/3$

–ü–æ–¥–±–æ—Ä —á–µ—Ä–µ–∑ CV!

**–ü—Ä–æ–±–ª–µ–º–∞ 4: Random Forest –Ω–µ —ç–∫—Å—Ç—Ä–∞–ø–æ–ª–∏—Ä—É–µ—Ç**

–î–µ—Ä–µ–≤—å—è –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—é—Ç —Ç–æ–ª—å–∫–æ –∑–Ω–∞—á–µ–Ω–∏—è –∏–∑ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏ (–ª–∏—Å—Ç—å—è = –∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã).

**–ü—Ä–∏–º–µ—Ä:**
```
–û–±—É—á–∞—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ: –≥–æ–¥—ã 2000-2020, —Ü–µ–Ω—ã $100k-$500k
Test: –≥–æ–¥ 2025, —Ä–µ–∞–ª—å–Ω–∞—è —Ü–µ–Ω–∞ $600k
Random Forest –ø—Ä–µ–¥—Å–∫–∞–∂–µ—Ç max ‚âà $500k (–Ω–µ –≤—ã–π–¥–µ—Ç –∑–∞ –ø—Ä–µ–¥–µ–ª—ã)
```

**–†–µ—à–µ–Ω–∏–µ:**  
–î–ª—è —ç–∫—Å—Ç—Ä–∞–ø–æ–ª—è—Ü–∏–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ **linear models** –∏–ª–∏ **gradient boosting** —Å –æ—Å—Ç–æ—Ä–æ–∂–Ω–æ—Å—Ç—å—é.

### 5) –°—Ä–∞–≤–Ω–µ–Ω–∏–µ

| –ú–µ—Ç–æ–¥ | Bias | Variance | –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å | –°–∫–æ—Ä–æ—Å—Ç—å | Feature Importance |
|-------|------|----------|-------------------|----------|-------------------|
| **Single Decision Tree** | Low (–µ—Å–ª–∏ –≥–ª—É–±–æ–∫–∞—è) | ‚ùå High | ‚úÖ –í—ã—Å–æ–∫–∞—è | ‚úÖ –ë—ã—Å—Ç—Ä–æ | ‚úÖ –î–∞ |
| **Random Forest** | Low | ‚úÖ Low (bagging) | ‚ö†Ô∏è –°—Ä–µ–¥–Ω—è—è | ‚ö†Ô∏è –°—Ä–µ–¥–Ω–µ | ‚úÖ –î–∞ (–∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω–∞—è) |
| **XGBoost** | ‚úÖ Lower (boosting) | Low (—Å regularization) | ‚ö†Ô∏è –°—Ä–µ–¥–Ω—è—è | ‚ö†Ô∏è –ú–µ–¥–ª–µ–Ω–Ω–µ–µ | ‚úÖ –î–∞ |

---

## 5.3 Boosting

### 1) –ò–Ω—Ç—É–∏—Ü–∏—è –∏ —Å—É—Ç—å

**–ü—Ä–æ—Å—Ç—ã–º–∏ —Å–ª–æ–≤–∞–º–∏ (ELI5):**  
–¢—ã —É—á–∏—à—å—Å—è –Ω–∞ –æ—à–∏–±–∫–∞—Ö:
1. –°–¥–∞–ª –ø–µ—Ä–≤—ã–π —ç–∫–∑–∞–º–µ–Ω ‚Üí –ø–æ–ª—É—á–∏–ª –æ—à–∏–±–∫–∏
2. –°–æ—Å—Ä–µ–¥–æ—Ç–æ—á–∏–ª—Å—è –Ω–∞ **–æ—à–∏–±–∫–∞—Ö** (–Ω–µ –∑–Ω–∞–ª —Ç–µ–º—É X) ‚Üí –ø–æ–¥—É—á–∏–ª —Ç–µ–º—É X
3. –ü–µ—Ä–µ—Å–¥–∞–ª ‚Üí –ø–æ–ª—É—á–∏–ª –Ω–æ–≤—ã–µ –æ—à–∏–±–∫–∏ ‚Üí –ø–æ–¥—É—á–∏–ª –µ—â–µ
4. **–ò—Ç–æ–≥–æ–≤—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç** = –∑–Ω–∞–Ω–∏—è –ø–æ—Å–ª–µ –≤—Å–µ—Ö –ø–µ—Ä–µ—Å–¥–∞—á

Boosting —Ç–∞–∫ –∂–µ: **–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ –æ–±—É—á–∞–µ—Ç —Å–ª–∞–±—ã–µ –º–æ–¥–µ–ª–∏**, –∫–∞–∂–¥–∞—è —Ñ–æ–∫—É—Å–∏—Ä—É–µ—Ç—Å—è –Ω–∞ –æ—à–∏–±–∫–∞—Ö –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö.

**–ê–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏:**  
Boosting ‚Äî —ç—Ç–æ **sequential ensemble**, –≥–¥–µ –∫–∞–∂–¥–∞—è –Ω–æ–≤–∞—è –º–æ–¥–µ–ª—å –æ–±—É—á–∞–µ—Ç—Å—è –∏—Å–ø—Ä–∞–≤–ª—è—Ç—å **–æ—Å—Ç–∞—Ç–æ—á–Ω—ã–µ –æ—à–∏–±–∫–∏** (residuals) –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –º–æ–¥–µ–ª–µ–π. –û—Å–Ω–æ–≤–Ω–∞—è —Ü–µ–ª—å ‚Äî **—Å–Ω–∏–∂–µ–Ω–∏–µ bias**.

**–§–∏–∑–∏—á–µ—Å–∫–∏–π —Å–º—ã—Å–ª:**  
**–ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫ –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ —Ñ—É–Ω–∫—Ü–∏–π**: –∫–∞–∂–¥–∞—è –Ω–æ–≤–∞—è –º–æ–¥–µ–ª—å ‚Äî —ç—Ç–æ —à–∞–≥ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ —Å–ø—É—Å–∫–∞ –∫ –º–∏–Ω–∏–º—É–º—É —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å.

### 2) –ó–∞—á–µ–º –∏ –∫–æ–≥–¥–∞ (Business Value)

**–ö–∞–∫—É—é –ø—Ä–æ–±–ª–µ–º—É —Ä–µ—à–∞–µ—Ç:**
- **–ù–µ–¥–æ–æ–±—É—á–µ–Ω–∏–µ** —Å–ª–∞–±—ã—Ö –º–æ–¥–µ–ª–µ–π (shallow trees)
- **–í—ã—Å–æ–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å**: –æ–±—ã—á–Ω–æ –ª—É—á—à–µ Random Forest
- **–ì–∏–±–∫–æ—Å—Ç—å**: —Ä–∞–∑–Ω—ã–µ loss functions –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –∑–∞–¥–∞—á

**–ì–¥–µ —ç—Ç–æ –∏–Ω–¥—É—Å—Ç—Ä–∏–∞–ª—å–Ω—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç:**
- **Kaggle**: XGBoost, LightGBM, CatBoost ‚Äî –¥–æ–º–∏–Ω–∏—Ä—É—é—Ç –≤ —Ç–∞–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö!
- **–§–∏–Ω–∞–Ω—Å—ã**: –∫—Ä–µ–¥–∏—Ç–Ω—ã–π —Å–∫–æ—Ä–∏–Ω–≥, fraud detection
- **–†–µ–∫–æ–º–µ–Ω–¥–∞—Ç–µ–ª—å–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã**: ranking
- **AdTech**: CTR prediction

**–ö–æ–≥–¥–∞ –ø—Ä–∏–º–µ–Ω—è—Ç—å –Ω–µ–ª—å–∑—è:**
- ‚ùå –ú–∞–ª—ã–µ –¥–∞–Ω–Ω—ã–µ + –º–Ω–æ–≥–æ —à—É–º–∞ (–ø–µ—Ä–µ–æ–±—É—á–∏—Ç—Å—è, –≤ –æ—Ç–ª–∏—á–∏–µ –æ—Ç RF)
- ‚ùå –î–∞–Ω–Ω—ã–µ —Å –≤—ã–±—Ä–æ—Å–∞–º–∏ (—á—É–≤—Å—Ç–≤–∏—Ç–µ–ª–µ–Ω, –Ω—É–∂–Ω–∞ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è)

### 3) –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —è–¥—Ä–æ

**Gradient Boosting (–æ–±—â–∏–π framework):**

1. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–Ω—Å—Ç–∞–Ω—Ç–æ–π: $F_0(\mathbf{x}) = \arg\min_\gamma \sum L(y_i, \gamma)$.
2. –î–ª—è $m = 1 \dots M$:
   - –í—ã—á–∏—Å–ª–∏—Ç—å –∞–Ω—Ç–∏–≥—Ä–∞–¥–∏–µ–Ω—Ç (–æ—Å—Ç–∞—Ç–∫–∏): $r_{mi} = -\left[\frac{\partial L(y_i, F(\mathbf{x}_i))}{\partial F(\mathbf{x}_i)}\right]_{F=F_{m-1}}$.
   - –û–±—É—á–∏—Ç—å "—Å–ª–∞–±–æ–µ" –¥–µ—Ä–µ–≤–æ $h_m$ –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å —ç—Ç–∏ –æ—Å—Ç–∞—Ç–∫–∏.
   - –û–±–Ω–æ–≤–∏—Ç—å –º–æ–¥–µ–ª—å: $F_m = F_{m-1} + \eta \cdot h_m(\mathbf{x})$.

**–§–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å:**
$$
F(\mathbf{x}) = \sum_{m=0}^{M} \eta \cdot h_m(\mathbf{x})
$$

> [!WARNING]
> **Boosting Overfitting Warning: Learning Rate vs Estimators**  
> –í –±—É—Å—Ç–∏–Ω–≥–µ `learning_rate` ($\eta$) –∏ `n_estimators` ($M$) —Å–≤—è–∑–∞–Ω—ã –æ–±—Ä–∞—Ç–Ω–æ–π –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å—é. –ï—Å–ª–∏ –≤—ã —É–º–µ–Ω—å—à–∞–µ—Ç–µ —à–∞–≥ (–¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç–∏), –≤—ã –û–ë–Ø–ó–ê–ù–´ —É–≤–µ–ª–∏—á–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–µ—Ä–µ–≤—å–µ–≤, –∏–Ω–∞—á–µ –º–æ–¥–µ–ª—å –Ω–µ —É—Å–ø–µ–µ—Ç –¥–æ–π—Ç–∏ –¥–æ –º–∏–Ω–∏–º—É–º–∞. –ò –Ω–∞–æ–±–æ—Ä–æ—Ç ‚Äî –≤—ã—Å–æ–∫–∏–π —à–∞–≥ –±—ã—Å—Ç—Ä–æ –≤–µ–¥–µ—Ç –∫ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—é.

> [!TIP]
> **–ê–Ω–∞–ª–æ–≥–∏—è: –°–∫—É–ª—å–ø—Ç–æ—Ä –∏ –º–µ–ª–∫–∞—è –Ω–∞–∂–¥–∞—á–∫–∞**  
> –ü–µ—Ä–≤–∞—è –º–æ–¥–µ–ª—å –≤ –±—É—Å—Ç–∏–Ω–≥–µ ‚Äî —ç—Ç–æ –≥—Ä—É–±–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞–º–Ω—è (–±–æ–ª—å—à–æ–π bias). –ö–∞–∂–¥–∞—è —Å–ª–µ–¥—É—é—â–∞—è –º–æ–¥–µ–ª—å ‚Äî —ç—Ç–æ –≤—Å–µ –±–æ–ª–µ–µ –º–µ–ª–∫–∞—è –Ω–∞–∂–¥–∞—á–Ω–∞—è –±—É–º–∞–≥–∞, –∫–æ—Ç–æ—Ä–∞—è —É–±–∏—Ä–∞–µ—Ç —à–µ—Ä–æ—Ö–æ–≤–∞—Ç–æ—Å—Ç–∏ (–æ—à–∏–±–∫–∏) –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —ç—Ç–∞–ø–∞. –í –∫–æ–Ω—Ü–µ –º—ã –ø–æ–ª—É—á–∞–µ–º –∏–¥–µ–∞–ª—å–Ω–æ –≥–ª–∞–¥–∫—É—é —Ñ–∏–≥—É—Ä—É. –ù–æ –µ—Å–ª–∏ –ø–µ—Ä–µ–±–æ—Ä—â–∏—Ç—å —Å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –ø—Ä–æ—Ö–æ–¥–æ–≤ –Ω–∞–∂–¥–∞—á–∫–æ–π (—Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –∏—Ç–µ—Ä–∞—Ü–∏–π), –º–æ–∂–Ω–æ —Å—Ç–µ—Ä–µ—Ç—å —Å–∞–º—É —Ñ–∏–≥—É—Ä—É (overfitting).

### 4) Middle-level –Ω—é–∞–Ω—Å—ã

**–ü—Ä–æ–±–ª–µ–º–∞ 1: –ü–æ—á–µ–º—É —Å–Ω–∏–∂–∞–µ—Ç—Å—è bias?**

–ö–∞–∂–¥–∞—è –º–æ–¥–µ–ª—å $h_m$ —É—á–∏—Ç—Å—è –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å –æ—Å—Ç–∞—Ç–∫–∏ ‚Üí –∏—Å–ø—Ä–∞–≤–ª—è–µ—Ç –æ—à–∏–±–∫–∏.

**–†–µ–∑—É–ª—å—Ç–∞—Ç:**
- **Bias** —Å–∏–ª—å–Ω–æ —Å–Ω–∏–∂–∞–µ—Ç—Å—è (–º–æ–¥–µ–ª—å —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è –º–æ—â–Ω–µ–µ)
- **Variance** –º–æ–∂–µ—Ç —Ä–∞—Å—Ç–∏ (—Ä–∏—Å–∫ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è!)

**–û—Ç–ª–∏—á–∏–µ –æ—Ç Bagging:**
- Bagging: variance ‚Üì, bias ‚âà
- Boosting: bias ‚Üì‚Üì, variance ‚Üë (–∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º —á–µ—Ä–µ–∑ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—é)

**–ü—Ä–æ–±–ª–µ–º–∞ 2: Learning Rate (Œ∑, shrinkage)**

**Œ∑ –≤–µ–ª–∏–∫ (Œ∑ = 1):**  
–ë—ã—Å—Ç—Ä–∞—è —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å, –Ω–æ —Ä–∏—Å–∫ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è.

**Œ∑ –º–∞–ª (Œ∑ = 0.01-0.1):**  
–ú–µ–¥–ª–µ–Ω–Ω–∞—è —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å, –Ω–æ –ª—É—á—à–µ –æ–±–æ–±—â–µ–Ω–∏–µ.

**Trade-off:**
```
–ú–µ–Ω—å—à–µ Œ∑ ‚Üí –Ω—É–∂–Ω–æ –±–æ–ª—å—à–µ –¥–µ—Ä–µ–≤—å–µ–≤ M
```

**–≠–º–ø–∏—Ä–∏—á–µ—Å–∫–æ–µ –ø—Ä–∞–≤–∏–ª–æ:**
- Œ∑ = 0.1, M = 100-500 (—Å—Ç–∞–Ω–¥–∞—Ä—Ç)
- Œ∑ = 0.01, M = 1000-5000 (–ª—É—á—à–µ, –Ω–æ –º–µ–¥–ª–µ–Ω–Ω–µ–µ)

**–ü—Ä–æ–±–ª–µ–º–∞ 3: Regularization**

**–ü—Ä–æ–±–ª–µ–º–∞:**  
Boosting sequentially ‚Üí –ª–µ–≥–∫–æ –ø–µ—Ä–µ–æ–±—É—á–∞–µ—Ç—Å—è –Ω–∞ —à—É–º–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.

**–†–µ—à–µ–Ω–∏—è:**

1. **Shrinkage (Œ∑ < 1)**: —É–º–µ–Ω—å—à–∞–µ—Ç –≤–∫–ª–∞–¥ –∫–∞–∂–¥–æ–≥–æ –¥–µ—Ä–µ–≤–∞
2. **Tree constraints**:
   - max_depth = 3-6 (shallow trees ‚Äî weak learners!)
   - min_samples_leaf = 5-20
3. **Subsampling (stochastic GB):**
   - –û–±—É—á–∞—Ç—å –∫–∞–∂–¥–æ–µ –¥–µ—Ä–µ–≤–æ –Ω–∞ —Å–ª—É—á–∞–π–Ω–æ–π —á–∞—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö (–∫–∞–∫ –≤ Random Forest)
4. **Early Stopping:**
   - –ú–æ–Ω–∏—Ç–æ—Ä–∏—Ç—å validation error, –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –ø—Ä–∏ —É—Ö—É–¥—à–µ–Ω–∏–∏

**–ü—Ä–æ–±–ª–µ–º–∞ 4: XGBoost —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞**

**XGBoost** (eXtreme Gradient Boosting) ‚Äî –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è GB:

**–£–ª—É—á—à–µ–Ω–∏—è:**
- **Regularization –≤ loss:**
$$
\text{Obj} = \sum_{i=1}^{n} L(y_i, \hat{y}_i) + \sum_{m=1}^{M} \Omega(h_m)
$$
–≥–¥–µ $\Omega(h) = \gamma T + \frac{\lambda}{2}\|\mathbf{w}\|^2$ ($T$ ‚Äî —á–∏—Å–ª–æ –ª–∏—Å—Ç—å–µ–≤, $\mathbf{w}$ ‚Äî –≤–µ—Å–∞ –ª–∏—Å—Ç—å–µ–≤).
  
- **Second-order approximation** (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≤—Ç–æ—Ä—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—É—é ‚Äî Hessian)
- **–ü–∞—Ä–∞–ª–ª–µ–ª–∏–∑–∞—Ü–∏—è** –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –¥–µ—Ä–µ–≤—å–µ–≤
- **Handling missing values** –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏

**–ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã XGBoost:**
- `n_estimators` (M): —á–∏—Å–ª–æ –¥–µ—Ä–µ–≤—å–µ–≤
- `learning_rate` (Œ∑): 0.01-0.3
- `max_depth`: 3-10
- `subsample`: 0.5-1.0 (–¥–æ–ª—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–µ—Ä–µ–≤–∞)
- `colsample_bytree`: 0.5-1.0 (–¥–æ–ª—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤)
- `lambda` (L2), `alpha` (L1): —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤

### 5) –°—Ä–∞–≤–Ω–µ–Ω–∏–µ Bagging vs Boosting

| –ê—Å–ø–µ–∫—Ç | Bagging (Random Forest) | Boosting (XGBoost) |
|--------|------------------------|-------------------|
| **–û–±—É—á–µ–Ω–∏–µ** | –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ | ‚ùå –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ |
| **–¶–µ–ª—å** | ‚Üì Variance | ‚Üì Bias |
| **–ë–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏** | Deep trees (high variance) | Shallow trees (low variance, high bias) |
| **–ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ** | ‚úÖ –£—Å—Ç–æ–π—á–∏–≤ | ‚ö†Ô∏è –ú–æ–∂–µ—Ç –ø–µ—Ä–µ–æ–±—É—á–∏—Ç—å—Å—è (–Ω—É–∂–Ω–∞ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è) |
| **–°–∫–æ—Ä–æ—Å—Ç—å** | ‚úÖ –ë—ã—Å—Ç—Ä–æ (–ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ—Å—Ç—å) | ‚ö†Ô∏è –ú–µ–¥–ª–µ–Ω–Ω–µ–µ |
| **–¢–æ—á–Ω–æ—Å—Ç—å** | ‚úÖ –•–æ—Ä–æ—à–∞—è | ‚úÖ‚úÖ –û–±—ã—á–Ω–æ –≤—ã—à–µ |
| **–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è** | ‚ö†Ô∏è –°–ª–æ–∂–Ω–æ | ‚ö†Ô∏è –°–ª–æ–∂–Ω–æ |

---

## 5.4 Stacking (Stacked Generalization)

### 1) –ò–Ω—Ç—É–∏—Ü–∏—è –∏ —Å—É—Ç—å

**–ü—Ä–æ—Å—Ç—ã–º–∏ —Å–ª–æ–≤–∞–º–∏ (ELI5):**  
–£ —Ç–µ–±—è –µ—Å—Ç—å 3 –¥—Ä—É–≥–∞-—ç–∫—Å–ø–µ—Ä—Ç–∞: –º–∞—Ç–µ–º–∞—Ç–∏–∫, —Ñ–∏–∑–∏–∫, —Ö–∏–º–∏–∫. –ö–∞–∂–¥—ã–π —Ä–µ—à–∞–µ—Ç –∑–∞–¥–∞—á—É –ø–æ-—Å–≤–æ–µ–º—É. –í–º–µ—Å—Ç–æ –ø—Ä–æ—Å—Ç–æ–≥–æ –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏—è, —Ç—ã –¥–∞–µ—à—å –∏—Ö –æ—Ç–≤–µ—Ç—ã **—á–µ—Ç–≤–µ—Ä—Ç–æ–º—É —ç–∫—Å–ø–µ—Ä—Ç—É-–∞—Ä–±–∏—Ç—Ä—É**, –∫–æ—Ç–æ—Ä—ã–π —É—á–∏–ª—Å—è **–∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞—Ç—å** –∏—Ö –º–Ω–µ–Ω–∏—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ.

**–ê–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏:**  
Stacking ‚Äî —ç—Ç–æ **meta-learning**: –æ–±—É—á–∞–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (level-0), –∑–∞—Ç–µ–º –æ–±—É—á–∞–µ–º **–º–µ—Ç–∞-–º–æ–¥–µ–ª—å** (level-1), –∫–æ—Ç–æ—Ä–∞—è —É—á–∏—Ç—Å—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π.

### 2) –ó–∞—á–µ–º –∏ –∫–æ–≥–¥–∞ (Business Value)

**–ö–∞–∫—É—é –ø—Ä–æ–±–ª–µ–º—É —Ä–µ—à–∞–µ—Ç:**
- **–ö–æ–º–±–∏–Ω–∞—Ü–∏—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π** (–ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è + Random Forest + SVM)
- **–£–ª—É—á—à–µ–Ω–∏–µ** –ø—Ä–æ–≥–Ω–æ–∑–∞ –∑–∞ —Å—á–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏

**–ì–¥–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è:**
- **Kaggle**: winning solutions —á–∞—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É—é—Ç stacking
- **–ö—Ä–∏—Ç–∏—á–Ω—ã–µ –∑–∞–¥–∞—á–∏**: –º–µ–¥–∏—Ü–∏–Ω–∞, —Ñ–∏–Ω–∞–Ω—Å—ã (–≤—ã–∂–∏–º–∞–µ–º –º–∞–∫—Å–∏–º—É–º —Ç–æ—á–Ω–æ—Å—Ç–∏)

**–ö–æ–≥–¥–∞ –ø—Ä–∏–º–µ–Ω—è—Ç—å –Ω–µ–ª—å–∑—è:**
- ‚ùå –ú–∞–ª—ã–µ –¥–∞–Ω–Ω—ã–µ (—Ä–∏—Å–∫ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è –º–µ—Ç–∞-–º–æ–¥–µ–ª–∏)
- ‚ùå –ü—Ä–æ–¥–∞–∫—à–Ω —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è–º–∏ –Ω–∞ latency (–º–Ω–æ–≥–æ –º–æ–¥–µ–ª–µ–π)

### 3) –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —è–¥—Ä–æ

**–ê–ª–≥–æ—Ä–∏—Ç–º:**

1. **–û–±—É—á–∏—Ç—å –±–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ f_1, ..., f_K:**
   - –†–∞–∑–Ω—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã (LogReg, RF, XGBoost, ...)
   - –° cross-validation –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è leakage:
     ```
     –î–ª—è –∫–∞–∂–¥–æ–≥–æ fold:
       –û–±—É—á–∏—Ç—å f_k –Ω–∞ train fold
       –ü—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å –Ω–∞ validation fold ‚Üí meta-feature
     ```

2. **–°–æ–∑–¥–∞—Ç—å meta-features:**
   ```
   X_meta = [f_1(x), f_2(x), ..., f_K(x)]
   ```

3. **–û–±—É—á–∏—Ç—å –º–µ—Ç–∞-–º–æ–¥–µ–ª—å g –Ω–∞ (X_meta, y):**
   ```
   g: ‚Ñù^K ‚Üí ‚Ñù  (–∏–ª–∏ {0, 1} –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏)
   ```
   –û–±—ã—á–Ω–æ –ø—Ä–æ—Å—Ç–∞—è –º–æ–¥–µ–ª—å: LogReg, Ridge.

4. **–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ:**
   ```
   ≈∑ = g(f_1(x), ..., f_K(x))
   ```

### 4) Middle-level –Ω—é–∞–Ω—Å—ã

**–ü—Ä–æ–±–ª–µ–º–∞ 1: Data Leakage**

**–ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ:**
```python
# –û–±—É—á–∏—Ç—å –±–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –≤—Å–µ—Ö train
f1.fit(X_train, y_train)
# –ü—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å –Ω–∞ train ‚Üí meta-features
meta_X_train = f1.predict(X_train)  # ‚Üê leakage!
```

–ú–µ—Ç–∞-–º–æ–¥–µ–ª—å –≤–∏–¥–∏—Ç "–∏–¥–µ–∞–ª—å–Ω—ã–µ" –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ train ‚Üí –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ!

**–ü—Ä–∞–≤–∏–ª—å–Ω–æ (cross-validation):**
```python
from sklearn.model_selection import cross_val_predict
meta_X_train = cross_val_predict(f1, X_train, y_train, cv=5)
```

**–ü—Ä–æ–±–ª–µ–º–∞ 2: –í—ã–±–æ—Ä –º–µ—Ç–∞-–º–æ–¥–µ–ª–∏**

**–ü—Ä–æ—Å—Ç–∞—è –º–æ–¥–µ–ª—å** (Linear Regression, Logistic Regression):
- ‚úÖ –ú–µ–Ω—å—à–µ —Ä–∏—Å–∫ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
- ‚úÖ –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è: –≤–µ—Å–∞ = –≤–∞–∂–Ω–æ—Å—Ç—å –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π

**–°–ª–æ–∂–Ω–∞—è –º–æ–¥–µ–ª—å** (XGBoost):
- ‚ö†Ô∏è –†–∏—Å–∫ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è (–º–µ—Ç–∞-—Ñ–∏—á–µ–π –º–∞–ª–æ ‚Äî K —à—Ç—É–∫!)
- –ú–æ–∂–µ—Ç –Ω–∞–π—Ç–∏ –Ω–µ–ª–∏–Ω–µ–π–Ω—ã–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è:**  
–ù–∞—á–Ω–∏—Ç–µ —Å Ridge/LogReg. –ï—Å–ª–∏ –º–∞–ª–æ –º–æ–¥–µ–ª–µ–π (K < 5), –æ—Å—Ç–∞–≤–∞–π—Ç–µ—Å—å –ø—Ä–æ—Å—Ç—ã–º–∏!

### 5) –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª–µ–π

| –ú–µ—Ç–æ–¥ | –ë–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ | –ö–æ–º–±–∏–Ω–∞—Ü–∏—è | –¶–µ–ª—å | –°–ª–æ–∂–Ω–æ—Å—Ç—å |
|-------|---------------|-----------|------|-----------|
| **Bagging** | –û–¥–∏–Ω–∞–∫–æ–≤—ã–µ (–¥–µ—Ä–µ–≤—å—è) | –£—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ | ‚Üì variance | ‚úÖ –ü—Ä–æ—Å—Ç–æ–π |
| **Boosting** | –û–¥–∏–Ω–∞–∫–æ–≤—ã–µ (–¥–µ—Ä–µ–≤—å—è) | –í–∑–≤–µ—à–µ–Ω–Ω–∞—è —Å—É–º–º–∞ (–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ) | ‚Üì bias | ‚ö†Ô∏è –°—Ä–µ–¥–Ω–∏–π |
| **Stacking** | –†–∞–∑–Ω—ã–µ | –û–±—É—á–µ–Ω–Ω–∞—è –º–µ—Ç–∞-–º–æ–¥–µ–ª—å | –û–ø—Ç–∏–º–∞–ª—å–Ω–∞—è –∫–æ–º–±–∏–Ω–∞—Ü–∏—è | ‚ö†Ô∏è –°–ª–æ–∂–Ω—ã–π |

---

## üìù –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –∫–æ–¥ (sklearn)

### Random Forest —Å hyperparameters

```python
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.model_selection import GridSearchCV

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# –ì–ò–ü–ï–†–ü–ê–†–ê–ú–ï–¢–†–´ RANDOM FOREST
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# n_estimators: —á–∏—Å–ª–æ –¥–µ—Ä–µ–≤—å–µ–≤
#   - 100: default, —Ö–æ—Ä–æ—à–æ –¥–ª—è —Å—Ç–∞—Ä—Ç–∞
#   - 500-1000: –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏
#   - –ë–æ–ª—å—à–µ –¥–µ—Ä–µ–≤—å–µ–≤ = –º–µ–Ω—å—à–µ variance (–Ω–æ diminishing returns)
#
# max_features: —á–∏—Å–ª–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è split
#   - 'sqrt': ‚àöp –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ (default)
#   - 'log2': log‚ÇÇ(p)
#   - 0.3-0.5: –¥–æ–ª—è –æ—Ç p (–¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ —á–∞—Å—Ç–æ p/3)
#
# max_depth: –≥–ª—É–±–∏–Ω–∞ –¥–µ—Ä–µ–≤—å–µ–≤
#   - None: —Ä–∞—Å—Ç–∏—Ç—å –¥–æ –∫–æ–Ω—Ü–∞ (default, high variance per tree)
#   - 10-20: –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–ª—è –∫–æ–Ω—Ç—Ä–æ–ª—è overfitting
#
# min_samples_split, min_samples_leaf: –∫–æ–Ω—Ç—Ä–æ–ª—å –ª–∏—Å—Ç—å–µ–≤
#   - –£–≤–µ–ª–∏—á–∏—Ç—å –ø—Ä–∏ overfitting
#
# oob_score: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å OOB –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ ("–±–µ—Å–ø–ª–∞—Ç–Ω—ã–π" CV!)
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

# –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
rf_clf = RandomForestClassifier(
    n_estimators=500,
    max_features='sqrt',
    max_depth=None,
    min_samples_leaf=1,
    oob_score=True,       # ‚Üê "–±–µ—Å–ø–ª–∞—Ç–Ω–∞—è" –≤–∞–ª–∏–¥–∞—Ü–∏—è
    n_jobs=-1,            # –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–∞—Ü–∏—è –Ω–∞ –≤—Å–µ—Ö —è–¥—Ä–∞—Ö
    random_state=42
)
rf_clf.fit(X_train, y_train)
print(f"OOB Score: {rf_clf.oob_score_:.3f}")

# Feature Importance
import pandas as pd
importance = pd.DataFrame({
    'feature': X_train.columns,
    'importance': rf_clf.feature_importances_
}).sort_values('importance', ascending=False)
```

### XGBoost —Å hyperparameters

```python
from xgboost import XGBClassifier, XGBRegressor
from sklearn.model_selection import GridSearchCV

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# –ì–ò–ü–ï–†–ü–ê–†–ê–ú–ï–¢–†–´ XGBOOST
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# n_estimators: —á–∏—Å–ª–æ –¥–µ—Ä–µ–≤—å–µ–≤ (100-1000)
#
# learning_rate (Œ∑): shrinkage
#   - 0.1: —Å—Ç–∞–Ω–¥–∞—Ä—Ç
#   - 0.01-0.05: –ª—É—á—à–µ –≥–µ–Ω–µ—Ä–∞–ª–∏–∑–∞—Ü–∏—è, –Ω—É–∂–Ω–æ –±–æ–ª—å—à–µ –¥–µ—Ä–µ–≤—å–µ–≤
#
# max_depth: –≥–ª—É–±–∏–Ω–∞ –¥–µ—Ä–µ–≤—å–µ–≤
#   - 3-6: —Å—Ç–∞–Ω–¥–∞—Ä—Ç (shallow trees = weak learners!)
#   - –ì–ª—É–±–∂–µ ‚Üí —Ä–∏—Å–∫ overfitting
#
# subsample: –¥–æ–ª—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–µ—Ä–µ–≤–∞
#   - 0.8: —Å—Ç–∞–Ω–¥–∞—Ä—Ç
#   - <1.0: —Å—Ç–æ—Ö–∞—Å—Ç–∏—á–µ—Å–∫–∏–π GB, —Å–Ω–∏–∂–∞–µ—Ç overfitting
#
# colsample_bytree: –¥–æ–ª—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–µ—Ä–µ–≤–∞
#   - 0.8: —Å—Ç–∞–Ω–¥–∞—Ä—Ç
#
# reg_lambda (L2), reg_alpha (L1): —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
#   - 1.0: default –¥–ª—è lambda
#   - –£–≤–µ–ª–∏—á–∏—Ç—å –ø—Ä–∏ overfitting
#
# early_stopping_rounds: –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø—Ä–∏ —É—Ö—É–¥—à–µ–Ω–∏–∏ validation
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

xgb = XGBClassifier(
    n_estimators=500,
    learning_rate=0.1,
    max_depth=5,
    subsample=0.8,
    colsample_bytree=0.8,
    reg_lambda=1.0,
    reg_alpha=0.0,
    use_label_encoder=False,
    eval_metric='logloss',
    random_state=42
)

# Early Stopping (–ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û!)
xgb.fit(
    X_train, y_train,
    eval_set=[(X_val, y_val)],
    early_stopping_rounds=50,
    verbose=False
)
print(f"Best iteration: {xgb.best_iteration}")

# GridSearch –¥–ª—è XGBoost
param_grid = {
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.1],
    'n_estimators': [100, 300, 500]
}
grid = GridSearchCV(XGBClassifier(), param_grid, cv=3, scoring='roc_auc')
```

### Stacking —Å sklearn

```python
from sklearn.ensemble import StackingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier

# Stacking: –∫–æ–º–±–∏–Ω–∞—Ü–∏—è —Ä–∞–∑–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
stacking = StackingClassifier(
    estimators=[
        ('rf', RandomForestClassifier(n_estimators=100)),
        ('xgb', XGBClassifier(n_estimators=100, use_label_encoder=False)),
    ],
    final_estimator=LogisticRegression(),  # –º–µ—Ç–∞-–º–æ–¥–µ–ª—å
    cv=5,                                   # CV –¥–ª—è –º–µ—Ç–∞-–ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    stack_method='predict_proba'           # –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –∫–∞–∫ –º–µ—Ç–∞-—Ñ–∏—á–∏
)
stacking.fit(X_train, y_train)
```

### –¢–∞–±–ª–∏—Ü–∞ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤

| –ú–æ–¥–µ–ª—å | –ü–∞—Ä–∞–º–µ—Ç—Ä | Default | –î–∏–∞–ø–∞–∑–æ–Ω | –ö–æ–≥–¥–∞ –º–µ–Ω—è—Ç—å |
|--------|----------|---------|----------|--------------|
| **RF** | n_estimators | 100 | 100-1000 | –ë–æ–ª—å—à–µ = –ª—É—á—à–µ (–¥–æ –ø–ª–∞—Ç–æ) |
| | max_features | sqrt | sqrt, log2, 0.3-1.0 | –ü–æ–¥–±–æ—Ä —á–µ—Ä–µ–∑ CV |
| | max_depth | None | 10-30, None | –ü—Ä–∏ overfitting |
| **XGB** | n_estimators | 100 | 100-5000 | –° early_stopping |
| | learning_rate | 0.3 | 0.01-0.3 | Œ∑‚Üì —Ç—Ä–µ–±—É–µ—Ç n_estimators‚Üë |
| | max_depth | 6 | 3-10 | –ö–ª—é—á–µ–≤–æ–π –¥–ª—è overfitting |
| | subsample | 1.0 | 0.5-1.0 | –ü—Ä–∏ overfitting |

---

## üéØ Q&A –¥–ª—è —ç–∫–∑–∞–º–µ–Ω–∞

**Q1: –ü–æ—á–µ–º—É Random Forest —É—Å—Ç–æ–π—á–∏–≤ –∫ overfitting?**
> Bagging —Å–Ω–∏–∂–∞–µ—Ç variance —á–µ—Ä–µ–∑ —É—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ. –î–∞–∂–µ –µ—Å–ª–∏ –∫–∞–∂–¥–æ–µ –¥–µ—Ä–µ–≤–æ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–æ, –∏—Ö —Å—Ä–µ–¥–Ω–µ–µ ‚Äî –Ω–µ—Ç (–ø—Ä–∏ —É—Å–ª–æ–≤–∏–∏ –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏). Random feature selection –¥–µ–ª–∞–µ—Ç –¥–µ—Ä–µ–≤—å—è –º–µ–Ω–µ–µ –∫–æ—Ä—Ä–µ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ ‚Üí —É—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ.

**Q2: –ß–µ–º –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è Bagging –æ—Ç Boosting?**
> Bagging: –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã—Ö –º–æ–¥–µ–ª–µ–π, —É—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ, —Ü–µ–ª—å ‚Äî —Å–Ω–∏–∑–∏—Ç—å variance. Boosting: –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ, –∫–∞–∂–¥–∞—è –º–æ–¥–µ–ª—å –∏—Å–ø—Ä–∞–≤–ª—è–µ—Ç –æ—à–∏–±–∫–∏ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö, —Ü–µ–ª—å ‚Äî —Å–Ω–∏–∑–∏—Ç—å bias.

**Q3: –ó–∞—á–µ–º –Ω—É–∂–µ–Ω early stopping –≤ XGBoost?**
> Boosting –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ —Å–Ω–∏–∂–∞–µ—Ç bias, –Ω–æ –º–æ–∂–µ—Ç —É–≤–µ–ª–∏—á–∏–≤–∞—Ç—å variance (overfitting). Early stopping –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –æ–±—É—á–µ–Ω–∏–µ –∫–æ–≥–¥–∞ validation error –Ω–∞—á–∏–Ω–∞–µ—Ç —Ä–∞—Å—Ç–∏ ‚Üí –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –±–∞–ª–∞–Ω—Å bias/variance.

**Q4: –ü–æ—á–µ–º—É OOB error –ø–æ—á—Ç–∏ —ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç–µ–Ω CV?**
> –ü—Ä–∏ bootstrap ~37% –¥–∞–Ω–Ω—ã—Ö –Ω–µ –ø–æ–ø–∞–¥–∞–µ—Ç –≤ –∫–∞–∂–¥—ã–π sample (e^(-1) ‚âà 0.368). –≠—Ç–∏ out-of-bag –ø—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏ ‚Üí —ç—Ñ—Ñ–µ–∫—Ç –∫–∞–∫ cross-validation, –Ω–æ "–±–µ—Å–ø–ª–∞—Ç–Ω–æ".

**Q5: –ö–æ–≥–¥–∞ Stacking –ª—É—á—à–µ –ø—Ä–æ—Å—Ç–æ–≥–æ —É—Å—Ä–µ–¥–Ω–µ–Ω–∏—è?**
> –ö–æ–≥–¥–∞ –±–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ —Ä–∞–∑–Ω—ã–µ –ø–æ –ø—Ä–∏—Ä–æ–¥–µ (LogReg + RF + XGB) –∏ –∏–º–µ—é—Ç —Ä–∞–∑–Ω—ã–µ —Å–∏–ª—å–Ω—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã. –ú–µ—Ç–∞-–º–æ–¥–µ–ª—å —É—á–∏—Ç—Å—è –≤–∑–≤–µ—à–∏–≤–∞—Ç—å –∏—Ö –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ. –î–ª—è –ø–æ—Ö–æ–∂–∏—Ö –º–æ–¥–µ–ª–µ–π (10 RF) –ø—Ä–æ—Å—Ç–æ–µ —É—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ.

---

## –†–µ–∑—é–º–µ –º–æ–¥—É–ª—è

–ê–Ω—Å–∞–º–±–ª–∏ ‚Äî —ç—Ç–æ **"–º—É–¥—Ä–æ—Å—Ç—å —Ç–æ–ª–ø—ã"** –≤ ML:
- **Bagging**: —Å–Ω–∏–∂–∞–µ—Ç variance —á–µ—Ä–µ–∑ —É—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã—Ö –º–æ–¥–µ–ª–µ–π, OOB validation
- **Random Forest**: bagging + random features ‚Üí decorrelation, feature importance
- **Boosting**: —Å–Ω–∏–∂–∞–µ—Ç bias —á–µ—Ä–µ–∑ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—É—é –∫–æ—Ä—Ä–µ–∫—Ü–∏—é –æ—à–∏–±–æ–∫, gradient descent –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ —Ñ—É–Ω–∫—Ü–∏–π
- **Stacking**: –º–µ—Ç–∞-–º–æ–¥–µ–ª—å –¥–ª—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π

Random Forest –∏ XGBoost ‚Äî —Ä–∞–±–æ—á–∏–µ –ª–æ—à–∞–¥–∫–∏ –¥–ª—è —Ç–∞–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö!
