# Модуль 5: Ансамбли (Ensemble Methods)

## 5.1 Bagging (Bootstrap Aggregating)

### 1) Интуиция и суть

**Простыми словами (ELI5):**  
У тебя есть сложная задача и 100 друзей. Вместо того чтобы один друг решал задачу (может ошибиться!), ты даешь задачу **каждому**, но даешь им слегка **разные версии условия** (bootstrap samples). Потом **усредняешь** их ответы. Коллективный разум обычно точнее!

**Академически:**  
Bagging — это **ensemble метод**, который обучает множество моделей на **bootstrap samples** (случайные выборки с возвращением из обучающих данных) и **усредняет** их предсказания. Основная цель — **снижение variance**.

**Физический смысл:**  
Усреднение независимых моделей снижает дисперсию: Var(average) = Var(model) / n_models.

### 2) Зачем и когда (Business Value)

**Какую проблему решает:**
- **Переобучение** (overfitting): снижает variance высокодисперсионных моделей
- **Нестабильность** моделей (например, decision trees меняются при малом изменении данных)
- **Улучшение точности** без изменения базового алгоритма

**Где это индустриальный стандарт:**
- **Random Forest** (bagging + random feature selection) — везде!
- **Healthcare**: ансамбли для диагностики
- **Финансы**: прогнозирование рынков
- **Kaggle**: bagging — обязательный инструмент

**Когда применять нельзя:**
- ❌ Базовая модель имеет low variance (например, linear regression) — bagging не поможет
- ❌ Вычислительные ресурсы ограничены (нужно обучать много моделей)

### 3) Математическое ядро

**Алгоритм Bagging:**

1. Для b = 1 до B (число моделей):
   - **Bootstrap sample**: 
     ```
     D_b = sample n объектов из D с возвращением
     ```
   - Обучить модель f_b на D_b

2. **Предсказание (регрессия):**
   ```
   ŷ = (1/B) Σ f_b(x)
   ```

3. **Предсказание (классификация):**
   ```
   ŷ = mode{f_b(x)}  (голосование большинства)
   ```

**Bootstrap Sample:**

Случайная выборка с возвращением:
- Размер = n (как исходный датасет)
- Вероятность, что объект **не попадет** в bootstrap sample:
  ```
  P(не выбран) = (1 - 1/n)^n → e^(-1) ≈ 0.368  (при n → ∞)
  ```
- **Out-of-Bag (OOB)**: ≈ 37% объектов не в sample → можно использовать для валидации!

**Снижение variance (математически):**

Пусть f_b независимы и Var(f_b) = σ².
```
Var(average) = Var((1/B) Σ f_b) = (1/B²) · B · σ² = σ² / B
```

При B → ∞, variance → 0!

**На практике:**  
Модели не полностью независимы (обучаются на похожих данных), но все равно variance снижается значительно.

### 4) Middle-level нюансы

**Проблема 1: Почему снижается variance?**

**Bias-Variance для Bagging:**
- **Bias**: примерно такой же, как у базовой модели (усреднение не снижает bias)
- **Variance**: сильно снижается (усреднение!)

**Следствие:**  
Bagging эффективен для **low-bias, high-variance моделей**:
- ✅ **Decision Trees** (глубокие) — идеально!
- ✅ Neural Networks (сложные)
- ❌ Linear Regression (уже low variance) — бесполезно

**Проблема 2: Out-of-Bag (OOB) Error**

**Идея:**  
≈ 37% объектов не попадают в каждый bootstrap sample.  
Используем их для оценки модели **без отдельного validation set**!

**OOB Prediction для объекта x_i:**
```
ŷ_OOB(x_i) = average{f_b(x_i) : x_i ∉ D_b}
```

**OOB Error:**
```
OOB_Error = (1/n) Σ L(y_i, ŷ_OOB(x_i))
```

**Преимущество:**  
"Бесплатная" валидация (не нужен отдельный validation set), почти эквивалентна cross-validation!

**Проблема 3: Число моделей B**

**Теория:**  
Чем больше B, тем меньше variance. Но **убывающая отдача** (diminishing returns).

**Практика:**
- B = 50-100: уже хорошо  
- B = 500-1000: стандарт для Random Forest  
- B > 1000: редко нужно (мало прироста)

**Мониторинг:**  
Постройте график OOB Error vs B → найдите "колено" (plateau).

**Проблема 4: Интерпретируемость**

Bagging ансамбль из 100 деревьев → сложно интерпретировать.

**Решение:**
- **Feature Importance**: усреднить importance по всем моделям
- **Partial Dependence Plots**: влияние признака на предсказание

### 5) Сравнение с альтернативами

| Метод | Цель | Как снижает ошибку | Базовые модели | Параллелизация |
|-------|------|-------------------|---------------|----------------|
| **Bagging** | Снижение variance | Усреднение независимых моделей | High variance (deep trees) | ✅ Легко (модели независимы) |
| **Boosting** | Снижение bias | Последовательная коррекция ошибок | Low variance (shallow trees) | ❌ Последовательно |
| **Stacking** | Комбинация | Мета-модель учится комбинировать | Разнообразные | ⚠️ Частично |

---

## 5.2 Random Forest

### 1) Интуиция и суть

**Простыми словами (ELI5):**  
Представь, что у тебя есть команда экспертов (деревьев), и каждый эксперт:
1. Смотрит на **случайную часть данных** (bootstrap)
2. Рассматривает только **случайные признаки** при каждом решении

Потом все голосуют → **коллективное решение**. Это Random Forest!

**Академически:**  
Random Forest = **Bagging** + **Random Feature Selection**. Обучается ансамбль решающих деревьев, где каждое дерево:
- Обучается на bootstrap sample
- При каждом split рассматривает только случайное подмножество признаков

**Физический смысл:**  
Случайные признаки делают деревья **менее коррелированными** → усреднение эффективнее снижает variance.

### 2) Зачем и когда (Business Value)

**Какую проблему решает:**
- **Переобучение** decision trees
- **Высокая точность** "из коробки" (мало гиперпараметров)
- **Feature Importance** для интерпретации
- **Работает** с малыми и большими данными

**Где это индустриальный стандарт (рабочая лошадка!):**
- **Всё!** Табличные данные: Healthcare, Finance, E-commerce, Insurance
- **Kaggle**: top baseline модель
- **Банки**: кредитный скоринг
- **Медицина**: прогноз заболеваний

**Когда применять нельзя:**
- ❌ Экстраполяция (деревья не предсказывают за пределами обучающих данных)
- ❌ Очень высокоразмерные разреженные данные (text, images) — используйте linear models или deep learning

### 3) Математическое ядро

**Алгоритм Random Forest:**

Для каждого дерева t = 1 до T:
1. **Bootstrap sample**: D_t ⊂ D (с возвращением)
2. Построить дерево:
   - **При каждом split**:
     - Выбрать **m случайных признаков** из p
     - Найти лучший split среди этих m
   - Рост до максимальной глубины или min_samples_leaf

**Гиперпараметры:**
- **m (max_features)**: число случайных признаков
  - Регрессия: m = p/3
  - Классификация: m = √p
- **T (n_estimators)**: число деревьев (100-500)
- **max_depth**: глубина деревьев (обычно None — растим до конца)
- **min_samples_split, min_samples_leaf**: контроль переобучения

**Предсказание:**

**Регрессия:**
```
ŷ(x) = (1/T) Σ tree_t(x)
```

**Классификация (soft voting):**
```
P(y = c | x) = (1/T) Σ I{tree_t(x) = c}
```

### 4) Middle-level нюансы

**Проблема 1: Зачем Random Feature Selection?**

**Без random features (обычный bagging):**  
Деревья **коррелированы**: все используют один и тот же сильный признак на первом split.

**Пример:**
```
Данные: прогноз цены дома
Признаки: площадь, район, год постройки, ...
Все деревья сделают первый split по "площадь" (самый важный)
→ деревья похожи → усреднение менее эффективно
```

**С random features:**  
Деревья **разнообразны** (decorrelated) → усреднение снижает variance лучше!

**Математика:**

Если деревья имеют корреляцию ρ:
```
Var(average) = ρ · σ² + (1 - ρ) · σ² / T
```

Random features → ρ ↓ → variance ↓↓

**Проблема 2: Feature Importance**

**Как измеряется:**

**Impurity-based (Gini importance):**
```
Importance(X_j) = Σ Σ (decrease in impurity at split on X_j)
                  trees  splits
```

Нормализуется на [0, 1], сумма = 1.

**Permutation Importance (более надежный):**
1. Измерить OOB error
2. Перемешать (permute) признак X_j
3. Измерить новый OOB error
4. Importance = увеличение ошибки

**Проблемы Gini importance:**
- Смещен к **high-cardinality** признакам (много уникальных значений)
- Смещен к **непрерывным** признакам

**Решение:**  
Используйте **Permutation Importance** для critical decisions.

**Проблема 3: Число признаков m**

**m слишком мал:**  
Деревья слабые (недостаточно информации) → high bias.

**m слишком велик:**  
Деревья коррелированы → bagging менее эффективен.

**Эмпирические правила:**
- **Классификация**: m = √p
- **Регрессия**: m = p/3

Подбор через CV!

**Проблема 4: Random Forest не экстраполирует**

Деревья предсказывают только значения из обучающей выборки (листья = константы).

**Пример:**
```
Обучающие данные: годы 2000-2020, цены $100k-$500k
Test: год 2025, реальная цена $600k
Random Forest предскажет max ≈ $500k (не выйдет за пределы)
```

**Решение:**  
Для экстраполяции используйте **linear models** или **gradient boosting** с осторожностью.

### 5) Сравнение

| Метод | Bias | Variance | Интерпретируемость | Скорость | Feature Importance |
|-------|------|----------|-------------------|----------|-------------------|
| **Single Decision Tree** | Low (если глубокая) | ❌ High | ✅ Высокая | ✅ Быстро | ✅ Да |
| **Random Forest** | Low | ✅ Low (bagging) | ⚠️ Средняя | ⚠️ Средне | ✅ Да (агрегированная) |
| **XGBoost** | ✅ Lower (boosting) | Low (с regularization) | ⚠️ Средняя | ⚠️ Медленнее | ✅ Да |

---

## 5.3 Boosting

### 1) Интуиция и суть

**Простыми словами (ELI5):**  
Ты учишься на ошибках:
1. Сдал первый экзамен → получил ошибки
2. Сосредоточился на **ошибках** (не знал тему X) → подучил тему X
3. Пересдал → получил новые ошибки → подучил еще
4. **Итоговый результат** = знания после всех пересдач

Boosting так же: **последовательно обучает слабые модели**, каждая фокусируется на ошибках предыдущих.

**Академически:**  
Boosting — это **sequential ensemble**, где каждая новая модель обучается исправлять **остаточные ошибки** (residuals) предыдущих моделей. Основная цель — **снижение bias**.

**Физический смысл:**  
**Градиентный спуск в пространстве функций**: каждая новая модель — это шаг градиентного спуска к минимуму функции потерь.

### 2) Зачем и когда (Business Value)

**Какую проблему решает:**
- **Недообучение** слабых моделей (shallow trees)
- **Высокая точность**: обычно лучше Random Forest
- **Гибкость**: разные loss functions для разных задач

**Где это индустриальный стандарт:**
- **Kaggle**: XGBoost, LightGBM, CatBoost — доминируют в табличных данных!
- **Финансы**: кредитный скоринг, fraud detection
- **Рекомендательные системы**: ranking
- **AdTech**: CTR prediction

**Когда применять нельзя:**
- ❌ Малые данные + много шума (переобучится, в отличие от RF)
- ❌ Данные с выбросами (чувствителен, нужна регуляризация)

### 3) Математическое ядро

**Gradient Boosting (общий framework):**

**Инициализация:**
```
F_0(x) = argmin Σ L(y_i, γ)
          γ
```
Обычно F_0 = константа (среднее для MSE).

**Для m = 1 до M:**

1. **Вычислить псевдо-остатки (negative gradient):**
   ```
   r_mi = -∂L(y_i, F(x_i)) / ∂F(x_i) |_{F = F_{m-1}}
   ```

2. **Обучить слабую модель h_m на {(x_i, r_mi)}:**
   ```
   h_m = argmin Σ (r_mi - h(x_i))²
         h
   ```

3. **Найти оптимальный шаг γ_m:**
   ```
   γ_m = argmin Σ L(y_i, F_{m-1}(x_i) + γ · h_m(x_i))
         γ
   ```

4. **Обновить модель:**
   ```
   F_m(x) = F_{m-1}(x) + η · γ_m · h_m(x)
   ```
   η — learning rate (shrinkage).

**Финальная модель:**
```
F(x) = F_0(x) + Σ η · γ_m · h_m(x)
```

**Примеры для разных loss:**

**Регрессия (MSE):**
```
L(y, F) = (y - F)² / 2
r_i = y_i - F_{m-1}(x_i)  (просто residuals!)
```

**Классификация (LogLoss):**
```
L(y, F) = -[y log(p) + (1-y) log(1-p)], где p = σ(F)
r_i = y_i - σ(F_{m-1}(x_i))  (разница между истинной и предсказанной вероятностью)
```

### 4) Middle-level нюансы

**Проблема 1: Почему снижается bias?**

**Boosting фокусируется на сложных примерах:**  
Каждая модель h_m учится предсказывать остатки → исправляет ошибки.

**Результат:**
- **Bias** сильно снижается (модель становится мощнее)
- **Variance** может расти (риск переобучения!)

**Отличие от Bagging:**
- Bagging: variance ↓, bias ≈
- Boosting: bias ↓↓, variance ↑ (контролируем через регуляризацию)

**Проблема 2: Learning Rate (η, shrinkage)**

**η велик (η = 1):**  
Быстрая сходимость, но риск переобучения.

**η мал (η = 0.01-0.1):**  
Медленная сходимость, но лучше обобщение.

**Trade-off:**
```
Меньше η → нужно больше деревьев M
```

**Эмпирическое правило:**
- η = 0.1, M = 100-500 (стандарт)
- η = 0.01, M = 1000-5000 (лучше, но медленнее)

**Проблема 3: Regularization**

**Проблема:**  
Boosting sequentially → легко переобучается на шумных данных.

**Решения:**

1. **Shrinkage (η < 1)**: уменьшает вклад каждого дерева
2. **Tree constraints**:
   - max_depth = 3-6 (shallow trees — weak learners!)
   - min_samples_leaf = 5-20
3. **Subsampling (stochastic GB):**
   - Обучать каждое дерево на случайной части данных (как в Random Forest)
4. **Early Stopping:**
   - Мониторить validation error, остановить при ухудшении

**Проблема 4: XGBoost специфика**

**XGBoost** (eXtreme Gradient Boosting) — оптимизированная реализация GB:

**Улучшения:**
- **Regularization в loss:**
  ```
  Obj = Σ L(y_i, ŷ_i) + Σ Ω(tree_m)
  ```
  где Ω(tree) = γ·T + (λ/2)||w||² (T — число листьев, w — веса листьев)
  
- **Second-order approximation** (использует вторую производную — Hessian)
- **Параллелизация** построения деревьев
- **Handling missing values** автоматически

**Гиперпараметры XGBoost:**
- `n_estimators` (M): число деревьев
- `learning_rate` (η): 0.01-0.3
- `max_depth`: 3-10
- `subsample`: 0.5-1.0 (доля данных для каждого дерева)
- `colsample_bytree`: 0.5-1.0 (доля признаков)
- `lambda` (L2), `alpha` (L1): регуляризация весов

### 5) Сравнение Bagging vs Boosting

| Аспект | Bagging (Random Forest) | Boosting (XGBoost) |
|--------|------------------------|-------------------|
| **Обучение** | Параллельно | ❌ Последовательно |
| **Цель** | ↓ Variance | ↓ Bias |
| **Базовые модели** | Deep trees (high variance) | Shallow trees (low variance, high bias) |
| **Переобучение** | ✅ Устойчив | ⚠️ Может переобучиться (нужна регуляризация) |
| **Скорость** | ✅ Быстро (параллельность) | ⚠️ Медленнее |
| **Точность** | ✅ Хорошая | ✅✅ Обычно выше |
| **Интерпретация** | ⚠️ Сложно | ⚠️ Сложно |

---

## 5.4 Stacking (Stacked Generalization)

### 1) Интуиция и суть

**Простыми словами (ELI5):**  
У тебя есть 3 друга-эксперта: математик, физик, химик. Каждый решает задачу по-своему. Вместо простого голосования, ты даешь их ответы **четвертому эксперту-арбитру**, который учился **комбинировать** их мнения оптимально.

**Академически:**  
Stacking — это **meta-learning**: обучаем несколько базовых моделей (level-0), затем обучаем **мета-модель** (level-1), которая учится оптимально комбинировать предсказания базовых моделей.

### 2) Зачем и когда (Business Value)

**Какую проблему решает:**
- **Комбинация разнообразных моделей** (логистическая регрессия + Random Forest + SVM)
- **Улучшение** прогноза за счет оптимальной комбинации

**Где используется:**
- **Kaggle**: winning solutions часто используют stacking
- **Критичные задачи**: медицина, финансы (выжимаем максимум точности)

**Когда применять нельзя:**
- ❌ Малые данные (риск переобучения мета-модели)
- ❌ Продакшн с ограничениями на latency (много моделей)

### 3) Математическое ядро

**Алгоритм:**

1. **Обучить базовые модели f_1, ..., f_K:**
   - Разные алгоритмы (LogReg, RF, XGBoost, ...)
   - С cross-validation для избежания leakage:
     ```
     Для каждого fold:
       Обучить f_k на train fold
       Предсказать на validation fold → meta-feature
     ```

2. **Создать meta-features:**
   ```
   X_meta = [f_1(x), f_2(x), ..., f_K(x)]
   ```

3. **Обучить мета-модель g на (X_meta, y):**
   ```
   g: ℝ^K → ℝ  (или {0, 1} для классификации)
   ```
   Обычно простая модель: LogReg, Ridge.

4. **Предсказание:**
   ```
   ŷ = g(f_1(x), ..., f_K(x))
   ```

### 4) Middle-level нюансы

**Проблема 1: Data Leakage**

**Неправильно:**
```python
# Обучить базовые модели на всех train
f1.fit(X_train, y_train)
# Предсказать на train → meta-features
meta_X_train = f1.predict(X_train)  # ← leakage!
```

Мета-модель видит "идеальные" предсказания базовых моделей на train → переобучение!

**Правильно (cross-validation):**
```python
from sklearn.model_selection import cross_val_predict
meta_X_train = cross_val_predict(f1, X_train, y_train, cv=5)
```

**Проблема 2: Выбор мета-модели**

**Простая модель** (Linear Regression, Logistic Regression):
- ✅ Меньше риск переобучения
- ✅ Интерпретация: веса = важность базовых моделей

**Сложная модель** (XGBoost):
- ⚠️ Риск переобучения (мета-фичей мало — K штук!)
- Может найти нелинейные комбинации

**Рекомендация:**  
Начните с Ridge/LogReg. Если мало моделей (K < 5), оставайтесь простыми!

### 5) Сравнение ансамблей

| Метод | Базовые модели | Комбинация | Цель | Сложность |
|-------|---------------|-----------|------|-----------|
| **Bagging** | Одинаковые (деревья) | Усреднение | ↓ variance | ✅ Простой |
| **Boosting** | Одинаковые (деревья) | Взвешенная сумма (последовательно) | ↓ bias | ⚠️ Средний |
| **Stacking** | Разные | Обученная мета-модель | Оптимальная комбинация | ⚠️ Сложный |

---

## Резюме модуля

Ансамбли — это **"мудрость толпы"** в ML:
- **Bagging**: снижает variance через усреднение независимых моделей, OOB validation
- **Random Forest**: bagging + random features → decorrelation, feature importance
- **Boosting**: снижает bias через последовательную коррекцию ошибок, gradient descent в пространстве функций
- **Stacking**: мета-модель для оптимальной комбинации разнообразных моделей

Random Forest и XGBoost — рабочие лошадки для табличных данных!
