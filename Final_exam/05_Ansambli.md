# –ú–æ–¥—É–ª—å 5: –ê–Ω—Å–∞–º–±–ª–∏ (Ensemble Methods)

## 5.1 Bagging (Bootstrap Aggregating)

### 1) –ò–Ω—Ç—É–∏—Ü–∏—è –∏ —Å—É—Ç—å

**–ü—Ä–æ—Å—Ç—ã–º–∏ —Å–ª–æ–≤–∞–º–∏ (ELI5):**  
–£ —Ç–µ–±—è –µ—Å—Ç—å —Å–ª–æ–∂–Ω–∞—è –∑–∞–¥–∞—á–∞ –∏ 100 –¥—Ä—É–∑–µ–π. –í–º–µ—Å—Ç–æ —Ç–æ–≥–æ —á—Ç–æ–±—ã –æ–¥–∏–Ω –¥—Ä—É–≥ —Ä–µ—à–∞–ª –∑–∞–¥–∞—á—É (–º–æ–∂–µ—Ç –æ—à–∏–±–∏—Ç—å—Å—è!), —Ç—ã –¥–∞–µ—à—å –∑–∞–¥–∞—á—É **–∫–∞–∂–¥–æ–º—É**, –Ω–æ –¥–∞–µ—à—å –∏–º —Å–ª–µ–≥–∫–∞ **—Ä–∞–∑–Ω—ã–µ –≤–µ—Ä—Å–∏–∏ —É—Å–ª–æ–≤–∏—è** (bootstrap samples). –ü–æ—Ç–æ–º **—É—Å—Ä–µ–¥–Ω—è–µ—à—å** –∏—Ö –æ—Ç–≤–µ—Ç—ã. –ö–æ–ª–ª–µ–∫—Ç–∏–≤–Ω—ã–π —Ä–∞–∑—É–º –æ–±—ã—á–Ω–æ —Ç–æ—á–Ω–µ–µ!

**–ê–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏:**  
Bagging ‚Äî —ç—Ç–æ **ensemble –º–µ—Ç–æ–¥**, –∫–æ—Ç–æ—Ä—ã–π –æ–±—É—á–∞–µ—Ç –º–Ω–æ–∂–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–µ–π –Ω–∞ **bootstrap samples** (—Å–ª—É—á–∞–π–Ω—ã–µ –≤—ã–±–æ—Ä–∫–∏ —Å –≤–æ–∑–≤—Ä–∞—â–µ–Ω–∏–µ–º –∏–∑ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö) –∏ **—É—Å—Ä–µ–¥–Ω—è–µ—Ç** –∏—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è. –û—Å–Ω–æ–≤–Ω–∞—è —Ü–µ–ª—å ‚Äî **—Å–Ω–∏–∂–µ–Ω–∏–µ variance**.

**–§–∏–∑–∏—á–µ—Å–∫–∏–π —Å–º—ã—Å–ª:**  
–£—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã—Ö –º–æ–¥–µ–ª–µ–π —Å–Ω–∏–∂–∞–µ—Ç –¥–∏—Å–ø–µ—Ä—Å–∏—é: Var(average) = Var(model) / n_models.

### 2) –ó–∞—á–µ–º –∏ –∫–æ–≥–¥–∞ (Business Value)

**–ö–∞–∫—É—é –ø—Ä–æ–±–ª–µ–º—É —Ä–µ—à–∞–µ—Ç:**
- **–ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ** (overfitting): —Å–Ω–∏–∂–∞–µ—Ç variance –≤—ã—Å–æ–∫–æ–¥–∏—Å–ø–µ—Ä—Å–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
- **–ù–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å** –º–æ–¥–µ–ª–µ–π (–Ω–∞–ø—Ä–∏–º–µ—Ä, decision trees –º–µ–Ω—è—é—Ç—Å—è –ø—Ä–∏ –º–∞–ª–æ–º –∏–∑–º–µ–Ω–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö)
- **–£–ª—É—á—à–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏** –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏—è –±–∞–∑–æ–≤–æ–≥–æ –∞–ª–≥–æ—Ä–∏—Ç–º–∞

**–ì–¥–µ —ç—Ç–æ –∏–Ω–¥—É—Å—Ç—Ä–∏–∞–ª—å–Ω—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç:**
- **Random Forest** (bagging + random feature selection) ‚Äî –≤–µ–∑–¥–µ!
- **Healthcare**: –∞–Ω—Å–∞–º–±–ª–∏ –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
- **–§–∏–Ω–∞–Ω—Å—ã**: –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä—ã–Ω–∫–æ–≤
- **Kaggle**: bagging ‚Äî –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç

**–ö–æ–≥–¥–∞ –ø—Ä–∏–º–µ–Ω—è—Ç—å –Ω–µ–ª—å–∑—è:**
- ‚ùå –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å –∏–º–µ–µ—Ç low variance (–Ω–∞–ø—Ä–∏–º–µ—Ä, linear regression) ‚Äî bagging –Ω–µ –ø–æ–º–æ–∂–µ—Ç
- ‚ùå –í—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã –æ–≥—Ä–∞–Ω–∏—á–µ–Ω—ã (–Ω—É–∂–Ω–æ –æ–±—É—á–∞—Ç—å –º–Ω–æ–≥–æ –º–æ–¥–µ–ª–µ–π)

### 3) –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —è–¥—Ä–æ

**–ê–ª–≥–æ—Ä–∏—Ç–º Bagging:**

1. –î–ª—è b = 1 –¥–æ B (—á–∏—Å–ª–æ –º–æ–¥–µ–ª–µ–π):
   - **Bootstrap sample**: 
     ```
     D_b = sample n –æ–±—ä–µ–∫—Ç–æ–≤ –∏–∑ D —Å –≤–æ–∑–≤—Ä–∞—â–µ–Ω–∏–µ–º
     ```
   - –û–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å f_b –Ω–∞ D_b

2. **–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ (—Ä–µ–≥—Ä–µ—Å—Å–∏—è):**
   ```
   ≈∑ = (1/B) Œ£ f_b(x)
   ```

3. **–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ (–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è):**
   ```
   ≈∑ = mode{f_b(x)}  (–≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏–µ –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–∞)
   ```

**Bootstrap Sample:**

–°–ª—É—á–∞–π–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞ —Å –≤–æ–∑–≤—Ä–∞—â–µ–Ω–∏–µ–º:
- –†–∞–∑–º–µ—Ä = n (–∫–∞–∫ –∏—Å—Ö–æ–¥–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç)
- –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å, —á—Ç–æ –æ–±—ä–µ–∫—Ç **–Ω–µ –ø–æ–ø–∞–¥–µ—Ç** –≤ bootstrap sample:
  ```
  P(–Ω–µ –≤—ã–±—Ä–∞–Ω) = (1 - 1/n)^n ‚Üí e^(-1) ‚âà 0.368  (–ø—Ä–∏ n ‚Üí ‚àû)
  ```
- **Out-of-Bag (OOB)**: ‚âà 37% –æ–±—ä–µ–∫—Ç–æ–≤ –Ω–µ –≤ sample ‚Üí –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏!

**–°–Ω–∏–∂–µ–Ω–∏–µ variance (–º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏):**

–ü—É—Å—Ç—å f_b –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã –∏ Var(f_b) = œÉ¬≤.
```
Var(average) = Var((1/B) Œ£ f_b) = (1/B¬≤) ¬∑ B ¬∑ œÉ¬≤ = œÉ¬≤ / B
```

–ü—Ä–∏ B ‚Üí ‚àû, variance ‚Üí 0!

**–ù–∞ –ø—Ä–∞–∫—Ç–∏–∫–µ:**  
–ú–æ–¥–µ–ª–∏ –Ω–µ –ø–æ–ª–Ω–æ—Å—Ç—å—é –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã (–æ–±—É—á–∞—é—Ç—Å—è –Ω–∞ –ø–æ—Ö–æ–∂–∏—Ö –¥–∞–Ω–Ω—ã—Ö), –Ω–æ –≤—Å–µ —Ä–∞–≤–Ω–æ variance —Å–Ω–∏–∂–∞–µ—Ç—Å—è –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ.

### 4) Middle-level –Ω—é–∞–Ω—Å—ã

**–ü—Ä–æ–±–ª–µ–º–∞ 1: –ü–æ—á–µ–º—É —Å–Ω–∏–∂–∞–µ—Ç—Å—è variance?**

**Bias-Variance –¥–ª—è Bagging:**
- **Bias**: –ø—Ä–∏–º–µ—Ä–Ω–æ —Ç–∞–∫–æ–π –∂–µ, –∫–∞–∫ —É –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏ (—É—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ –Ω–µ —Å–Ω–∏–∂–∞–µ—Ç bias)
- **Variance**: —Å–∏–ª—å–Ω–æ —Å–Ω–∏–∂–∞–µ—Ç—Å—è (—É—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ!)

**–°–ª–µ–¥—Å—Ç–≤–∏–µ:**  
Bagging —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–µ–Ω –¥–ª—è **low-bias, high-variance –º–æ–¥–µ–ª–µ–π**:
- ‚úÖ **Decision Trees** (–≥–ª—É–±–æ–∫–∏–µ) ‚Äî –∏–¥–µ–∞–ª—å–Ω–æ!
- ‚úÖ Neural Networks (—Å–ª–æ–∂–Ω—ã–µ)
- ‚ùå Linear Regression (—É–∂–µ low variance) ‚Äî –±–µ—Å–ø–æ–ª–µ–∑–Ω–æ

**–ü—Ä–æ–±–ª–µ–º–∞ 2: Out-of-Bag (OOB) Error**

**–ò–¥–µ—è:**  
‚âà 37% –æ–±—ä–µ–∫—Ç–æ–≤ –Ω–µ –ø–æ–ø–∞–¥–∞—é—Ç –≤ –∫–∞–∂–¥—ã–π bootstrap sample.  
–ò—Å–ø–æ–ª—å–∑—É–µ–º –∏—Ö –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–∏ **–±–µ–∑ –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ validation set**!

**OOB Prediction –¥–ª—è –æ–±—ä–µ–∫—Ç–∞ x_i:**
```
≈∑_OOB(x_i) = average{f_b(x_i) : x_i ‚àâ D_b}
```

**OOB Error:**
```
OOB_Error = (1/n) Œ£ L(y_i, ≈∑_OOB(x_i))
```

**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ:**  
"–ë–µ—Å–ø–ª–∞—Ç–Ω–∞—è" –≤–∞–ª–∏–¥–∞—Ü–∏—è (–Ω–µ –Ω—É–∂–µ–Ω –æ—Ç–¥–µ–ª—å–Ω—ã–π validation set), –ø–æ—á—Ç–∏ —ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç–Ω–∞ cross-validation!

**–ü—Ä–æ–±–ª–µ–º–∞ 3: –ß–∏—Å–ª–æ –º–æ–¥–µ–ª–µ–π B**

**–¢–µ–æ—Ä–∏—è:**  
–ß–µ–º –±–æ–ª—å—à–µ B, —Ç–µ–º –º–µ–Ω—å—à–µ variance. –ù–æ **—É–±—ã–≤–∞—é—â–∞—è –æ—Ç–¥–∞—á–∞** (diminishing returns).

**–ü—Ä–∞–∫—Ç–∏–∫–∞:**
- B = 50-100: —É–∂–µ —Ö–æ—Ä–æ—à–æ  
- B = 500-1000: —Å—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è Random Forest  
- B > 1000: —Ä–µ–¥–∫–æ –Ω—É–∂–Ω–æ (–º–∞–ª–æ –ø—Ä–∏—Ä–æ—Å—Ç–∞)

**–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥:**  
–ü–æ—Å—Ç—Ä–æ–π—Ç–µ –≥—Ä–∞—Ñ–∏–∫ OOB Error vs B ‚Üí –Ω–∞–π–¥–∏—Ç–µ "–∫–æ–ª–µ–Ω–æ" (plateau).

**–ü—Ä–æ–±–ª–µ–º–∞ 4: –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å**

Bagging –∞–Ω—Å–∞–º–±–ª—å –∏–∑ 100 –¥–µ—Ä–µ–≤—å–µ–≤ ‚Üí —Å–ª–æ–∂–Ω–æ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä–æ–≤–∞—Ç—å.

**–†–µ—à–µ–Ω–∏–µ:**
- **Feature Importance**: —É—Å—Ä–µ–¥–Ω–∏—Ç—å importance –ø–æ –≤—Å–µ–º –º–æ–¥–µ–ª—è–º
- **Partial Dependence Plots**: –≤–ª–∏—è–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∞ –Ω–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ

### 5) –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞–º–∏

| –ú–µ—Ç–æ–¥ | –¶–µ–ª—å | –ö–∞–∫ —Å–Ω–∏–∂–∞–µ—Ç –æ—à–∏–±–∫—É | –ë–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ | –ü–∞—Ä–∞–ª–ª–µ–ª–∏–∑–∞—Ü–∏—è |
|-------|------|-------------------|---------------|----------------|
| **Bagging** | –°–Ω–∏–∂–µ–Ω–∏–µ variance | –£—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã—Ö –º–æ–¥–µ–ª–µ–π | High variance (deep trees) | ‚úÖ –õ–µ–≥–∫–æ (–º–æ–¥–µ–ª–∏ –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã) |
| **Boosting** | –°–Ω–∏–∂–µ–Ω–∏–µ bias | –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–∞—è –∫–æ—Ä—Ä–µ–∫—Ü–∏—è –æ—à–∏–±–æ–∫ | Low variance (shallow trees) | ‚ùå –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ |
| **Stacking** | –ö–æ–º–±–∏–Ω–∞—Ü–∏—è | –ú–µ—Ç–∞-–º–æ–¥–µ–ª—å —É—á–∏—Ç—Å—è –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞—Ç—å | –†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ | ‚ö†Ô∏è –ß–∞—Å—Ç–∏—á–Ω–æ |

---

## 5.2 Random Forest

### 1) –ò–Ω—Ç—É–∏—Ü–∏—è –∏ —Å—É—Ç—å

**–ü—Ä–æ—Å—Ç—ã–º–∏ —Å–ª–æ–≤–∞–º–∏ (ELI5):**  
–ü—Ä–µ–¥—Å—Ç–∞–≤—å, —á—Ç–æ —É —Ç–µ–±—è –µ—Å—Ç—å –∫–æ–º–∞–Ω–¥–∞ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ (–¥–µ—Ä–µ–≤—å–µ–≤), –∏ –∫–∞–∂–¥—ã–π —ç–∫—Å–ø–µ—Ä—Ç:
1. –°–º–æ—Ç—Ä–∏—Ç –Ω–∞ **—Å–ª—É—á–∞–π–Ω—É—é —á–∞—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö** (bootstrap)
2. –†–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç —Ç–æ–ª—å–∫–æ **—Å–ª—É—á–∞–π–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏** –ø—Ä–∏ –∫–∞–∂–¥–æ–º —Ä–µ—à–µ–Ω–∏–∏

–ü–æ—Ç–æ–º –≤—Å–µ –≥–æ–ª–æ—Å—É—é—Ç ‚Üí **–∫–æ–ª–ª–µ–∫—Ç–∏–≤–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ**. –≠—Ç–æ Random Forest!

**–ê–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏:**  
Random Forest = **Bagging** + **Random Feature Selection**. –û–±—É—á–∞–µ—Ç—Å—è –∞–Ω—Å–∞–º–±–ª—å —Ä–µ—à–∞—é—â–∏—Ö –¥–µ—Ä–µ–≤—å–µ–≤, –≥–¥–µ –∫–∞–∂–¥–æ–µ –¥–µ—Ä–µ–≤–æ:
- –û–±—É—á–∞–µ—Ç—Å—è –Ω–∞ bootstrap sample
- –ü—Ä–∏ –∫–∞–∂–¥–æ–º split —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç —Ç–æ–ª—å–∫–æ —Å–ª—É—á–∞–π–Ω–æ–µ –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤

**–§–∏–∑–∏—á–µ—Å–∫–∏–π —Å–º—ã—Å–ª:**  
–°–ª—É—á–∞–π–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–µ–ª–∞—é—Ç –¥–µ—Ä–µ–≤—å—è **–º–µ–Ω–µ–µ –∫–æ—Ä—Ä–µ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏** ‚Üí —É—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ —Å–Ω–∏–∂–∞–µ—Ç variance.

### 2) –ó–∞—á–µ–º –∏ –∫–æ–≥–¥–∞ (Business Value)

**–ö–∞–∫—É—é –ø—Ä–æ–±–ª–µ–º—É —Ä–µ—à–∞–µ—Ç:**
- **–ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ** decision trees
- **–í—ã—Å–æ–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å** "–∏–∑ –∫–æ—Ä–æ–±–∫–∏" (–º–∞–ª–æ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤)
- **Feature Importance** –¥–ª—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏
- **–†–∞–±–æ—Ç–∞–µ—Ç** —Å –º–∞–ª—ã–º–∏ –∏ –±–æ–ª—å—à–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏

**–ì–¥–µ —ç—Ç–æ –∏–Ω–¥—É—Å—Ç—Ä–∏–∞–ª—å–Ω—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç (—Ä–∞–±–æ—á–∞—è –ª–æ—à–∞–¥–∫–∞!):**
- **–í—Å—ë!** –¢–∞–±–ª–∏—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ: Healthcare, Finance, E-commerce, Insurance
- **Kaggle**: top baseline –º–æ–¥–µ–ª—å
- **–ë–∞–Ω–∫–∏**: –∫—Ä–µ–¥–∏—Ç–Ω—ã–π —Å–∫–æ—Ä–∏–Ω–≥
- **–ú–µ–¥–∏—Ü–∏–Ω–∞**: –ø—Ä–æ–≥–Ω–æ–∑ –∑–∞–±–æ–ª–µ–≤–∞–Ω–∏–π

**–ö–æ–≥–¥–∞ –ø—Ä–∏–º–µ–Ω—è—Ç—å –Ω–µ–ª—å–∑—è:**
- ‚ùå –≠–∫—Å—Ç—Ä–∞–ø–æ–ª—è—Ü–∏—è (–¥–µ—Ä–µ–≤—å—è –Ω–µ –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—é—Ç –∑–∞ –ø—Ä–µ–¥–µ–ª–∞–º–∏ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö)
- ‚ùå –û—á–µ–Ω—å –≤—ã—Å–æ–∫–æ—Ä–∞–∑–º–µ—Ä–Ω—ã–µ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (text, images) ‚Äî –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ linear models –∏–ª–∏ deep learning

### 3) –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —è–¥—Ä–æ

**–ê–ª–≥–æ—Ä–∏—Ç–º Random Forest:**

–î–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–µ—Ä–µ–≤–∞ t = 1 –¥–æ T:
1. **Bootstrap sample**: D_t ‚äÇ D (—Å –≤–æ–∑–≤—Ä–∞—â–µ–Ω–∏–µ–º)
2. –ü–æ—Å—Ç—Ä–æ–∏—Ç—å –¥–µ—Ä–µ–≤–æ:
   - **–ü—Ä–∏ –∫–∞–∂–¥–æ–º split**:
     - –í—ã–±—Ä–∞—Ç—å **m —Å–ª—É—á–∞–π–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤** –∏–∑ p
     - –ù–∞–π—Ç–∏ –ª—É—á—à–∏–π split —Å—Ä–µ–¥–∏ —ç—Ç–∏—Ö m
   - –†–æ—Å—Ç –¥–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –≥–ª—É–±–∏–Ω—ã –∏–ª–∏ min_samples_leaf

**–ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã:**
- **m (max_features)**: —á–∏—Å–ª–æ —Å–ª—É—á–∞–π–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
  - –†–µ–≥—Ä–µ—Å—Å–∏—è: m = p/3
  - –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è: m = ‚àöp
- **T (n_estimators)**: —á–∏—Å–ª–æ –¥–µ—Ä–µ–≤—å–µ–≤ (100-500)
- **max_depth**: –≥–ª—É–±–∏–Ω–∞ –¥–µ—Ä–µ–≤—å–µ–≤ (–æ–±—ã—á–Ω–æ None ‚Äî —Ä–∞—Å—Ç–∏–º –¥–æ –∫–æ–Ω—Ü–∞)
- **min_samples_split, min_samples_leaf**: –∫–æ–Ω—Ç—Ä–æ–ª—å –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è

**–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ:**

**–†–µ–≥—Ä–µ—Å—Å–∏—è:**
```
≈∑(x) = (1/T) Œ£ tree_t(x)
```

**–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è (soft voting):**
```
P(y = c | x) = (1/T) Œ£ I{tree_t(x) = c}
```

### 4) Middle-level –Ω—é–∞–Ω—Å—ã

**–ü—Ä–æ–±–ª–µ–º–∞ 1: –ó–∞—á–µ–º Random Feature Selection?**

**–ë–µ–∑ random features (–æ–±—ã—á–Ω—ã–π bagging):**  
–î–µ—Ä–µ–≤—å—è **–∫–æ—Ä—Ä–µ–ª–∏—Ä–æ–≤–∞–Ω—ã**: –≤—Å–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç –æ–¥–∏–Ω –∏ —Ç–æ—Ç –∂–µ —Å–∏–ª—å–Ω—ã–π –ø—Ä–∏–∑–Ω–∞–∫ –Ω–∞ –ø–µ—Ä–≤–æ–º split.

**–ü—Ä–∏–º–µ—Ä:**
```
–î–∞–Ω–Ω—ã–µ: –ø—Ä–æ–≥–Ω–æ–∑ —Ü–µ–Ω—ã –¥–æ–º–∞
–ü—Ä–∏–∑–Ω–∞–∫–∏: –ø–ª–æ—â–∞–¥—å, —Ä–∞–π–æ–Ω, –≥–æ–¥ –ø–æ—Å—Ç—Ä–æ–π–∫–∏, ...
–í—Å–µ –¥–µ—Ä–µ–≤—å—è —Å–¥–µ–ª–∞—é—Ç –ø–µ—Ä–≤—ã–π split –ø–æ "–ø–ª–æ—â–∞–¥—å" (—Å–∞–º—ã–π –≤–∞–∂–Ω—ã–π)
‚Üí –¥–µ—Ä–µ–≤—å—è –ø–æ—Ö–æ–∂–∏ ‚Üí —É—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ –º–µ–Ω–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ
```

**–° random features:**  
–î–µ—Ä–µ–≤—å—è **—Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã** (decorrelated) ‚Üí —É—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ —Å–Ω–∏–∂–∞–µ—Ç variance –ª—É—á—à–µ!

**–ú–∞—Ç–µ–º–∞—Ç–∏–∫–∞:**

–ï—Å–ª–∏ –¥–µ—Ä–µ–≤—å—è –∏–º–µ—é—Ç –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—é œÅ:
```
Var(average) = œÅ ¬∑ œÉ¬≤ + (1 - œÅ) ¬∑ œÉ¬≤ / T
```

Random features ‚Üí œÅ ‚Üì ‚Üí variance ‚Üì‚Üì

**–ü—Ä–æ–±–ª–µ–º–∞ 2: Feature Importance**

**–ö–∞–∫ –∏–∑–º–µ—Ä—è–µ—Ç—Å—è:**

**Impurity-based (Gini importance):**
```
Importance(X_j) = Œ£ Œ£ (decrease in impurity at split on X_j)
                  trees  splits
```

–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç—Å—è –Ω–∞ [0, 1], —Å—É–º–º–∞ = 1.

**Permutation Importance (–±–æ–ª–µ–µ –Ω–∞–¥–µ–∂–Ω—ã–π):**
1. –ò–∑–º–µ—Ä–∏—Ç—å OOB error
2. –ü–µ—Ä–µ–º–µ—à–∞—Ç—å (permute) –ø—Ä–∏–∑–Ω–∞–∫ X_j
3. –ò–∑–º–µ—Ä–∏—Ç—å –Ω–æ–≤—ã–π OOB error
4. Importance = —É–≤–µ–ª–∏—á–µ–Ω–∏–µ –æ—à–∏–±–∫–∏

**–ü—Ä–æ–±–ª–µ–º—ã Gini importance:**
- –°–º–µ—â–µ–Ω –∫ **high-cardinality** –ø—Ä–∏–∑–Ω–∞–∫–∞–º (–º–Ω–æ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π)
- –°–º–µ—â–µ–Ω –∫ **–Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–º** –ø—Ä–∏–∑–Ω–∞–∫–∞–º

**–†–µ—à–µ–Ω–∏–µ:**  
–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ **Permutation Importance** –¥–ª—è critical decisions.

**–ü—Ä–æ–±–ª–µ–º–∞ 3: –ß–∏—Å–ª–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ m**

**m —Å–ª–∏—à–∫–æ–º –º–∞–ª:**  
–î–µ—Ä–µ–≤—å—è —Å–ª–∞–±—ã–µ (–Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏) ‚Üí high bias.

**m —Å–ª–∏—à–∫–æ–º –≤–µ–ª–∏–∫:**  
–î–µ—Ä–µ–≤—å—è –∫–æ—Ä—Ä–µ–ª–∏—Ä–æ–≤–∞–Ω—ã ‚Üí bagging –º–µ–Ω–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–µ–Ω.

**–≠–º–ø–∏—Ä–∏—á–µ—Å–∫–∏–µ –ø—Ä–∞–≤–∏–ª–∞:**
- **–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è**: m = ‚àöp
- **–†–µ–≥—Ä–µ—Å—Å–∏—è**: m = p/3

–ü–æ–¥–±–æ—Ä —á–µ—Ä–µ–∑ CV!

**–ü—Ä–æ–±–ª–µ–º–∞ 4: Random Forest –Ω–µ —ç–∫—Å—Ç—Ä–∞–ø–æ–ª–∏—Ä—É–µ—Ç**

–î–µ—Ä–µ–≤—å—è –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—é—Ç —Ç–æ–ª—å–∫–æ –∑–Ω–∞—á–µ–Ω–∏—è –∏–∑ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏ (–ª–∏—Å—Ç—å—è = –∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã).

**–ü—Ä–∏–º–µ—Ä:**
```
–û–±—É—á–∞—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ: –≥–æ–¥—ã 2000-2020, —Ü–µ–Ω—ã $100k-$500k
Test: –≥–æ–¥ 2025, —Ä–µ–∞–ª—å–Ω–∞—è —Ü–µ–Ω–∞ $600k
Random Forest –ø—Ä–µ–¥—Å–∫–∞–∂–µ—Ç max ‚âà $500k (–Ω–µ –≤—ã–π–¥–µ—Ç –∑–∞ –ø—Ä–µ–¥–µ–ª—ã)
```

**–†–µ—à–µ–Ω–∏–µ:**  
–î–ª—è —ç–∫—Å—Ç—Ä–∞–ø–æ–ª—è—Ü–∏–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ **linear models** –∏–ª–∏ **gradient boosting** —Å –æ—Å—Ç–æ—Ä–æ–∂–Ω–æ—Å—Ç—å—é.

### 5) –°—Ä–∞–≤–Ω–µ–Ω–∏–µ

| –ú–µ—Ç–æ–¥ | Bias | Variance | –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å | –°–∫–æ—Ä–æ—Å—Ç—å | Feature Importance |
|-------|------|----------|-------------------|----------|-------------------|
| **Single Decision Tree** | Low (–µ—Å–ª–∏ –≥–ª—É–±–æ–∫–∞—è) | ‚ùå High | ‚úÖ –í—ã—Å–æ–∫–∞—è | ‚úÖ –ë—ã—Å—Ç—Ä–æ | ‚úÖ –î–∞ |
| **Random Forest** | Low | ‚úÖ Low (bagging) | ‚ö†Ô∏è –°—Ä–µ–¥–Ω—è—è | ‚ö†Ô∏è –°—Ä–µ–¥–Ω–µ | ‚úÖ –î–∞ (–∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω–∞—è) |
| **XGBoost** | ‚úÖ Lower (boosting) | Low (—Å regularization) | ‚ö†Ô∏è –°—Ä–µ–¥–Ω—è—è | ‚ö†Ô∏è –ú–µ–¥–ª–µ–Ω–Ω–µ–µ | ‚úÖ –î–∞ |

---

## 5.3 Boosting

### 1) –ò–Ω—Ç—É–∏—Ü–∏—è –∏ —Å—É—Ç—å

**–ü—Ä–æ—Å—Ç—ã–º–∏ —Å–ª–æ–≤–∞–º–∏ (ELI5):**  
–¢—ã —É—á–∏—à—å—Å—è –Ω–∞ –æ—à–∏–±–∫–∞—Ö:
1. –°–¥–∞–ª –ø–µ—Ä–≤—ã–π —ç–∫–∑–∞–º–µ–Ω ‚Üí –ø–æ–ª—É—á–∏–ª –æ—à–∏–±–∫–∏
2. –°–æ—Å—Ä–µ–¥–æ—Ç–æ—á–∏–ª—Å—è –Ω–∞ **–æ—à–∏–±–∫–∞—Ö** (–Ω–µ –∑–Ω–∞–ª —Ç–µ–º—É X) ‚Üí –ø–æ–¥—É—á–∏–ª —Ç–µ–º—É X
3. –ü–µ—Ä–µ—Å–¥–∞–ª ‚Üí –ø–æ–ª—É—á–∏–ª –Ω–æ–≤—ã–µ –æ—à–∏–±–∫–∏ ‚Üí –ø–æ–¥—É—á–∏–ª –µ—â–µ
4. **–ò—Ç–æ–≥–æ–≤—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç** = –∑–Ω–∞–Ω–∏—è –ø–æ—Å–ª–µ –≤—Å–µ—Ö –ø–µ—Ä–µ—Å–¥–∞—á

Boosting —Ç–∞–∫ –∂–µ: **–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ –æ–±—É—á–∞–µ—Ç —Å–ª–∞–±—ã–µ –º–æ–¥–µ–ª–∏**, –∫–∞–∂–¥–∞—è —Ñ–æ–∫—É—Å–∏—Ä—É–µ—Ç—Å—è –Ω–∞ –æ—à–∏–±–∫–∞—Ö –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö.

**–ê–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏:**  
Boosting ‚Äî —ç—Ç–æ **sequential ensemble**, –≥–¥–µ –∫–∞–∂–¥–∞—è –Ω–æ–≤–∞—è –º–æ–¥–µ–ª—å –æ–±—É—á–∞–µ—Ç—Å—è –∏—Å–ø—Ä–∞–≤–ª—è—Ç—å **–æ—Å—Ç–∞—Ç–æ—á–Ω—ã–µ –æ—à–∏–±–∫–∏** (residuals) –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –º–æ–¥–µ–ª–µ–π. –û—Å–Ω–æ–≤–Ω–∞—è —Ü–µ–ª—å ‚Äî **—Å–Ω–∏–∂–µ–Ω–∏–µ bias**.

**–§–∏–∑–∏—á–µ—Å–∫–∏–π —Å–º—ã—Å–ª:**  
**–ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫ –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ —Ñ—É–Ω–∫—Ü–∏–π**: –∫–∞–∂–¥–∞—è –Ω–æ–≤–∞—è –º–æ–¥–µ–ª—å ‚Äî —ç—Ç–æ —à–∞–≥ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ —Å–ø—É—Å–∫–∞ –∫ –º–∏–Ω–∏–º—É–º—É —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å.

### 2) –ó–∞—á–µ–º –∏ –∫–æ–≥–¥–∞ (Business Value)

**–ö–∞–∫—É—é –ø—Ä–æ–±–ª–µ–º—É —Ä–µ—à–∞–µ—Ç:**
- **–ù–µ–¥–æ–æ–±—É—á–µ–Ω–∏–µ** —Å–ª–∞–±—ã—Ö –º–æ–¥–µ–ª–µ–π (shallow trees)
- **–í—ã—Å–æ–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å**: –æ–±—ã—á–Ω–æ –ª—É—á—à–µ Random Forest
- **–ì–∏–±–∫–æ—Å—Ç—å**: —Ä–∞–∑–Ω—ã–µ loss functions –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –∑–∞–¥–∞—á

**–ì–¥–µ —ç—Ç–æ –∏–Ω–¥—É—Å—Ç—Ä–∏–∞–ª—å–Ω—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç:**
- **Kaggle**: XGBoost, LightGBM, CatBoost ‚Äî –¥–æ–º–∏–Ω–∏—Ä—É—é—Ç –≤ —Ç–∞–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö!
- **–§–∏–Ω–∞–Ω—Å—ã**: –∫—Ä–µ–¥–∏—Ç–Ω—ã–π —Å–∫–æ—Ä–∏–Ω–≥, fraud detection
- **–†–µ–∫–æ–º–µ–Ω–¥–∞—Ç–µ–ª—å–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã**: ranking
- **AdTech**: CTR prediction

**–ö–æ–≥–¥–∞ –ø—Ä–∏–º–µ–Ω—è—Ç—å –Ω–µ–ª—å–∑—è:**
- ‚ùå –ú–∞–ª—ã–µ –¥–∞–Ω–Ω—ã–µ + –º–Ω–æ–≥–æ —à—É–º–∞ (–ø–µ—Ä–µ–æ–±—É—á–∏—Ç—Å—è, –≤ –æ—Ç–ª–∏—á–∏–µ –æ—Ç RF)
- ‚ùå –î–∞–Ω–Ω—ã–µ —Å –≤—ã–±—Ä–æ—Å–∞–º–∏ (—á—É–≤—Å—Ç–≤–∏—Ç–µ–ª–µ–Ω, –Ω—É–∂–Ω–∞ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è)

### 3) –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —è–¥—Ä–æ

**Gradient Boosting (–æ–±—â–∏–π framework):**

**–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è:**
```
F_0(x) = argmin Œ£ L(y_i, Œ≥)
          Œ≥
```
–û–±—ã—á–Ω–æ F_0 = –∫–æ–Ω—Å—Ç–∞–Ω—Ç–∞ (—Å—Ä–µ–¥–Ω–µ–µ –¥–ª—è MSE).

**–î–ª—è m = 1 –¥–æ M:**

1. **–í—ã—á–∏—Å–ª–∏—Ç—å –ø—Å–µ–≤–¥–æ-–æ—Å—Ç–∞—Ç–∫–∏ (negative gradient):**
   ```
   r_mi = -‚àÇL(y_i, F(x_i)) / ‚àÇF(x_i) |_{F = F_{m-1}}
   ```

2. **–û–±—É—á–∏—Ç—å —Å–ª–∞–±—É—é –º–æ–¥–µ–ª—å h_m –Ω–∞ {(x_i, r_mi)}:**
   ```
   h_m = argmin Œ£ (r_mi - h(x_i))¬≤
         h
   ```

3. **–ù–∞–π—Ç–∏ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —à–∞–≥ Œ≥_m:**
   ```
   Œ≥_m = argmin Œ£ L(y_i, F_{m-1}(x_i) + Œ≥ ¬∑ h_m(x_i))
         Œ≥
   ```

4. **–û–±–Ω–æ–≤–∏—Ç—å –º–æ–¥–µ–ª—å:**
   ```
   F_m(x) = F_{m-1}(x) + Œ∑ ¬∑ Œ≥_m ¬∑ h_m(x)
   ```
   Œ∑ ‚Äî learning rate (shrinkage).

**–§–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å:**
```
F(x) = F_0(x) + Œ£ Œ∑ ¬∑ Œ≥_m ¬∑ h_m(x)
```

**–ü—Ä–∏–º–µ—Ä—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö loss:**

**–†–µ–≥—Ä–µ—Å—Å–∏—è (MSE):**
```
L(y, F) = (y - F)¬≤ / 2
r_i = y_i - F_{m-1}(x_i)  (–ø—Ä–æ—Å—Ç–æ residuals!)
```

**–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è (LogLoss):**
```
L(y, F) = -[y log(p) + (1-y) log(1-p)], –≥–¥–µ p = œÉ(F)
r_i = y_i - œÉ(F_{m-1}(x_i))  (—Ä–∞–∑–Ω–∏—Ü–∞ –º–µ–∂–¥—É –∏—Å—Ç–∏–Ω–Ω–æ–π –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–æ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é)
```

### 4) Middle-level –Ω—é–∞–Ω—Å—ã

**–ü—Ä–æ–±–ª–µ–º–∞ 1: –ü–æ—á–µ–º—É —Å–Ω–∏–∂–∞–µ—Ç—Å—è bias?**

**Boosting —Ñ–æ–∫—É—Å–∏—Ä—É–µ—Ç—Å—è –Ω–∞ —Å–ª–æ–∂–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–∞—Ö:**  
–ö–∞–∂–¥–∞—è –º–æ–¥–µ–ª—å h_m —É—á–∏—Ç—Å—è –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å –æ—Å—Ç–∞—Ç–∫–∏ ‚Üí –∏—Å–ø—Ä–∞–≤–ª—è–µ—Ç –æ—à–∏–±–∫–∏.

**–†–µ–∑—É–ª—å—Ç–∞—Ç:**
- **Bias** —Å–∏–ª—å–Ω–æ —Å–Ω–∏–∂–∞–µ—Ç—Å—è (–º–æ–¥–µ–ª—å —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è –º–æ—â–Ω–µ–µ)
- **Variance** –º–æ–∂–µ—Ç —Ä–∞—Å—Ç–∏ (—Ä–∏—Å–∫ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è!)

**–û—Ç–ª–∏—á–∏–µ –æ—Ç Bagging:**
- Bagging: variance ‚Üì, bias ‚âà
- Boosting: bias ‚Üì‚Üì, variance ‚Üë (–∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º —á–µ—Ä–µ–∑ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—é)

**–ü—Ä–æ–±–ª–µ–º–∞ 2: Learning Rate (Œ∑, shrinkage)**

**Œ∑ –≤–µ–ª–∏–∫ (Œ∑ = 1):**  
–ë—ã—Å—Ç—Ä–∞—è —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å, –Ω–æ —Ä–∏—Å–∫ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è.

**Œ∑ –º–∞–ª (Œ∑ = 0.01-0.1):**  
–ú–µ–¥–ª–µ–Ω–Ω–∞—è —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å, –Ω–æ –ª—É—á—à–µ –æ–±–æ–±—â–µ–Ω–∏–µ.

**Trade-off:**
```
–ú–µ–Ω—å—à–µ Œ∑ ‚Üí –Ω—É–∂–Ω–æ –±–æ–ª—å—à–µ –¥–µ—Ä–µ–≤—å–µ–≤ M
```

**–≠–º–ø–∏—Ä–∏—á–µ—Å–∫–æ–µ –ø—Ä–∞–≤–∏–ª–æ:**
- Œ∑ = 0.1, M = 100-500 (—Å—Ç–∞–Ω–¥–∞—Ä—Ç)
- Œ∑ = 0.01, M = 1000-5000 (–ª—É—á—à–µ, –Ω–æ –º–µ–¥–ª–µ–Ω–Ω–µ–µ)

**–ü—Ä–æ–±–ª–µ–º–∞ 3: Regularization**

**–ü—Ä–æ–±–ª–µ–º–∞:**  
Boosting sequentially ‚Üí –ª–µ–≥–∫–æ –ø–µ—Ä–µ–æ–±—É—á–∞–µ—Ç—Å—è –Ω–∞ —à—É–º–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.

**–†–µ—à–µ–Ω–∏—è:**

1. **Shrinkage (Œ∑ < 1)**: —É–º–µ–Ω—å—à–∞–µ—Ç –≤–∫–ª–∞–¥ –∫–∞–∂–¥–æ–≥–æ –¥–µ—Ä–µ–≤–∞
2. **Tree constraints**:
   - max_depth = 3-6 (shallow trees ‚Äî weak learners!)
   - min_samples_leaf = 5-20
3. **Subsampling (stochastic GB):**
   - –û–±—É—á–∞—Ç—å –∫–∞–∂–¥–æ–µ –¥–µ—Ä–µ–≤–æ –Ω–∞ —Å–ª—É—á–∞–π–Ω–æ–π —á–∞—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö (–∫–∞–∫ –≤ Random Forest)
4. **Early Stopping:**
   - –ú–æ–Ω–∏—Ç–æ—Ä–∏—Ç—å validation error, –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –ø—Ä–∏ —É—Ö—É–¥—à–µ–Ω–∏–∏

**–ü—Ä–æ–±–ª–µ–º–∞ 4: XGBoost —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞**

**XGBoost** (eXtreme Gradient Boosting) ‚Äî –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è GB:

**–£–ª—É—á—à–µ–Ω–∏—è:**
- **Regularization –≤ loss:**
  ```
  Obj = Œ£ L(y_i, ≈∑_i) + Œ£ Œ©(tree_m)
  ```
  –≥–¥–µ Œ©(tree) = Œ≥¬∑T + (Œª/2)||w||¬≤ (T ‚Äî —á–∏—Å–ª–æ –ª–∏—Å—Ç—å–µ–≤, w ‚Äî –≤–µ—Å–∞ –ª–∏—Å—Ç—å–µ–≤)
  
- **Second-order approximation** (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≤—Ç–æ—Ä—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—É—é ‚Äî Hessian)
- **–ü–∞—Ä–∞–ª–ª–µ–ª–∏–∑–∞—Ü–∏—è** –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –¥–µ—Ä–µ–≤—å–µ–≤
- **Handling missing values** –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏

**–ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã XGBoost:**
- `n_estimators` (M): —á–∏—Å–ª–æ –¥–µ—Ä–µ–≤—å–µ–≤
- `learning_rate` (Œ∑): 0.01-0.3
- `max_depth`: 3-10
- `subsample`: 0.5-1.0 (–¥–æ–ª—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–µ—Ä–µ–≤–∞)
- `colsample_bytree`: 0.5-1.0 (–¥–æ–ª—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤)
- `lambda` (L2), `alpha` (L1): —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤

### 5) –°—Ä–∞–≤–Ω–µ–Ω–∏–µ Bagging vs Boosting

| –ê—Å–ø–µ–∫—Ç | Bagging (Random Forest) | Boosting (XGBoost) |
|--------|------------------------|-------------------|
| **–û–±—É—á–µ–Ω–∏–µ** | –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ | ‚ùå –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ |
| **–¶–µ–ª—å** | ‚Üì Variance | ‚Üì Bias |
| **–ë–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏** | Deep trees (high variance) | Shallow trees (low variance, high bias) |
| **–ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ** | ‚úÖ –£—Å—Ç–æ–π—á–∏–≤ | ‚ö†Ô∏è –ú–æ–∂–µ—Ç –ø–µ—Ä–µ–æ–±—É—á–∏—Ç—å—Å—è (–Ω—É–∂–Ω–∞ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è) |
| **–°–∫–æ—Ä–æ—Å—Ç—å** | ‚úÖ –ë—ã—Å—Ç—Ä–æ (–ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ—Å—Ç—å) | ‚ö†Ô∏è –ú–µ–¥–ª–µ–Ω–Ω–µ–µ |
| **–¢–æ—á–Ω–æ—Å—Ç—å** | ‚úÖ –•–æ—Ä–æ—à–∞—è | ‚úÖ‚úÖ –û–±—ã—á–Ω–æ –≤—ã—à–µ |
| **–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è** | ‚ö†Ô∏è –°–ª–æ–∂–Ω–æ | ‚ö†Ô∏è –°–ª–æ–∂–Ω–æ |

---

## 5.4 Stacking (Stacked Generalization)

### 1) –ò–Ω—Ç—É–∏—Ü–∏—è –∏ —Å—É—Ç—å

**–ü—Ä–æ—Å—Ç—ã–º–∏ —Å–ª–æ–≤–∞–º–∏ (ELI5):**  
–£ —Ç–µ–±—è –µ—Å—Ç—å 3 –¥—Ä—É–≥–∞-—ç–∫—Å–ø–µ—Ä—Ç–∞: –º–∞—Ç–µ–º–∞—Ç–∏–∫, —Ñ–∏–∑–∏–∫, —Ö–∏–º–∏–∫. –ö–∞–∂–¥—ã–π —Ä–µ—à–∞–µ—Ç –∑–∞–¥–∞—á—É –ø–æ-—Å–≤–æ–µ–º—É. –í–º–µ—Å—Ç–æ –ø—Ä–æ—Å—Ç–æ–≥–æ –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏—è, —Ç—ã –¥–∞–µ—à—å –∏—Ö –æ—Ç–≤–µ—Ç—ã **—á–µ—Ç–≤–µ—Ä—Ç–æ–º—É —ç–∫—Å–ø–µ—Ä—Ç—É-–∞—Ä–±–∏—Ç—Ä—É**, –∫–æ—Ç–æ—Ä—ã–π —É—á–∏–ª—Å—è **–∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞—Ç—å** –∏—Ö –º–Ω–µ–Ω–∏—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ.

**–ê–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏:**  
Stacking ‚Äî —ç—Ç–æ **meta-learning**: –æ–±—É—á–∞–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (level-0), –∑–∞—Ç–µ–º –æ–±—É—á–∞–µ–º **–º–µ—Ç–∞-–º–æ–¥–µ–ª—å** (level-1), –∫–æ—Ç–æ—Ä–∞—è —É—á–∏—Ç—Å—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π.

### 2) –ó–∞—á–µ–º –∏ –∫–æ–≥–¥–∞ (Business Value)

**–ö–∞–∫—É—é –ø—Ä–æ–±–ª–µ–º—É —Ä–µ—à–∞–µ—Ç:**
- **–ö–æ–º–±–∏–Ω–∞—Ü–∏—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π** (–ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è + Random Forest + SVM)
- **–£–ª—É—á—à–µ–Ω–∏–µ** –ø—Ä–æ–≥–Ω–æ–∑–∞ –∑–∞ —Å—á–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏

**–ì–¥–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è:**
- **Kaggle**: winning solutions —á–∞—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É—é—Ç stacking
- **–ö—Ä–∏—Ç–∏—á–Ω—ã–µ –∑–∞–¥–∞—á–∏**: –º–µ–¥–∏—Ü–∏–Ω–∞, —Ñ–∏–Ω–∞–Ω—Å—ã (–≤—ã–∂–∏–º–∞–µ–º –º–∞–∫—Å–∏–º—É–º —Ç–æ—á–Ω–æ—Å—Ç–∏)

**–ö–æ–≥–¥–∞ –ø—Ä–∏–º–µ–Ω—è—Ç—å –Ω–µ–ª—å–∑—è:**
- ‚ùå –ú–∞–ª—ã–µ –¥–∞–Ω–Ω—ã–µ (—Ä–∏—Å–∫ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è –º–µ—Ç–∞-–º–æ–¥–µ–ª–∏)
- ‚ùå –ü—Ä–æ–¥–∞–∫—à–Ω —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è–º–∏ –Ω–∞ latency (–º–Ω–æ–≥–æ –º–æ–¥–µ–ª–µ–π)

### 3) –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —è–¥—Ä–æ

**–ê–ª–≥–æ—Ä–∏—Ç–º:**

1. **–û–±—É—á–∏—Ç—å –±–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ f_1, ..., f_K:**
   - –†–∞–∑–Ω—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã (LogReg, RF, XGBoost, ...)
   - –° cross-validation –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è leakage:
     ```
     –î–ª—è –∫–∞–∂–¥–æ–≥–æ fold:
       –û–±—É—á–∏—Ç—å f_k –Ω–∞ train fold
       –ü—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å –Ω–∞ validation fold ‚Üí meta-feature
     ```

2. **–°–æ–∑–¥–∞—Ç—å meta-features:**
   ```
   X_meta = [f_1(x), f_2(x), ..., f_K(x)]
   ```

3. **–û–±—É—á–∏—Ç—å –º–µ—Ç–∞-–º–æ–¥–µ–ª—å g –Ω–∞ (X_meta, y):**
   ```
   g: ‚Ñù^K ‚Üí ‚Ñù  (–∏–ª–∏ {0, 1} –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏)
   ```
   –û–±—ã—á–Ω–æ –ø—Ä–æ—Å—Ç–∞—è –º–æ–¥–µ–ª—å: LogReg, Ridge.

4. **–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ:**
   ```
   ≈∑ = g(f_1(x), ..., f_K(x))
   ```

### 4) Middle-level –Ω—é–∞–Ω—Å—ã

**–ü—Ä–æ–±–ª–µ–º–∞ 1: Data Leakage**

**–ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ:**
```python
# –û–±—É—á–∏—Ç—å –±–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –≤—Å–µ—Ö train
f1.fit(X_train, y_train)
# –ü—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å –Ω–∞ train ‚Üí meta-features
meta_X_train = f1.predict(X_train)  # ‚Üê leakage!
```

–ú–µ—Ç–∞-–º–æ–¥–µ–ª—å –≤–∏–¥–∏—Ç "–∏–¥–µ–∞–ª—å–Ω—ã–µ" –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ train ‚Üí –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ!

**–ü—Ä–∞–≤–∏–ª—å–Ω–æ (cross-validation):**
```python
from sklearn.model_selection import cross_val_predict
meta_X_train = cross_val_predict(f1, X_train, y_train, cv=5)
```

**–ü—Ä–æ–±–ª–µ–º–∞ 2: –í—ã–±–æ—Ä –º–µ—Ç–∞-–º–æ–¥–µ–ª–∏**

**–ü—Ä–æ—Å—Ç–∞—è –º–æ–¥–µ–ª—å** (Linear Regression, Logistic Regression):
- ‚úÖ –ú–µ–Ω—å—à–µ —Ä–∏—Å–∫ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
- ‚úÖ –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è: –≤–µ—Å–∞ = –≤–∞–∂–Ω–æ—Å—Ç—å –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π

**–°–ª–æ–∂–Ω–∞—è –º–æ–¥–µ–ª—å** (XGBoost):
- ‚ö†Ô∏è –†–∏—Å–∫ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è (–º–µ—Ç–∞-—Ñ–∏—á–µ–π –º–∞–ª–æ ‚Äî K —à—Ç—É–∫!)
- –ú–æ–∂–µ—Ç –Ω–∞–π—Ç–∏ –Ω–µ–ª–∏–Ω–µ–π–Ω—ã–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è:**  
–ù–∞—á–Ω–∏—Ç–µ —Å Ridge/LogReg. –ï—Å–ª–∏ –º–∞–ª–æ –º–æ–¥–µ–ª–µ–π (K < 5), –æ—Å—Ç–∞–≤–∞–π—Ç–µ—Å—å –ø—Ä–æ—Å—Ç—ã–º–∏!

### 5) –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª–µ–π

| –ú–µ—Ç–æ–¥ | –ë–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ | –ö–æ–º–±–∏–Ω–∞—Ü–∏—è | –¶–µ–ª—å | –°–ª–æ–∂–Ω–æ—Å—Ç—å |
|-------|---------------|-----------|------|-----------|
| **Bagging** | –û–¥–∏–Ω–∞–∫–æ–≤—ã–µ (–¥–µ—Ä–µ–≤—å—è) | –£—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ | ‚Üì variance | ‚úÖ –ü—Ä–æ—Å—Ç–æ–π |
| **Boosting** | –û–¥–∏–Ω–∞–∫–æ–≤—ã–µ (–¥–µ—Ä–µ–≤—å—è) | –í–∑–≤–µ—à–µ–Ω–Ω–∞—è —Å—É–º–º–∞ (–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ) | ‚Üì bias | ‚ö†Ô∏è –°—Ä–µ–¥–Ω–∏–π |
| **Stacking** | –†–∞–∑–Ω—ã–µ | –û–±—É—á–µ–Ω–Ω–∞—è –º–µ—Ç–∞-–º–æ–¥–µ–ª—å | –û–ø—Ç–∏–º–∞–ª—å–Ω–∞—è –∫–æ–º–±–∏–Ω–∞—Ü–∏—è | ‚ö†Ô∏è –°–ª–æ–∂–Ω—ã–π |

---

## üìù –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –∫–æ–¥ (sklearn)

### Random Forest —Å hyperparameters

```python
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.model_selection import GridSearchCV

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# –ì–ò–ü–ï–†–ü–ê–†–ê–ú–ï–¢–†–´ RANDOM FOREST
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# n_estimators: —á–∏—Å–ª–æ –¥–µ—Ä–µ–≤—å–µ–≤
#   - 100: default, —Ö–æ—Ä–æ—à–æ –¥–ª—è —Å—Ç–∞—Ä—Ç–∞
#   - 500-1000: –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏
#   - –ë–æ–ª—å—à–µ –¥–µ—Ä–µ–≤—å–µ–≤ = –º–µ–Ω—å—à–µ variance (–Ω–æ diminishing returns)
#
# max_features: —á–∏—Å–ª–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è split
#   - 'sqrt': ‚àöp –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ (default)
#   - 'log2': log‚ÇÇ(p)
#   - 0.3-0.5: –¥–æ–ª—è –æ—Ç p (–¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ —á–∞—Å—Ç–æ p/3)
#
# max_depth: –≥–ª—É–±–∏–Ω–∞ –¥–µ—Ä–µ–≤—å–µ–≤
#   - None: —Ä–∞—Å—Ç–∏—Ç—å –¥–æ –∫–æ–Ω—Ü–∞ (default, high variance per tree)
#   - 10-20: –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–ª—è –∫–æ–Ω—Ç—Ä–æ–ª—è overfitting
#
# min_samples_split, min_samples_leaf: –∫–æ–Ω—Ç—Ä–æ–ª—å –ª–∏—Å—Ç—å–µ–≤
#   - –£–≤–µ–ª–∏—á–∏—Ç—å –ø—Ä–∏ overfitting
#
# oob_score: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å OOB –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ ("–±–µ—Å–ø–ª–∞—Ç–Ω—ã–π" CV!)
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

# –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
rf_clf = RandomForestClassifier(
    n_estimators=500,
    max_features='sqrt',
    max_depth=None,
    min_samples_leaf=1,
    oob_score=True,       # ‚Üê "–±–µ—Å–ø–ª–∞—Ç–Ω–∞—è" –≤–∞–ª–∏–¥–∞—Ü–∏—è
    n_jobs=-1,            # –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–∞—Ü–∏—è –Ω–∞ –≤—Å–µ—Ö —è–¥—Ä–∞—Ö
    random_state=42
)
rf_clf.fit(X_train, y_train)
print(f"OOB Score: {rf_clf.oob_score_:.3f}")

# Feature Importance
import pandas as pd
importance = pd.DataFrame({
    'feature': X_train.columns,
    'importance': rf_clf.feature_importances_
}).sort_values('importance', ascending=False)
```

### XGBoost —Å hyperparameters

```python
from xgboost import XGBClassifier, XGBRegressor
from sklearn.model_selection import GridSearchCV

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# –ì–ò–ü–ï–†–ü–ê–†–ê–ú–ï–¢–†–´ XGBOOST
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# n_estimators: —á–∏—Å–ª–æ –¥–µ—Ä–µ–≤—å–µ–≤ (100-1000)
#
# learning_rate (Œ∑): shrinkage
#   - 0.1: —Å—Ç–∞–Ω–¥–∞—Ä—Ç
#   - 0.01-0.05: –ª—É—á—à–µ –≥–µ–Ω–µ—Ä–∞–ª–∏–∑–∞—Ü–∏—è, –Ω—É–∂–Ω–æ –±–æ–ª—å—à–µ –¥–µ—Ä–µ–≤—å–µ–≤
#
# max_depth: –≥–ª—É–±–∏–Ω–∞ –¥–µ—Ä–µ–≤—å–µ–≤
#   - 3-6: —Å—Ç–∞–Ω–¥–∞—Ä—Ç (shallow trees = weak learners!)
#   - –ì–ª—É–±–∂–µ ‚Üí —Ä–∏—Å–∫ overfitting
#
# subsample: –¥–æ–ª—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–µ—Ä–µ–≤–∞
#   - 0.8: —Å—Ç–∞–Ω–¥–∞—Ä—Ç
#   - <1.0: —Å—Ç–æ—Ö–∞—Å—Ç–∏—á–µ—Å–∫–∏–π GB, —Å–Ω–∏–∂–∞–µ—Ç overfitting
#
# colsample_bytree: –¥–æ–ª—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–µ—Ä–µ–≤–∞
#   - 0.8: —Å—Ç–∞–Ω–¥–∞—Ä—Ç
#
# reg_lambda (L2), reg_alpha (L1): —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
#   - 1.0: default –¥–ª—è lambda
#   - –£–≤–µ–ª–∏—á–∏—Ç—å –ø—Ä–∏ overfitting
#
# early_stopping_rounds: –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø—Ä–∏ —É—Ö—É–¥—à–µ–Ω–∏–∏ validation
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

xgb = XGBClassifier(
    n_estimators=500,
    learning_rate=0.1,
    max_depth=5,
    subsample=0.8,
    colsample_bytree=0.8,
    reg_lambda=1.0,
    reg_alpha=0.0,
    use_label_encoder=False,
    eval_metric='logloss',
    random_state=42
)

# Early Stopping (–ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û!)
xgb.fit(
    X_train, y_train,
    eval_set=[(X_val, y_val)],
    early_stopping_rounds=50,
    verbose=False
)
print(f"Best iteration: {xgb.best_iteration}")

# GridSearch –¥–ª—è XGBoost
param_grid = {
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.1],
    'n_estimators': [100, 300, 500]
}
grid = GridSearchCV(XGBClassifier(), param_grid, cv=3, scoring='roc_auc')
```

### Stacking —Å sklearn

```python
from sklearn.ensemble import StackingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier

# Stacking: –∫–æ–º–±–∏–Ω–∞—Ü–∏—è —Ä–∞–∑–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
stacking = StackingClassifier(
    estimators=[
        ('rf', RandomForestClassifier(n_estimators=100)),
        ('xgb', XGBClassifier(n_estimators=100, use_label_encoder=False)),
    ],
    final_estimator=LogisticRegression(),  # –º–µ—Ç–∞-–º–æ–¥–µ–ª—å
    cv=5,                                   # CV –¥–ª—è –º–µ—Ç–∞-–ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    stack_method='predict_proba'           # –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –∫–∞–∫ –º–µ—Ç–∞-—Ñ–∏—á–∏
)
stacking.fit(X_train, y_train)
```

### –¢–∞–±–ª–∏—Ü–∞ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤

| –ú–æ–¥–µ–ª—å | –ü–∞—Ä–∞–º–µ—Ç—Ä | Default | –î–∏–∞–ø–∞–∑–æ–Ω | –ö–æ–≥–¥–∞ –º–µ–Ω—è—Ç—å |
|--------|----------|---------|----------|--------------|
| **RF** | n_estimators | 100 | 100-1000 | –ë–æ–ª—å—à–µ = –ª—É—á—à–µ (–¥–æ –ø–ª–∞—Ç–æ) |
| | max_features | sqrt | sqrt, log2, 0.3-1.0 | –ü–æ–¥–±–æ—Ä —á–µ—Ä–µ–∑ CV |
| | max_depth | None | 10-30, None | –ü—Ä–∏ overfitting |
| **XGB** | n_estimators | 100 | 100-5000 | –° early_stopping |
| | learning_rate | 0.3 | 0.01-0.3 | Œ∑‚Üì —Ç—Ä–µ–±—É–µ—Ç n_estimators‚Üë |
| | max_depth | 6 | 3-10 | –ö–ª—é—á–µ–≤–æ–π –¥–ª—è overfitting |
| | subsample | 1.0 | 0.5-1.0 | –ü—Ä–∏ overfitting |

---

## üéØ Q&A –¥–ª—è —ç–∫–∑–∞–º–µ–Ω–∞

**Q1: –ü–æ—á–µ–º—É Random Forest —É—Å—Ç–æ–π—á–∏–≤ –∫ overfitting?**
> Bagging —Å–Ω–∏–∂–∞–µ—Ç variance —á–µ—Ä–µ–∑ —É—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ. –î–∞–∂–µ –µ—Å–ª–∏ –∫–∞–∂–¥–æ–µ –¥–µ—Ä–µ–≤–æ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–æ, –∏—Ö —Å—Ä–µ–¥–Ω–µ–µ ‚Äî –Ω–µ—Ç (–ø—Ä–∏ —É—Å–ª–æ–≤–∏–∏ –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏). Random feature selection –¥–µ–ª–∞–µ—Ç –¥–µ—Ä–µ–≤—å—è –º–µ–Ω–µ–µ –∫–æ—Ä—Ä–µ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ ‚Üí —É—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ.

**Q2: –ß–µ–º –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è Bagging –æ—Ç Boosting?**
> Bagging: –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã—Ö –º–æ–¥–µ–ª–µ–π, —É—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ, —Ü–µ–ª—å ‚Äî —Å–Ω–∏–∑–∏—Ç—å variance. Boosting: –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ, –∫–∞–∂–¥–∞—è –º–æ–¥–µ–ª—å –∏—Å–ø—Ä–∞–≤–ª—è–µ—Ç –æ—à–∏–±–∫–∏ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö, —Ü–µ–ª—å ‚Äî —Å–Ω–∏–∑–∏—Ç—å bias.

**Q3: –ó–∞—á–µ–º –Ω—É–∂–µ–Ω early stopping –≤ XGBoost?**
> Boosting –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ —Å–Ω–∏–∂–∞–µ—Ç bias, –Ω–æ –º–æ–∂–µ—Ç —É–≤–µ–ª–∏—á–∏–≤–∞—Ç—å variance (overfitting). Early stopping –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –æ–±—É—á–µ–Ω–∏–µ –∫–æ–≥–¥–∞ validation error –Ω–∞—á–∏–Ω–∞–µ—Ç —Ä–∞—Å—Ç–∏ ‚Üí –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –±–∞–ª–∞–Ω—Å bias/variance.

**Q4: –ü–æ—á–µ–º—É OOB error –ø–æ—á—Ç–∏ —ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç–µ–Ω CV?**
> –ü—Ä–∏ bootstrap ~37% –¥–∞–Ω–Ω—ã—Ö –Ω–µ –ø–æ–ø–∞–¥–∞–µ—Ç –≤ –∫–∞–∂–¥—ã–π sample (e^(-1) ‚âà 0.368). –≠—Ç–∏ out-of-bag –ø—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏ ‚Üí —ç—Ñ—Ñ–µ–∫—Ç –∫–∞–∫ cross-validation, –Ω–æ "–±–µ—Å–ø–ª–∞—Ç–Ω–æ".

**Q5: –ö–æ–≥–¥–∞ Stacking –ª—É—á—à–µ –ø—Ä–æ—Å—Ç–æ–≥–æ —É—Å—Ä–µ–¥–Ω–µ–Ω–∏—è?**
> –ö–æ–≥–¥–∞ –±–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ —Ä–∞–∑–Ω—ã–µ –ø–æ –ø—Ä–∏—Ä–æ–¥–µ (LogReg + RF + XGB) –∏ –∏–º–µ—é—Ç —Ä–∞–∑–Ω—ã–µ —Å–∏–ª—å–Ω—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã. –ú–µ—Ç–∞-–º–æ–¥–µ–ª—å —É—á–∏—Ç—Å—è –≤–∑–≤–µ—à–∏–≤–∞—Ç—å –∏—Ö –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ. –î–ª—è –ø–æ—Ö–æ–∂–∏—Ö –º–æ–¥–µ–ª–µ–π (10 RF) –ø—Ä–æ—Å—Ç–æ–µ —É—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ.

---

## –†–µ–∑—é–º–µ –º–æ–¥—É–ª—è

–ê–Ω—Å–∞–º–±–ª–∏ ‚Äî —ç—Ç–æ **"–º—É–¥—Ä–æ—Å—Ç—å —Ç–æ–ª–ø—ã"** –≤ ML:
- **Bagging**: —Å–Ω–∏–∂–∞–µ—Ç variance —á–µ—Ä–µ–∑ —É—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã—Ö –º–æ–¥–µ–ª–µ–π, OOB validation
- **Random Forest**: bagging + random features ‚Üí decorrelation, feature importance
- **Boosting**: —Å–Ω–∏–∂–∞–µ—Ç bias —á–µ—Ä–µ–∑ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—É—é –∫–æ—Ä—Ä–µ–∫—Ü–∏—é –æ—à–∏–±–æ–∫, gradient descent –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ —Ñ—É–Ω–∫—Ü–∏–π
- **Stacking**: –º–µ—Ç–∞-–º–æ–¥–µ–ª—å –¥–ª—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π

Random Forest –∏ XGBoost ‚Äî —Ä–∞–±–æ—á–∏–µ –ª–æ—à–∞–¥–∫–∏ –¥–ª—è —Ç–∞–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö!
