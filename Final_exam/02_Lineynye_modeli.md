# –ú–æ–¥—É–ª—å 2: –õ–∏–Ω–µ–π–Ω—ã–µ –º–æ–¥–µ–ª–∏

> **–ó–∞—á–µ–º —ç—Ç–æ—Ç –º–æ–¥—É–ª—å?**  
> –õ–∏–Ω–µ–π–Ω—ã–µ –º–æ–¥–µ–ª–∏ ‚Äî —ç—Ç–æ **—Ä–∞–±–æ—á–∞—è –ª–æ—à–∞–¥–∫–∞** Machine Learning. –ù–µ—Å–º–æ—Ç—Ä—è –Ω–∞ –ø—Ä–æ—Å—Ç–æ—Ç—É, –æ–Ω–∏:
> - **Baseline** –¥–ª—è –ª—é–±–æ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞ (–Ω–∞—á–∏–Ω–∞–π—Ç–µ –≤—Å–µ–≥–¥–∞ —Å –ø—Ä–æ—Å—Ç–æ–≥–æ!)
> - **Production-ready**: –±—ã—Å—Ç—Ä—ã–µ, –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º—ã–µ, –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—ã–µ
> - **–§—É–Ω–¥–∞–º–µ–Ω—Ç** –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏, loss functions –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
> 
> –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è (Ridge/Lasso), –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è –∏ SVM –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è –≤ 80% —Ä–µ–∞–ª—å–Ω—ã—Ö –ø—Ä–æ–µ–∫—Ç–æ–≤.

---

## 2.1 –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è (Regularization): Ridge (L2) –∏ Lasso (L1)

### 1) –ò–Ω—Ç—É–∏—Ü–∏—è –∏ —Å—É—Ç—å

**–ü—Ä–æ—Å—Ç—ã–º–∏ —Å–ª–æ–≤–∞–º–∏ (ELI5):**  
–ü—Ä–µ–¥—Å—Ç–∞–≤—å, —á—Ç–æ —Ç—ã —Å—Ç—Ä–æ–∏—à—å –º–æ–¥–µ–ª—å —Å–∞–º–æ–ª–µ—Ç–∞ –∏–∑ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä–∞. –ï—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –í–°–ï –¥–µ—Ç–∞–ª–∏ –∏–∑ –∫–æ—Ä–æ–±–∫–∏ (–≤—Å–µ –ø—Ä–∏–∑–Ω–∞–∫–∏), —Å–∞–º–æ–ª–µ—Ç –ø–æ–ª—É—á–∏—Ç—Å—è —Å–ª–∏—à–∫–æ–º —Å–ª–æ–∂–Ω—ã–º –∏ —Ä–∞–∑–≤–∞–ª–∏—Ç—Å—è –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –ø–æ–ª–µ—Ç–µ (–ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ). –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è ‚Äî —ç—Ç–æ –ø—Ä–∞–≤–∏–ª–æ: "–∏—Å–ø–æ–ª—å–∑—É–π –∫–∞–∫ –º–æ–∂–Ω–æ –º–µ–Ω—å—à–µ –¥–µ—Ç–∞–ª–µ–π, –Ω–æ —á—Ç–æ–±—ã —Å–∞–º–æ–ª–µ—Ç –ª–µ—Ç–∞–ª". 

- **L2 (Ridge)**: –¥–µ–ª–∞–µ—Ç –≤—Å–µ –¥–µ—Ç–∞–ª–∏ –º–µ–Ω—å—à–µ (—É–º–µ–Ω—å—à–∞–µ—Ç –≤—Å–µ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã)
- **L1 (Lasso)**: –≤—ã–±—Ä–∞—Å—ã–≤–∞–µ—Ç –Ω–µ–Ω—É–∂–Ω—ã–µ –¥–µ—Ç–∞–ª–∏ —Å–æ–≤—Å–µ–º (–æ–±–Ω—É–ª—è–µ—Ç –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã)

**–ê–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏:**  
–†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è ‚Äî —ç—Ç–æ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ **—à—Ç—Ä–∞—Ñ–∞ (penalty)** –∫ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –∑–∞ —Å–ª–æ–∂–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏. –¶–µ–ª—å ‚Äî –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—Ç–∏—Ç—å –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ, –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É—è –≤–µ–ª–∏—á–∏–Ω—É –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–≤.

**–§–∏–∑–∏—á–µ—Å–∫–∏–π —Å–º—ã—Å–ª:**  
–ë–æ–ª—å—à–∏–µ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã ‚Üí –º–æ–¥–µ–ª—å —Å–∏–ª—å–Ω–æ —Ä–µ–∞–≥–∏—Ä—É–µ—Ç –Ω–∞ –º–∞–ª—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ ‚Üí –≤—ã—Å–æ–∫–∞—è **variance**. –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è "—Å–∂–∏–º–∞–µ—Ç" –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã, –¥–µ–ª–∞—è –º–æ–¥–µ–ª—å –±–æ–ª–µ–µ —É—Å—Ç–æ–π—á–∏–≤–æ–π.

### 2) –ó–∞—á–µ–º –∏ –∫–æ–≥–¥–∞ (Business Value)

**–ö–∞–∫—É—é –ø—Ä–æ–±–ª–µ–º—É —Ä–µ—à–∞–µ—Ç:**
- **–ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ** (overfitting) –ø—Ä–∏ –±–æ–ª—å—à–æ–º —á–∏—Å–ª–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
- **–ú—É–ª—å—Ç–∏–∫–æ–ª–ª–∏–Ω–µ–∞—Ä–Ω–æ—Å—Ç—å** (—Å–∏–ª—å–Ω–æ –∫–æ—Ä—Ä–µ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏)
- **Feature selection** (L1 –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤—ã–±–∏—Ä–∞–µ—Ç –≤–∞–∂–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏)
- **–ß–∏—Å–ª–µ–Ω–Ω–∞—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å** (Ridge –¥–µ–ª–∞–µ—Ç (X^T X)^(-1) –±–æ–ª–µ–µ —É—Å—Ç–æ–π—á–∏–≤–æ–π)

**–ì–¥–µ —ç—Ç–æ –∏–Ω–¥—É—Å—Ç—Ä–∏–∞–ª—å–Ω—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç:**
- **–§–∏–Ω–∞–Ω—Å—ã**: –∫—Ä–µ–¥–∏—Ç–Ω—ã–π —Å–∫–æ—Ä–∏–Ω–≥ (–º–Ω–æ–≥–æ –∫–æ—Ä—Ä–µ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤)
- **–ì–µ–Ω–æ–º–∏–∫–∞**: p >> n (–ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –±–æ–ª—å—à–µ, —á–µ–º –æ–±—ä–µ–∫—Ç–æ–≤)
- **NLP**: text classification (–æ–≥—Ä–æ–º–Ω–æ–µ —á–∏—Å–ª–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ ‚Äî —Å–ª–æ–≤–∞)
- **–†–µ–∫–æ–º–µ–Ω–¥–∞—Ç–µ–ª—å–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã**: matrix factorization (Ridge –¥–ª—è regularization)

**–ö–æ–≥–¥–∞ –ø—Ä–∏–º–µ–Ω—è—Ç—å –Ω–µ–ª—å–∑—è:**
- ‚ùå –ú–∞–ª–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏ –Ω–µ—Ç –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è (–Ω–µ–∑–∞—á–µ–º)
- ‚ùå –í—Å–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –≤–∞–∂–Ω—ã –∏ –Ω–µ–∫–æ—Ä—Ä–µ–ª–∏—Ä–æ–≤–∞–Ω—ã (Lasso –º–æ–∂–µ—Ç —É–¥–∞–ª–∏—Ç—å –Ω—É–∂–Ω—ã–µ)
- ‚ùå –ù—É–∂–Ω–∞ —Ç–æ—á–Ω–∞—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–≤ (—Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è –∏—Ö "—Ä–∞–∑–º—ã–≤–∞–µ—Ç")

### 3) –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —è–¥—Ä–æ

**Ridge Regression (L2):**

**–§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å:**
$$
L(\boldsymbol{\beta}) = \|\mathbf{y} - \mathbf{X}\boldsymbol{\beta}\|^2 + \lambda\|\boldsymbol{\beta}\|^2_2 = \sum_{i=1}^{n}(y_i - \mathbf{x}_i^T \boldsymbol{\beta})^2 + \lambda \sum_{j=1}^{p} \beta_j^2
$$

$$
\underbrace{\text{MSE}}_{\text{fit to data}} + \underbrace{\lambda\|\boldsymbol{\beta}\|^2_2}_{\text{L2 penalty (complexity)}}
$$

**–ê–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–æ–µ —Ä–µ—à–µ–Ω–∏–µ:**
$$
\hat{\boldsymbol{\beta}}_{\text{Ridge}} = (\mathbf{X}^T \mathbf{X} + \lambda \mathbf{I})^{-1} \mathbf{X}^T \mathbf{y}
$$

> [!IMPORTANT]
> –î–æ–±–∞–≤–ª–µ–Ω–∏–µ $\lambda \mathbf{I}$ –∫ $\mathbf{X}^T \mathbf{X}$ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç **–æ–±—Ä–∞—Ç–∏–º–æ—Å—Ç—å** –¥–∞–∂–µ –ø—Ä–∏ –º—É–ª—å—Ç–∏–∫–æ–ª–ª–∏–Ω–µ–∞—Ä–Ω–æ—Å—Ç–∏! –≠—Ç–æ –∫–ª—é—á–µ–≤–æ–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ Ridge.

**Lasso Regression (L1):**

**–§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å:**
$$
L(\boldsymbol{\beta}) = \|\mathbf{y} - \mathbf{X}\boldsymbol{\beta}\|^2 + \lambda\|\boldsymbol{\beta}\|_1 = \sum_{i=1}^{n}(y_i - \mathbf{x}_i^T \boldsymbol{\beta})^2 + \lambda \sum_{j=1}^{p} |\beta_j|
$$

**–†–µ—à–µ–Ω–∏–µ:**  
–ù–µ—Ç –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–æ–π —Ñ–æ—Ä–º—É–ª—ã! –ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã:
- **Coordinate Descent**
- **Proximal Gradient Descent**

**Elastic Net (–∫–æ–º–±–∏–Ω–∞—Ü–∏—è L1 –∏ L2):**
$$
L(\boldsymbol{\beta}) = \|\mathbf{y} - \mathbf{X}\boldsymbol{\beta}\|^2 + \lambda_1\|\boldsymbol{\beta}\|_1 + \lambda_2\|\boldsymbol{\beta}\|^2_2
$$

–°–æ—á–µ—Ç–∞–µ—Ç –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –æ–±–æ–∏—Ö: feature selection (L1) + —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∫ –º—É–ª—å—Ç–∏–∫–æ–ª–ª–∏–Ω–µ–∞—Ä–Ω–æ—Å—Ç–∏ (L2).

> [!TIP]
> **–ê–Ω–∞–ª–æ–≥–∏—è: –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è –∫–∞–∫ —à—Ç—Ä–∞—Ñ –∑–∞ —Å–ª–æ–∂–Ω–æ—Å—Ç—å**  
> –ü—Ä–µ–¥—Å—Ç–∞–≤—å—Ç–µ, —á—Ç–æ –≤—ã –ø–∏—à–µ—Ç–µ —ç—Å—Å–µ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –ø–æ –¥–ª–∏–Ω–µ. L2 (Ridge) –≥–æ–≤–æ—Ä–∏—Ç: "–∏—Å–ø–æ–ª—å–∑—É–π –≤—Å–µ —Å–ª–æ–≤–∞, –Ω–æ –¥–µ–ª–∞–π –∏—Ö –∫–æ—Ä–æ—á–µ". L1 (Lasso) –≥–æ–≤–æ—Ä–∏—Ç: "–≤—ã–±—Ä–æ—Å—å –Ω–µ–Ω—É–∂–Ω—ã–µ —Å–ª–æ–≤–∞ —Å–æ–≤—Å–µ–º".

### 4) Middle-level –Ω—é–∞–Ω—Å—ã

**–ü—Ä–æ–±–ª–µ–º–∞ 1: –ì–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–π —Å–º—ã—Å–ª L1 vs L2**

**L2 (Ridge):**
```
Constraint region: ||Œ≤||¬≤‚ÇÇ ‚â§ t  (–∫—Ä—É–≥/—Å—Ñ–µ—Ä–∞)
```

–†–µ—à–µ–Ω–∏–µ ‚Äî —Ç–æ—á–∫–∞ –∫–∞—Å–∞–Ω–∏—è —ç–ª–ª–∏–ø—Å–∞ (contour lines MSE) –∏ –∫—Ä—É–≥–∞ (L2 constraint).  
–ö—Ä—É–≥ –≥–ª–∞–¥–∫–∏–π ‚Üí —Ä–µ—à–µ–Ω–∏—è —Ä–µ–¥–∫–æ –ø–æ–ø–∞–¥–∞—é—Ç –Ω–∞ –æ—Å–∏ ‚Üí **–∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã –º–∞–ª—ã–µ, –Ω–æ –Ω–µ–Ω—É–ª–µ–≤—ã–µ**.

**L1 (Lasso):**
```
Constraint region: ||Œ≤||‚ÇÅ ‚â§ t  (—Ä–æ–º–±/–≥–∏–ø–µ—Ä—Ä–æ–º–±)
```

–†–æ–º–± –∏–º–µ–µ—Ç **–æ—Å—Ç—Ä—ã–µ —É–≥–ª—ã –Ω–∞ –æ—Å—è—Ö** ‚Üí —Ä–µ—à–µ–Ω–∏—è —á–∞—Å—Ç–æ –ø–æ–ø–∞–¥–∞—é—Ç –Ω–∞ —É–≥–ª—ã ‚Üí **–Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã = 0**.

**–í–∏–∑—É–∞–ª—å–Ω–æ (2D):**

```
       Œ≤‚ÇÇ               L1 (Lasso)
        ‚îÇ    ‚ï±‚ï≤
        ‚îÇ   ‚ï±  ‚ï≤        ‚Üê –†–æ–º–±
        ‚îÇ  ‚ï± ‚óè  ‚ï≤       ‚óè ‚Äî —Ç–æ—á–∫–∞ –∫–∞—Å–∞–Ω–∏—è
        ‚îÇ ‚ï±  ‚ï≤
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Œ≤‚ÇÅ
        
       Œ≤‚ÇÇ               L2 (Ridge)
        ‚îÇ   ‚ï≠‚îÄ‚îÄ‚ïÆ
        ‚îÇ  ‚îÇ ‚óè ‚îÇ        ‚Üê –ö—Ä—É–≥
        ‚îÇ   ‚ï∞‚îÄ‚îÄ‚ïØ
        ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Œ≤‚ÇÅ
```

**–í—ã–≤–æ–¥:**  
L1 –æ–±–Ω—É–ª—è–µ—Ç –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã (feature selection), L2 ‚Äî —Ç–æ–ª—å–∫–æ —É–º–µ–Ω—å—à–∞–µ—Ç.

**–ü—Ä–æ–±–ª–µ–º–∞ 2: –í—ã–±–æ—Ä Œª (–≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏)**

**Œª = 0**: –Ω–µ—Ç —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏ ‚Üí –æ–±—ã—á–Ω–∞—è OLS ‚Üí –º–æ–∂–µ—Ç –ø–µ—Ä–µ–æ–±—É—á–∞—Ç—å—Å—è  
**Œª ‚Üí ‚àû**: –≤—Å–µ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã ‚Üí 0 ‚Üí –º–æ–¥–µ–ª—å –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç —Ç–æ–ª—å–∫–æ —Å—Ä–µ–¥–Ω–µ–µ (underfitting)

**–ú–µ—Ç–æ–¥ –ø–æ–¥–±–æ—Ä–∞:**
- **Cross-Validation** (GridSearchCV, RidgeCV, LassoCV)
- **Regularization path**: –ø–æ—Å—Ç—Ä–æ–∏—Ç—å –≥—Ä–∞—Ñ–∏–∫ MSE(Œª) –∏ –≤—ã–±—Ä–∞—Ç—å –º–∏–Ω–∏–º—É–º –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏

**–ü—Ä–∏–º–µ—Ä:**
```python
from sklearn.linear_model import RidgeCV
ridge = RidgeCV(alphas=[0.01, 0.1, 1, 10, 100], cv=5)
ridge.fit(X_train, y_train)
print(ridge.alpha_)  # –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ Œª
```

**–ü—Ä–æ–±–ª–µ–º–∞ 3: –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–ö–†–ò–¢–ò–ß–ù–û!)**

–†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è —à—Ç—Ä–∞—Ñ—É–µ—Ç –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã **–ø–æ –∞–±—Å–æ–ª—é—Ç–Ω–æ–π –≤–µ–ª–∏—á–∏–Ω–µ**.  
–ï—Å–ª–∏ –ø—Ä–∏–∑–Ω–∞–∫ X‚ÇÅ ‚àà [0, 1], –∞ X‚ÇÇ ‚àà [0, 1000], —Ç–æ Œ≤‚ÇÇ –±—É–¥–µ—Ç –≤ ~1000 —Ä–∞–∑ –º–µ–Ω—å—à–µ Œ≤‚ÇÅ –ø—Ä–æ—Å—Ç–æ –∏–∑-–∑–∞ –º–∞—Å—à—Ç–∞–±–∞!

**–†–µ—à–µ–Ω–∏–µ:**  
–í–°–ï–ì–î–ê **—Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞—Ç—å** –ø—Ä–∏–∑–Ω–∞–∫–∏ –ø–µ—Ä–µ–¥ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–µ–π:

```python
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_train)
```

> [!CAUTION]
> **Production Warning: Feature Scaling –æ–±—è–∑–∞—Ç–µ–ª–µ–Ω!**  
> –ï—Å–ª–∏ –ø—Ä–∏–∑–Ω–∞–∫ "–≤–æ–∑—Ä–∞—Å—Ç" [0-100], –∞ "–∑–∞—Ä–ø–ª–∞—Ç–∞" [0-1000000], —Ç–æ –±–µ–∑ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è —É–±—å—ë—Ç –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –∑–∞—Ä–ø–ª–∞—Ç—ã (–æ–Ω –±—É–¥–µ—Ç –≤ 10000 —Ä–∞–∑ –º–µ–Ω—å—à–µ!). –ú–æ–¥–µ–ª—å –ø–æ—Ç–µ—Ä—è–µ—Ç –≤–∞–∂–Ω—ã–π –ø—Ä–∏–∑–Ω–∞–∫ –∏–∑-–∑–∞ –º–∞—Å—à—Ç–∞–±–∞, –∞ –Ω–µ –∏–∑-–∑–∞ –µ–≥–æ –Ω–µ–≤–∞–∂–Ω–æ—Å—Ç–∏.

**–ü—Ä–æ–±–ª–µ–º–∞ 4: Lasso –Ω–µ—Å—Ç–∞–±–∏–ª–µ–Ω –ø—Ä–∏ –º—É–ª—å—Ç–∏–∫–æ–ª–ª–∏–Ω–µ–∞—Ä–Ω–æ—Å—Ç–∏**

–ï—Å–ª–∏ –¥–≤–∞ –ø—Ä–∏–∑–Ω–∞–∫–∞ —Å–∏–ª—å–Ω–æ –∫–æ—Ä—Ä–µ–ª–∏—Ä–æ–≤–∞–Ω—ã, Lasso **—Å–ª—É—á–∞–π–Ω–æ –≤—ã–±–µ—Ä–µ—Ç –æ–¥–∏–Ω**, –∞ –¥—Ä—É–≥–æ–π –æ–±–Ω—É–ª–∏—Ç.  
–ü—Ä–∏ –º–∞–ª–æ–º –∏–∑–º–µ–Ω–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö –º–æ–∂–µ—Ç –≤—ã–±—Ä–∞—Ç—å –¥—Ä—É–≥–æ–π –ø—Ä–∏–∑–Ω–∞–∫!

**–ü—Ä–∏–º–µ—Ä:**
```
–ü—Ä–∏–∑–Ω–∞–∫–∏: –ø–ª–æ—â–∞–¥—å_–∫–≤_–º –∏ –ø–ª–æ—â–∞–¥—å_–∫–≤_—Ñ—É—Ç—ã (–ø–æ–ª–Ω–æ—Å—Ç—å—é –∫–æ—Ä—Ä–µ–ª–∏—Ä–æ–≤–∞–Ω—ã)
Lasso –º–æ–∂–µ—Ç –≤—ã–±—Ä–∞—Ç—å –ª—é–±–æ–π –∏–∑ –Ω–∏—Ö, —Ä–µ–∑—É–ª—å—Ç–∞—Ç –Ω–µ—Å—Ç–∞–±–∏–ª–µ–Ω
```

**–†–µ—à–µ–Ω–∏–µ:**
- **Elastic Net**: Œ±¬∑L1 + (1-Œ±)¬∑L2, –æ–±—ã—á–Ω–æ Œ± = 0.5
- –í—Ä—É—á–Ω—É—é —É–¥–∞–ª–∏—Ç—å –æ–¥–∏–Ω –∏–∑ –∫–æ—Ä—Ä–µ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤

### 5) –°—Ä–∞–≤–Ω–µ–Ω–∏–µ Ridge vs Lasso vs Elastic Net

| –ú–µ—Ç–æ–¥ | L1 penalty | L2 penalty | Feature Selection | –ú—É–ª—å—Ç–∏–∫–æ–ª–ª–∏–Ω–µ–∞—Ä–Ω–æ—Å—Ç—å | –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å |
|-------|------------|------------|-------------------|---------------------|-------------------|
| **OLS** | ‚ùå | ‚ùå | ‚ùå | ‚ùå –ß—É–≤—Å—Ç–≤–∏—Ç–µ–ª–µ–Ω | –ú–∞–ª–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –Ω–µ—Ç –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π |
| **Ridge (L2)** | ‚ùå | ‚úÖ | ‚ùå –í—Å–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –æ—Å—Ç–∞—é—Ç—Å—è | ‚úÖ –£—Å—Ç–æ–π—á–∏–≤ | –ú–Ω–æ–≥–æ –∫–æ—Ä—Ä–µ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –≤—Å–µ –≤–∞–∂–Ω—ã |
| **Lasso (L1)** | ‚úÖ | ‚ùå | ‚úÖ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π | ‚ö†Ô∏è –ù–µ—Å—Ç–∞–±–∏–ª–µ–Ω | p >> n, –Ω—É–∂–Ω–∞ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è, feature selection |
| **Elastic Net** | ‚úÖ | ‚úÖ | ‚úÖ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π | ‚úÖ –£—Å—Ç–æ–π—á–∏–≤ | –ö–æ–º–±–∏–Ω–∞—Ü–∏—è: feature selection + –≥—Ä—É–ø–ø—ã –∫–æ—Ä—Ä–µ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ |

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:**
- **Ridge**: –∫–æ–≥–¥–∞ –Ω—É–∂–Ω—ã –≤—Å–µ –ø—Ä–∏–∑–Ω–∞–∫–∏, –µ—Å—Ç—å –º—É–ª—å—Ç–∏–∫–æ–ª–ª–∏–Ω–µ–∞—Ä–Ω–æ—Å—Ç—å
- **Lasso**: –∫–æ–≥–¥–∞ –Ω—É–∂–Ω–∞ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç—å (sparsity), feature selection
- **Elastic Net**: –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ —Ä–µ–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á (–∑–æ–ª–æ—Ç–∞—è —Å–µ—Ä–µ–¥–∏–Ω–∞)

---

## 2.2 –õ–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è (Logistic Regression)

### 1) –ò–Ω—Ç—É–∏—Ü–∏—è –∏ —Å—É—Ç—å

**–ü—Ä–æ—Å—Ç—ã–º–∏ —Å–ª–æ–≤–∞–º–∏ (ELI5):**  
–¢—ã –Ω–∞ —Å–æ–±–µ—Å–µ–¥–æ–≤–∞–Ω–∏–∏ –∏ —Ä–µ—à–∞–µ—à—å: –≤–∑—è—Ç—å –∫–∞–Ω–¥–∏–¥–∞—Ç–∞ –∏–ª–∏ –Ω–µ—Ç. –£ —Ç–µ–±—è –µ—Å—Ç—å –µ–≥–æ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ (–æ–ø—ã—Ç, –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ, –Ω–∞–≤—ã–∫–∏). –õ–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è –ø—Ä–µ–≤—Ä–∞—â–∞–µ—Ç —ç—Ç–∏ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –≤ **–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å** –æ—Ç 0 –¥–æ 1: –Ω–∞–ø—Ä–∏–º–µ—Ä, 0.85 = "—Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ –ø–æ–¥—Ö–æ–¥–∏—Ç". –ï—Å–ª–∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å > 0.5, –±–µ—Ä–µ–º –∫–∞–Ω–¥–∏–¥–∞—Ç–∞.

**–ê–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏:**  
–õ–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è ‚Äî —ç—Ç–æ **–ª–∏–Ω–µ–π–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –±–∏–Ω–∞—Ä–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏**, –∫–æ—Ç–æ—Ä–∞—è –º–æ–¥–µ–ª–∏—Ä—É–µ—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–Ω–æ—Å—Ç–∏ –∫ –∫–ª–∞—Å—Å—É 1 —á–µ—Ä–µ–∑ **–ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫—É—é —Ñ—É–Ω–∫—Ü–∏—é (—Å–∏–≥–º–æ–∏–¥—É)**. –ù–µ—Å–º–æ—Ç—Ä—è –Ω–∞ –Ω–∞–∑–≤–∞–Ω–∏–µ "—Ä–µ–≥—Ä–µ—Å—Å–∏—è", —ç—Ç–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä!

**–§–∏–∑–∏—á–µ—Å–∫–∏–π —Å–º—ã—Å–ª:**  
–õ–∏–Ω–µ–π–Ω–∞—è –∫–æ–º–±–∏–Ω–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (z = w^T x) –º–æ–∂–µ—Ç –±—ã—Ç—å –ª—é–±—ã–º —á–∏—Å–ª–æ–º, –Ω–æ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –≤ [0, 1]. –°–∏–≥–º–æ–∏–¥–∞ "—Å–∂–∏–º–∞–µ—Ç" z –≤ (0, 1).

### 2) –ó–∞—á–µ–º –∏ –∫–æ–≥–¥–∞ (Business Value)

**–ö–∞–∫—É—é –ø—Ä–æ–±–ª–µ–º—É —Ä–µ—à–∞–µ—Ç:**
- **–ë–∏–Ω–∞—Ä–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è** —Å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω—ã–º–∏ –æ—Ü–µ–Ω–∫–∞–º–∏
- **–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –≤–∞–∂–Ω–æ—Å—Ç–∏** –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (odds ratios)
- **–ö–∞–ª–∏–±—Ä–æ–≤–∫–∞**: –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –±–ª–∏–∑–∫–∏ –∫ —Ä–µ–∞–ª—å–Ω—ã–º —á–∞—Å—Ç–æ—Ç–∞–º

**–ì–¥–µ —ç—Ç–æ –∏–Ω–¥—É—Å—Ç—Ä–∏–∞–ª—å–Ω—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç:**
- **–§–∏–Ω–∞–Ω—Å—ã**: –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –¥–µ—Ñ–æ–ª—Ç–∞ –ø–æ –∫—Ä–µ–¥–∏—Ç—É
- **–ú–µ–¥–∏—Ü–∏–Ω–∞**: –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –∑–∞–±–æ–ª–µ–≤–∞–Ω–∏—è (–¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞)
- **–ú–∞—Ä–∫–µ—Ç–∏–Ω–≥**: –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ø–æ–∫—É–ø–∫–∏ (click-through rate, CTR)
- **Fraud detection**: –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–∞

**–ö–æ–≥–¥–∞ –ø—Ä–∏–º–µ–Ω—è—Ç—å –Ω–µ–ª—å–∑—è:**
- ‚ùå –ù–µ–ª–∏–Ω–µ–π–Ω—ã–µ –≥—Ä–∞–Ω–∏—Ü—ã —Ä–µ—à–µ–Ω–∏–π (–∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ kernel SVM, tree-based)
- ‚ùå –ú—É–ª—å—Ç–∏–∫–ª–∞—Å—Å–æ–≤–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è (–∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ One-vs-Rest –∏–ª–∏ Multinomial Logistic)
- ‚ùå –û—á–µ–Ω—å –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫–ª–∞—Å—Å—ã –±–µ–∑ –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏ (–Ω—É–∂–Ω—ã class weights –∏–ª–∏ resampling)

### 3) –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —è–¥—Ä–æ

**–õ–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ñ—É–Ω–∫—Ü–∏—è (—Å–∏–≥–º–æ–∏–¥–∞):**
$$
\sigma(z) = \frac{1}{1 + e^{-z}} = \frac{e^z}{e^z + 1}
$$

**–°–≤–æ–π—Å—Ç–≤–∞:**
- $\sigma(z) \in (0, 1)$ –¥–ª—è –ª—é–±–æ–≥–æ $z \in \mathbb{R}$
- $\sigma(0) = 0.5$
- $\sigma(z) \to 1$ –ø—Ä–∏ $z \to \infty$
- $\sigma(z) \to 0$ –ø—Ä–∏ $z \to -\infty$
- **–°–∏–º–º–µ—Ç—Ä–∏—è**: $\sigma(-z) = 1 - \sigma(z)$

**–ú–æ–¥–µ–ª—å:**
$$
z = \mathbf{w}^T \mathbf{x} + b  \quad \text{(–ª–∏–Ω–µ–π–Ω–∞—è –∫–æ–º–±–∏–Ω–∞—Ü–∏—è)}
$$
$$
P(y=1|\mathbf{x}) = \sigma(z) = \frac{1}{1 + e^{-(\mathbf{w}^T \mathbf{x} + b)}}
$$

**–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ:**
$$
\hat{y} = \begin{cases} 1, & \text{–µ—Å–ª–∏ } P(y=1|\mathbf{x}) \geq 0.5 \\ 0, & \text{–∏–Ω–∞—á–µ} \end{cases}
$$

**–§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å: Log Loss (Binary Cross-Entropy)**
$$
L(\mathbf{w}) = -\frac{1}{n} \sum_{i=1}^{n} \left[ y_i \log(p_i) + (1 - y_i) \log(1 - p_i) \right]
$$

–≥–¥–µ $p_i = \sigma(\mathbf{w}^T \mathbf{x}_i)$ ‚Äî –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å.

> [!TIP]
> **–ê–Ω–∞–ª–æ–≥–∏—è: –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –∫–∞–∫ "—Å—Ç–µ–ø–µ–Ω—å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏"**  
> –ü—Ä–µ–¥—Å—Ç–∞–≤—å—Ç–µ, —á—Ç–æ –≤—ã ‚Äî –≤—Ä–∞—á. –õ–∏–Ω–µ–π–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è –≤—ã–¥–∞–ª–∞ –±—ã –≤–∞–º —á–∏—Å–ª–æ "0.8" –Ω–∞ –≤–æ–ø—Ä–æ—Å –æ –±–æ–ª–µ–∑–Ω–∏. –°–∏–≥–º–æ–∏–¥–∞ –ø—Ä–µ–≤—Ä–∞—â–∞–µ—Ç —ç—Ç–æ "0.8" –≤ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å: "–Ø —É–≤–µ—Ä–µ–Ω –Ω–∞ 80%, —á—Ç–æ –ø–∞—Ü–∏–µ–Ω—Ç –±–æ–ª–µ–Ω". –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø—Ä–∏–Ω–∏–º–∞—Ç—å —Ä–µ—à–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∏—Å–∫–æ–≤ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –µ—Å–ª–∏ —Ü–µ–Ω–∞ –æ—à–∏–±–∫–∏ –≤–µ–ª–∏–∫–∞, –º—ã –Ω–∞–∑–Ω–∞—á–∞–µ–º –ª–µ—á–µ–Ω–∏–µ —É–∂–µ –ø—Ä–∏ 30% –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏).

**–ì—Ä–∞–¥–∏–µ–Ω—Ç:**
$$
\nabla L(\mathbf{w}) = \frac{1}{n} \mathbf{X}^T (\sigma(\mathbf{X}\mathbf{w}) - \mathbf{y})
$$

–£–¥–∏–≤–∏—Ç–µ–ª—å–Ω–æ –ø–æ—Ö–æ–∂ –Ω–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç OLS! –≠—Ç–æ –¥–µ–ª–∞–µ—Ç –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –±—ã—Å—Ç—Ä–æ–π –∏ —Å—Ç–∞–±–∏–ª—å–Ω–æ–π.

> [!WARNING]
> **Production Warning: –ö–∞–ª–∏–±—Ä–æ–≤–∫–∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π**  
> –õ–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è —Ö–æ—Ä–æ—à–æ –∫–∞–ª–∏–±—Ä—É–µ—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ "–∏–∑ –∫–æ—Ä–æ–±–∫–∏" (–µ—Å–ª–∏ –¥–∞–Ω–Ω—ã–µ –ª–∏–Ω–µ–π–Ω–æ —Ä–∞–∑–¥–µ–ª–∏–º—ã). –û–¥–Ω–∞–∫–æ –ø—Ä–∏ —Å–∏–ª—å–Ω–æ–º –¥–∏—Å–±–∞–ª–∞–Ω—Å–µ –∏–ª–∏ –µ—Å–ª–∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –Ω–µ–ª–∏–Ω–µ–π–Ω—ã, –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å 0.8 –º–æ–∂–µ—Ç –Ω–µ –æ–∑–Ω–∞—á–∞—Ç—å 80% —á–∞—Å—Ç–æ—Ç—É –Ω–∞ —Ç–µ—Å—Ç–µ. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ `CalibrationDisplay` –∏–∑ sklearn –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏.

### 4) Middle-level –Ω—é–∞–Ω—Å—ã

**–ü—Ä–æ–±–ª–µ–º–∞ 1: –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–≤**

**Odds (—à–∞–Ω—Å—ã):**
```
Odds = P(y=1) / P(y=0) = e^(w^T x)
```

**Odds Ratio –ø—Ä–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–∏ x_j –Ω–∞ 1:**
```
OR = e^(w_j)
```

**–ü—Ä–∏–º–µ—Ä:**
- w_–æ–ø—ã—Ç = 0.5
- OR = e^0.5 ‚âà 1.65
- **–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è**: –ø—Ä–∏ —É–≤–µ–ª–∏—á–µ–Ω–∏–∏ –æ–ø—ã—Ç–∞ –Ω–∞ 1 –≥–æ–¥ **—à–∞–Ω—Å—ã** –ø—Ä–∏–Ω—è—Ç–∏—è —É–≤–µ–ª–∏—á–∏–≤–∞—é—Ç—Å—è –≤ 1.65 —Ä–∞–∑–∞

**–ü—Ä–æ–±–ª–µ–º–∞ 2: –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è**

–ê–Ω–∞–ª–æ–≥–∏—á–Ω–æ –ª–∏–Ω–µ–π–Ω–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏:
- **L2 (Ridge)**: penalty = Œª||w||¬≤
- **L1 (Lasso)**: penalty = Œª||w||‚ÇÅ

–í sklearn:
```python
from sklearn.linear_model import LogisticRegression
# C = 1/Œª (—á–µ–º –º–µ–Ω—å—à–µ C, —Ç–µ–º —Å–∏–ª—å–Ω–µ–µ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è!)
model = LogisticRegression(penalty='l2', C=1.0)
```

**–ü—Ä–æ–±–ª–µ–º–∞ 3: –ù–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫–ª–∞—Å—Å—ã**

–ï—Å–ª–∏ –∫–ª–∞—Å—Å 0: 95%, –∫–ª–∞—Å—Å 1: 5%, –º–æ–¥–µ–ª—å –º–æ–∂–µ—Ç –Ω–∞—É—á–∏—Ç—å—Å—è –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å –≤—Å–µ–≥–¥–∞ 0 (accuracy = 95%!).

**–†–µ—à–µ–Ω–∏—è:**

1. **Class weights:**
```python
LogisticRegression(class_weight='balanced')
```
–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏: weight_class_i = n / (2 ¬∑ n_class_i)

2. **Resampling:**
- Oversampling –º–µ–Ω—å—à–∏–Ω—Å—Ç–≤–∞ (SMOTE)
- Undersampling –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–∞

3. **Adjust threshold:**
```python
# –í–º–µ—Å—Ç–æ 0.5 –∏—Å–ø–æ–ª—å–∑—É–µ–º 0.2 –¥–ª—è –∫–ª–∞—Å—Å–∞ 1
y_pred = (proba[:, 1] > 0.2).astype(int)
```

**–ü—Ä–æ–±–ª–µ–º–∞ 4: Decision Boundary**

–ì—Ä–∞–Ω–∏—Ü–∞ —Ä–µ—à–µ–Ω–∏–π: œÉ(w^T x + b) = 0.5
‚ü∫ w^T x + b = 0 (–≥–∏–ø–µ—Ä–ø–ª–æ—Å–∫–æ—Å—Ç—å!)

**–õ–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è = –ª–∏–Ω–µ–π–Ω—ã–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä**.

–î–ª—è –Ω–µ–ª–∏–Ω–µ–π–Ω—ã—Ö –≥—Ä–∞–Ω–∏—Ü:
- Polynomial features: x, x¬≤, x¬≥, x‚ÇÅx‚ÇÇ, ...
- Kernel trick (–Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è –Ω–∞–ø—Ä—è–º—É—é –≤ LogReg, –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ SVM)

### 5) –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞–º–∏

| –ú–µ—Ç–æ–¥ | –í—ã—Ö–æ–¥ | –ì—Ä–∞–Ω–∏—Ü–∞ —Ä–µ—à–µ–Ω–∏–π | –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ | –û–±—É—á–µ–Ω–∏–µ | –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å |
|-------|-------|----------------|-------------|----------|-------------------|
| **Logistic Regression** | –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å | –õ–∏–Ω–µ–π–Ω–∞—è | ‚úÖ –ö–∞–ª–∏–±—Ä–æ–≤–∫–∞ —Ö–æ—Ä–æ—à–∞—è | –ë—ã—Å—Ç—Ä–æ (gradient descent) | –õ–∏–Ω–µ–π–Ω–æ —Ä–∞–∑–¥–µ–ª–∏–º—ã–µ, –Ω—É–∂–Ω—ã –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ |
| **SVM (linear)** | –†–∞—Å—Å—Ç–æ—è–Ω–∏–µ –¥–æ –≥—Ä–∞–Ω–∏—Ü—ã | –õ–∏–Ω–µ–π–Ω–∞—è | ‚ö†Ô∏è –ù–µ–∫–∞–ª–∏–±—Ä–æ–≤–∞–Ω–Ω—ã–µ | –ú–µ–¥–ª–µ–Ω–Ω–µ–µ (QP solver) | –ù—É–∂–Ω–∞ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è margin, –≤—ã–±—Ä–æ—Å—ã |
| **Naive Bayes** | –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å | –ù–µ–ª–∏–Ω–µ–π–Ω–∞—è (–µ—Å–ª–∏ –ø—Ä–∏–∑–Ω–∞–∫–∏ –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã) | ‚ö†Ô∏è –°–ª–∏—à–∫–æ–º —É–≤–µ—Ä–µ–Ω–Ω—ã–µ | –û—á–µ–Ω—å –±—ã—Å—Ç—Ä–æ | –ú–∞–ª—ã–µ –¥–∞–Ω–Ω—ã–µ, text classification |
| **Random Forest** | –ì–æ–ª–æ—Å–æ–≤–∞–Ω–∏–µ –¥–µ—Ä–µ–≤—å–µ–≤ | –ù–µ–ª–∏–Ω–µ–π–Ω–∞—è | ‚ö†Ô∏è –ù–µ–∫–∞–ª–∏–±—Ä–æ–≤–∞–Ω–Ω—ã–µ | –ú–µ–¥–ª–µ–Ω–Ω–µ–µ | –ù–µ–ª–∏–Ω–µ–π–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏, feature importance |

---

## 2.3 Support Vector Machines (SVM)

### 1) –ò–Ω—Ç—É–∏—Ü–∏—è –∏ —Å—É—Ç—å

**–ü—Ä–æ—Å—Ç—ã–º–∏ —Å–ª–æ–≤–∞–º–∏ (ELI5):**  
–ü—Ä–µ–¥—Å—Ç–∞–≤—å, —á—Ç–æ —É —Ç–µ–±—è –µ—Å—Ç—å —è–±–ª–æ–∫–∏ –∏ –∞–ø–µ–ª—å—Å–∏–Ω—ã –Ω–∞ —Å—Ç–æ–ª–µ, –∏ –Ω—É–∂–Ω–æ –ø—Ä–æ–≤–µ—Å—Ç–∏ –º–µ–∂–¥—É –Ω–∏–º–∏ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å–Ω—É—é –ª–∏–Ω–∏—é. SVM –Ω–µ –ø—Ä–æ—Å—Ç–æ –ø—Ä–æ–≤–æ–¥–∏—Ç –ª—é–±—É—é –ª–∏–Ω–∏—é, –∞ –∏—â–µ—Ç —Ç–∞–∫—É—é, –∫–æ—Ç–æ—Ä–∞—è **–º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –¥–∞–ª–µ–∫–æ** –æ—Ç –±–ª–∏–∂–∞–π—à–∏—Ö —è–±–ª–æ–∫ –∏ –∞–ø–µ–ª—å—Å–∏–Ω–æ–≤. –≠—Ç–∏ –±–ª–∏–∂–∞–π—à–∏–µ —Ñ—Ä—É–∫—Ç—ã ‚Äî "support vectors" (–æ–ø–æ—Ä–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä—ã), –æ–Ω–∏ –∏ –æ–ø—Ä–µ–¥–µ–ª—è—é—Ç –ª–∏–Ω–∏—é.

**–ê–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏:**  
SVM ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏, –∫–æ—Ç–æ—Ä—ã–π —Å—Ç—Ä–æ–∏—Ç **–æ–ø—Ç–∏–º–∞–ª—å–Ω—É—é —Ä–∞–∑–¥–µ–ª—è—é—â—É—é –≥–∏–ø–µ—Ä–ø–ª–æ—Å–∫–æ—Å—Ç—å** —Å **–º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º –æ—Ç—Å—Ç—É–ø–æ–º (margin)** –º–µ–∂–¥—É –∫–ª–∞—Å—Å–∞–º–∏. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç **—è–¥–µ—Ä–Ω—ã–π —Ç—Ä—é–∫ (kernel trick)** –¥–ª—è –Ω–µ–ª–∏–Ω–µ–π–Ω—ã—Ö –≥—Ä–∞–Ω–∏—Ü.

**–§–∏–∑–∏—á–µ—Å–∫–∏–π —Å–º—ã—Å–ª:**  
–ò–Ω—Ç—É–∏—Ü–∏—è: —á–µ–º –±–æ–ª—å—à–µ margin, —Ç–µ–º –±–æ–ª–µ–µ "—É–≤–µ—Ä–µ–Ω–Ω–æ" —Ä–∞–∑–¥–µ–ª–µ–Ω—ã –∫–ª–∞—Å—Å—ã ‚Üí –ª—É—á—à–µ –æ–±–æ–±—â–µ–Ω–∏–µ –Ω–∞ –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ.

### 2) –ó–∞—á–µ–º –∏ –∫–æ–≥–¥–∞ (Business Value)

**–ö–∞–∫—É—é –ø—Ä–æ–±–ª–µ–º—É —Ä–µ—à–∞–µ—Ç:**
- –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Å **–º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π margin** (–ª—É—á—à–µ –æ–±–æ–±—â–µ–Ω–∏–µ)
- **–ù–µ–ª–∏–Ω–µ–π–Ω—ã–µ –≥—Ä–∞–Ω–∏—Ü—ã** —á–µ—Ä–µ–∑ kernel trick (–±–µ–∑ —è–≤–Ω–æ–≥–æ —Å–æ–∑–¥–∞–Ω–∏—è polynomial features!)
- –£—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∫ **–≤—ã–±—Ä–æ—Å–∞–º** (—Ç–æ–ª—å–∫–æ support vectors –≤–ª–∏—è—é—Ç –Ω–∞ —Ä–µ—à–µ–Ω–∏–µ)

**–ì–¥–µ —ç—Ç–æ –∏–Ω–¥—É—Å—Ç—Ä–∏–∞–ª—å–Ω—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç:**
- **Bioinformatics**: –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –±–µ–ª–∫–æ–≤, –≥–µ–Ω–æ–≤
- **Computer Vision**: —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –æ–±—Ä–∞–∑–æ–≤ (–¥–æ —ç—Ä—ã deep learning)
- **Text classification**: spam detection (—Å linear kernel)
- **–§–∏–Ω–∞–Ω—Å—ã**: fraud detection

**–ö–æ–≥–¥–∞ –ø—Ä–∏–º–µ–Ω—è—Ç—å –Ω–µ–ª—å–∑—è:**
- ‚ùå –û—á–µ–Ω—å –±–æ–ª—å—à–∏–µ –¥–∞–Ω–Ω—ã–µ (O(n¬≥) –¥–ª—è –æ–±—É—á–µ–Ω–∏—è ‚Üí –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ linear SVM –∏–ª–∏ LogReg)
- ‚ùå –ù—É–∂–Ω—ã –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ (SVM –¥–∞–µ—Ç —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è, –Ω–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏; –∫–∞–ª–∏–±—Ä–æ–≤–∫–∞ –Ω—É–∂–Ω–∞)
- ‚ùå –ú–Ω–æ–≥–æ —à—É–º–∞ –≤ –¥–∞–Ω–Ω—ã—Ö (SVM —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª–µ–Ω –∫ –≤—ã–±—Ä–æ—Å–∞–º –±–µ–∑ soft margin)

### 3) –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —è–¥—Ä–æ

**Hard Margin SVM (–ª–∏–Ω–µ–π–Ω–æ —Ä–∞–∑–¥–µ–ª–∏–º—ã–µ –¥–∞–Ω–Ω—ã–µ):**

**–¶–µ–ª—å:**  
–ù–∞–π—Ç–∏ –≥–∏–ø–µ—Ä–ø–ª–æ—Å–∫–æ—Å—Ç—å $\mathbf{w}^T \mathbf{x} + b = 0$ —Å **–º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º margin**.

**Margin:**
$$
\text{margin} = \frac{2}{\|\mathbf{w}\|}
$$

**–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–æ–Ω–Ω–∞—è –∑–∞–¥–∞—á–∞:**
$$
\min_{\mathbf{w}, b} \frac{1}{2}\|\mathbf{w}\|^2
$$
$$
s.t. \quad y_i(\mathbf{w}^T \mathbf{x}_i + b) \geq 1, \quad \forall i
$$

**–°–º—ã—Å–ª constraint:**  
–í—Å–µ —Ç–æ—á–∫–∏ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å **–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω—ã** —Å –æ—Ç—Å—Ç—É–ø–æ–º $\geq 1$.

**–†–µ—à–µ–Ω–∏–µ:**  
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è **Lagrange multipliers** (–¥–≤–æ–π—Å—Ç–≤–µ–Ω–Ω–∞—è –∑–∞–¥–∞—á–∞):
$$
\max_{\boldsymbol{\alpha}} \sum_{i=1}^{n} \alpha_i - \frac{1}{2} \sum_{i,j=1}^{n} \alpha_i \alpha_j y_i y_j \mathbf{x}_i^T \mathbf{x}_j
$$
$$
s.t. \quad \alpha_i \geq 0, \quad \sum \alpha_i y_i = 0
$$

**Support Vectors:**  
–¢–æ—á–∫–∏ —Å $\alpha_i > 0$. –û—Å—Ç–∞–ª—å–Ω—ã–µ —Ç–æ—á–∫–∏ –Ω–µ –≤–ª–∏—è—é—Ç –Ω–∞ —Ä–µ—à–µ–Ω–∏–µ!

**Soft Margin SVM (–¥–ª—è –Ω–µ—Ä–∞–∑–¥–µ–ª–∏–º—ã—Ö –¥–∞–Ω–Ω—ã—Ö):**

–î–æ–±–∞–≤–ª—è–µ–º **slack variables $\xi_i$** (–¥–æ–ø—É—Å–∫–∞–µ–º –æ—à–∏–±–∫–∏):
$$
\min_{\mathbf{w}, b, \boldsymbol{\xi}} \frac{1}{2}\|\mathbf{w}\|^2 + C \sum_{i=1}^{n} \xi_i
$$
$$
s.t. \quad y_i(\mathbf{w}^T \mathbf{x}_i + b) \geq 1 - \xi_i, \quad \xi_i \geq 0
$$

> [!IMPORTANT]
> **C (–≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä):**
> - **C –≤–µ–ª–∏–∫**: "–®—Ç—Ä–∞—Ñ –∑–∞ –æ—à–∏–±–∫—É –≤–µ–ª–∏–∫". –ñ–∞–¥–Ω–æ —Å—Ç–∞—Ä–∞–µ–º—Å—è –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å –≤—Å—ë –ø—Ä–∞–≤–∏–ª—å–Ω–æ ‚Üí —É–∑–∫–∏–π margin ‚Üí —Ä–∏—Å–∫ **overfitting**.
> - **C –º–∞–ª**: "–î–æ–ø—É—Å–∫–∞–µ–º –æ—à–∏–±–∫–∏ —Ä–∞–¥–∏ —à–∏—Ä–∏–Ω—ã –ø–æ–ª–æ—Å—ã". –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –∑–∞—à—É–º–ª–µ–Ω–Ω—ã–µ —Ç–æ—á–∫–∏ ‚Üí —à–∏—Ä–æ–∫–∏–π margin ‚Üí —Ä–∏—Å–∫ **underfitting**.

**Kernel Trick (–Ω–µ–ª–∏–Ω–µ–π–Ω—ã–µ –≥—Ä–∞–Ω–∏—Ü—ã):**

**–ò–¥–µ—è:**  
–û—Ç–æ–±—Ä–∞–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –≤ **–≤—ã—Å–æ–∫–æ—Ä–∞–∑–º–µ—Ä–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ** $\phi(\mathbf{x})$, –≥–¥–µ –æ–Ω–∏ –ª–∏–Ω–µ–π–Ω–æ —Ä–∞–∑–¥–µ–ª–∏–º—ã. –ó–∞–º–µ–Ω–∏—Ç—å $\mathbf{x}_i^T \mathbf{x}_j$ –Ω–∞ $K(\mathbf{x}_i, \mathbf{x}_j) = \phi(\mathbf{x}_i)^T \phi(\mathbf{x}_j)$ (kernel function).

**–ü–æ–ø—É–ª—è—Ä–Ω—ã–µ —è–¥—Ä–∞:**
1. **Linear:** $K(\mathbf{x}, \mathbf{x}') = \mathbf{x}^T \mathbf{x}'$
2. **Polynomial:** $K(\mathbf{x}, \mathbf{x}') = (\gamma \mathbf{x}^T \mathbf{x}' + c)^d$
3. **RBF (Radial Basis Function):**
$$
K(\mathbf{x}, \mathbf{x}') = \exp(-\gamma\|\mathbf{x} - \mathbf{x}'\|^2)
$$

**–†–µ—à–∞—é—â–∞—è —Ñ—É–Ω–∫—Ü–∏—è:**
$$
f(\mathbf{x}) = \text{sign}\left(\sum \alpha_i y_i K(\mathbf{x}_i, \mathbf{x}) + b\right)
$$

> [!TIP]
> **–ê–Ω–∞–ª–æ–≥–∏—è: SVM –∫–∞–∫ "–î–µ–º–∏–ª–∏—Ç–∞—Ä–∏–∑–æ–≤–∞–Ω–Ω–∞—è –∑–æ–Ω–∞"**  
> –ü—Ä–µ–¥—Å—Ç–∞–≤—å—Ç–µ, —á—Ç–æ SVM ‚Äî —ç—Ç–æ –≥—Ä–∞–Ω–∏—Ü–∞ –º–µ–∂–¥—É –¥–≤—É–º—è —Å—Ç—Ä–∞–Ω–∞–º–∏. –ú—ã —Ö–æ—Ç–∏–º —Å–¥–µ–ª–∞—Ç—å —ç—Ç—É –≥—Ä–∞–Ω–∏—Ü—É (DMZ) –∫–∞–∫ –º–æ–∂–Ω–æ —à–∏—Ä–µ, —á—Ç–æ–±—ã –∫–æ–Ω—Ñ–ª–∏–∫—Ç—ã (–æ—à–∏–±–∫–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏) –±—ã–ª–∏ –º–µ–Ω–µ–µ –≤–µ—Ä–æ—è—Ç–Ω—ã. –¢–æ—á–∫–∏-–Ω–∞—Ä—É—à–∏—Ç–µ–ª–∏ (–æ–ø–æ—Ä–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä–∞) ‚Äî —ç—Ç–æ –ø–æ–≥—Ä–∞–Ω–∏—á–Ω—ã–µ –ø–æ—Å—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ —Å—Ç–æ—è—Ç –±–ª–∏–∂–µ –≤—Å–µ–≥–æ –∫ DMZ –∏ —É–¥–µ—Ä–∂–∏–≤–∞—é—Ç –µ—ë. –û—Å—Ç–∞–ª—å–Ω—ã–µ –∂–∏—Ç–µ–ª–∏ –≤–Ω—É—Ç—Ä–∏ —Å—Ç—Ä–∞–Ω –Ω–∞ —à–∏—Ä–∏–Ω—É –≥—Ä–∞–Ω–∏—Ü—ã –Ω–µ –≤–ª–∏—è—é—Ç.

### 4) Middle-level –Ω—é–∞–Ω—Å—ã

**–ü—Ä–æ–±–ª–µ–º–∞ 1: –í—ã–±–æ—Ä kernel –∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤**

| Kernel | –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã | –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å |
|--------|---------------|-------------------|
| **Linear** | C | –í—ã—Å–æ–∫–æ—Ä–∞–∑–º–µ—Ä–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (text, gene data), d >> n |
| **RBF** | C, Œ≥ | –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π, –Ω–µ–ª–∏–Ω–µ–π–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ |
| **Polynomial** | C, degree, Œ≥ | –ò–∑–≤–µ—Å—Ç–Ω–∞—è –ø–æ–ª–∏–Ω–æ–º–∏–∞–ª—å–Ω–∞—è –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å |

**–ü–æ–¥–±–æ—Ä:**
- GridSearchCV –ø–æ C –∏ Œ≥ (–¥–ª—è RBF)
- **–≠–º–ø–∏—Ä–∏—á–µ—Å–∫–∏–µ –ø—Ä–∞–≤–∏–ª–∞ –¥–ª—è RBF:**
  - Œ≥ = 1 / (n_features ¬∑ Var(X))
  - C ‚àà [0.1, 1, 10, 100]

**–ü—Ä–æ–±–ª–µ–º–∞ 2: RBF kernel ‚Äî —Ä–∏—Å–∫ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è**

**Œ≥ —Å–ª–∏—à–∫–æ–º –≤–µ–ª–∏–∫:**  
–ö–∞–∂–¥–∞—è —Ç–æ—á–∫–∞ ‚Äî "–æ—Å—Ç—Ä–æ–≤" ‚Üí –º–æ–¥–µ–ª—å –∑–∞–ø–æ–º–∏–Ω–∞–µ—Ç –æ–±—É—á–∞—é—â—É—é –≤—ã–±–æ—Ä–∫—É.

**Œ≥ —Å–ª–∏—à–∫–æ–º –º–∞–ª:**  
Kernel –ø–æ—á—Ç–∏ –ª–∏–Ω–µ–π–Ω—ã–π ‚Üí –Ω–µ–¥–æ–æ–±—É—á–µ–Ω–∏–µ.

**–ü—Ä–æ–±–ª–µ–º–∞ 3: –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–ö–†–ò–¢–ò–ß–ù–û –¥–ª—è RBF!)**

RBF kernel –∑–∞–≤–∏—Å–∏—Ç –æ—Ç **–µ–≤–∫–ª–∏–¥–æ–≤–∞ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è** ||x - x'||.  
–ü—Ä–∏–∑–Ω–∞–∫–∏ —Å –±–æ–ª—å—à–∏–º –º–∞—Å—à—Ç–∞–±–æ–º –¥–æ–º–∏–Ω–∏—Ä—É—é—Ç!

**–†–µ—à–µ–Ω–∏–µ:**  
–í–°–ï–ì–î–ê —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞—Ç—å:
```python
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
```

**–ü—Ä–æ–±–ª–µ–º–∞ 4: –í—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å**

- **–û–±—É—á–µ–Ω–∏–µ**: O(n¬≤ ¬∑ d) –¥–æ O(n¬≥) –¥–ª—è QP solver
- **–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ**: O(n_sv ¬∑ d), –≥–¥–µ n_sv ‚Äî —á–∏—Å–ª–æ support vectors

–î–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö:
- **LinearSVC** (liblinear library): –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω –¥–ª—è linear kernel
- **SGDClassifier** —Å loss='hinge': –∞–ø–ø—Ä–æ–∫—Å–∏–º–∞—Ü–∏—è SVM —á–µ—Ä–µ–∑ SGD

### 5) –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞–º–∏

| –ú–µ—Ç–æ–¥ | –ì—Ä–∞–Ω–∏—Ü–∞ | –£—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∫ –≤—ã–±—Ä–æ—Å–∞–º | –ù–µ–ª–∏–Ω–µ–π–Ω–æ—Å—Ç—å | –í—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å | –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ |
|-------|---------|------------------------|-------------|--------------------------|-------------|
| **SVM (RBF)** | –ù–µ–ª–∏–Ω–µ–π–Ω–∞—è | ‚ö†Ô∏è –°—Ä–µ–¥–Ω—è—è (–∑–∞–≤–∏—Å–∏—Ç –æ—Ç C) | ‚úÖ Kernel trick | ‚ùå O(n¬≤-n¬≥) | ‚ùå (–Ω—É–∂–Ω–∞ –∫–∞–ª–∏–±—Ä–æ–≤–∫–∞) |
| **Logistic Regression** | –õ–∏–Ω–µ–π–Ω–∞—è | ‚ö†Ô∏è –ß—É–≤—Å—Ç–≤–∏—Ç–µ–ª–µ–Ω –∫ –≤—ã–±—Ä–æ—Å–∞–º | ‚ùå (–±–µ–∑ polynomial features) | ‚úÖ O(n¬∑d) | ‚úÖ –ö–∞–ª–∏–±—Ä–æ–≤–∞–Ω–Ω—ã–µ |
| **Random Forest** | –ù–µ–ª–∏–Ω–µ–π–Ω–∞—è | ‚úÖ –£—Å—Ç–æ–π—á–∏–≤ (–∞–Ω—Å–∞–º–±–ª—å) | ‚úÖ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ | ‚ö†Ô∏è O(n¬∑log n ¬∑ d ¬∑ trees) | ‚ö†Ô∏è –ù–µ–∫–∞–ª–∏–±—Ä–æ–≤–∞–Ω–Ω—ã–µ |
| **k-NN** | –ù–µ–ª–∏–Ω–µ–π–Ω–∞—è | ‚ùå –û—á–µ–Ω—å —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª–µ–Ω | ‚úÖ –õ–æ–∫–∞–ª—å–Ω–æ—Å—Ç—å | ‚ùå O(n¬∑d) –Ω–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ | ‚ö†Ô∏è –õ–æ–∫–∞–ª—å–Ω—ã–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ |

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:**
- **–ú–∞–ª—ã–µ/—Å—Ä–µ–¥–Ω–∏–µ –¥–∞–Ω–Ω—ã–µ, –Ω–µ–ª–∏–Ω–µ–π–Ω–∞—è –∑–∞–¥–∞—á–∞**: SVM —Å RBF
- **–í—ã—Å–æ–∫–æ—Ä–∞–∑–º–µ—Ä–Ω—ã–µ, —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–µ (text)**: SVM —Å linear kernel
- **–ù—É–∂–Ω—ã –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏, –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è**: Logistic Regression
- **–ë–æ–ª—å—à–∏–µ –¥–∞–Ω–Ω—ã–µ, –Ω–µ–ª–∏–Ω–µ–π–Ω–æ—Å—Ç—å**: Random Forest, XGBoost

---

## 2.4 Maximum Likelihood Estimation (MLE)

### 1) –ò–Ω—Ç—É–∏—Ü–∏—è –∏ —Å—É—Ç—å

**–ü—Ä–æ—Å—Ç—ã–º–∏ —Å–ª–æ–≤–∞–º–∏ (ELI5):**  
–£ —Ç–µ–±—è –µ—Å—Ç—å –º–æ–Ω–µ—Ç–∫–∞, –∏ —Ç—ã –ø–æ–¥–±—Ä–æ—Å–∏–ª –µ–µ 10 —Ä–∞–∑: 7 –æ—Ä–ª–æ–≤, 3 —Ä–µ—à–∫–∏. –ö–∞–∫–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –≤—ã–ø–∞–¥–µ–Ω–∏—è –æ—Ä–ª–∞? MLE –≥–æ–≤–æ—Ä–∏—Ç: "–≤—ã–±–µ—Ä–∏ —Ç–∞–∫—É—é –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å, –ø—Ä–∏ –∫–æ—Ç–æ—Ä–æ–π –Ω–∞–±–ª—é–¥–∞–µ–º—ã–µ –¥–∞–Ω–Ω—ã–µ (7 –æ—Ä–ª–æ–≤, 3 —Ä–µ—à–∫–∏) –±—ã–ª–∏ –±—ã **–Ω–∞–∏–±–æ–ª–µ–µ –≤–µ—Ä–æ—è—Ç–Ω—ã**". –û—Ç–≤–µ—Ç: p = 7/10 = 0.7.

**–ê–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏:**  
MLE ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥ –æ—Ü–µ–Ω–∫–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–æ–π –º–æ–¥–µ–ª–∏ –ø—É—Ç–µ–º –º–∞–∫—Å–∏–º–∏–∑–∞—Ü–∏–∏ **—Ñ—É–Ω–∫—Ü–∏–∏ –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–∏—è (likelihood function)** ‚Äî –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –Ω–∞–±–ª—é–¥–∞–µ–º—ã—Ö –¥–∞–Ω–Ω—ã—Ö –ø—Ä–∏ –¥–∞–Ω–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–∞—Ö.

**–§–∏–∑–∏—á–µ—Å–∫–∏–π —Å–º—ã—Å–ª:**  
–ú—ã –∏—â–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏, –∫–æ—Ç–æ—Ä—ã–µ "–ª—É—á—à–µ –≤—Å–µ–≥–æ –æ–±—ä—è—Å–Ω—è—é—Ç" –Ω–∞–±–ª—é–¥–∞–µ–º—ã–µ –¥–∞–Ω–Ω—ã–µ.

### 2) –ó–∞—á–µ–º –∏ –∫–æ–≥–¥–∞ (Business Value)

**–ö–∞–∫—É—é –ø—Ä–æ–±–ª–µ–º—É —Ä–µ—à–∞–µ—Ç:**
- **–û—Ü–µ–Ω–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤** –ª—é–±—ã—Ö –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
- –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π framework –¥–ª—è **—Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ –≤—ã–≤–æ–¥–∞**
- **–¢–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–æ–µ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ** –º–Ω–æ–≥–∏—Ö ML –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤

**–ì–¥–µ —ç—Ç–æ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç:**
- **Linear Regression** (MLE —Å Gaussian noise = OLS!)
- **Logistic Regression** (MLE —Å Bernoulli —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º = Log Loss)
- **Naive Bayes** (MLE –¥–ª—è –æ—Ü–µ–Ω–∫–∏ P(X|Y))
- **GMM (Gaussian Mixture Models)** (EM algorithm = –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω—ã–π MLE)

**–°–≤—è–∑—å —Å ML:**  
–ú–∏–Ω–∏–º–∏–∑–∞—Ü–∏—è —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å —á–∞—Å—Ç–æ —ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç–Ω–∞ MLE –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è!

### 3) –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —è–¥—Ä–æ

**Likelihood Function:**

–ü—É—Å—Ç—å –¥–∞–Ω–Ω—ã–µ $\{x_1, \dots, x_n\}$ –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã –∏ –æ–¥–∏–Ω–∞–∫–æ–≤–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω—ã (i.i.d.) —Å –ø–ª–æ—Ç–Ω–æ—Å—Ç—å—é $p(x|\theta)$.

**Likelihood:**
$$
L(\theta | x_1, \dots, x_n) = \prod_{i=1}^{n} p(x_i | \theta)
$$

**Log-Likelihood (—É–¥–æ–±–Ω–µ–µ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏):**
$$
\ell(\theta) = \log L(\theta) = \sum_{i=1}^{n} \log p(x_i | \theta)
$$

**MLE:**
$$
\hat{\theta}_{MLE} = \arg\max_{\theta} \ell(\theta)
$$

**–ü—Ä–∏–º–µ—Ä 1: OLS –∫–∞–∫ MLE**

**–ü—Ä–µ–¥–ø–æ–ª–æ–∂–µ–Ω–∏–µ:**  
$y_i = \mathbf{w}^T \mathbf{x}_i + \epsilon_i$,  –≥–¥–µ $\epsilon_i \sim \mathcal{N}(0, \sigma^2)$

**Likelihood:**
$$
p(y_i | \mathbf{x}_i, \mathbf{w}) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(y_i - \mathbf{w}^T \mathbf{x}_i)^2}{2\sigma^2}\right)
$$

**Log-Likelihood:**
$$
\ell(\mathbf{w}) = \sum \left[-\frac{(y_i - \mathbf{w}^T \mathbf{x}_i)^2}{2\sigma^2} - \log(\sqrt{2\pi\sigma^2})\right] \propto -\sum (y_i - \mathbf{w}^T \mathbf{x}_i)^2
$$

**MLE:**
$$
\hat{\mathbf{w}}_{MLE} = \arg\max_{\mathbf{w}} \ell(\mathbf{w}) = \arg\min_{\mathbf{w}} \sum (y_i - \mathbf{w}^T \mathbf{x}_i)^2
$$

**–≠—Ç–æ MSE!** OLS = MLE –¥–ª—è Gaussian noise.

**–ü—Ä–∏–º–µ—Ä 2: Logistic Regression –∫–∞–∫ MLE**

**–ü—Ä–µ–¥–ø–æ–ª–æ–∂–µ–Ω–∏–µ:**  
$y_i \sim \text{Bernoulli}(p_i)$,  –≥–¥–µ $p_i = \sigma(\mathbf{w}^T \mathbf{x}_i)$

**Likelihood:**
$$
p(y_i | \mathbf{x}_i, \mathbf{w}) = p_i^{y_i} \cdot (1 - p_i)^{1 - y_i}
$$

**Log-Likelihood:**
$$
\ell(\mathbf{w}) = \sum [y_i \log(p_i) + (1 - y_i) \log(1 - p_i)]
$$

**MLE:**
$$
\hat{\mathbf{w}}_{MLE} = \arg\max_{\mathbf{w}} \ell(\mathbf{w})
$$

**–≠—Ç–æ –º–∏–Ω–∏–º–∏–∑–∞—Ü–∏—è Log Loss!**

### 4) Middle-level –Ω—é–∞–Ω—Å—ã

**–ü—Ä–æ–±–ª–µ–º–∞ 1: MLE –º–æ–∂–µ—Ç –ø–µ—Ä–µ–æ–±—É—á–∞—Ç—å—Å—è**

**–ü—Ä–∏–º–µ—Ä:**  
–ü–æ–¥–±—Ä–æ—Å–∏–ª–∏ –º–æ–Ω–µ—Ç–∫—É 3 —Ä–∞–∑–∞, –≤—Å–µ –æ—Ä–ª—ã.  
MLE: pÃÇ = 3/3 = 1.0 (—É–≤–µ—Ä–µ–Ω—ã, —á—Ç–æ –º–æ–Ω–µ—Ç–∞ –≤—Å–µ–≥–¥–∞ –æ—Ä–µ–ª!)

–≠—Ç–æ **overfitting**. –ú–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö ‚Üí MLE –Ω–µ—É—Å—Ç–æ–π—á–∏–≤–∞.

**–†–µ—à–µ–Ω–∏–µ:**
- **MAP (Maximum A Posteriori)**: –¥–æ–±–∞–≤–ª—è–µ–º prior (–±–∞–π–µ—Å–æ–≤—Å–∫–∏–π –ø–æ–¥—Ö–æ–¥):
  ```
  Œ∏_MAP = argmax [log p(data|Œ∏) + log p(Œ∏)]
          Œ∏
  ```
- **–†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è –≤ ML = prior –≤ MLE!**
  - Ridge (L2) ‚ü∫ Gaussian prior: p(w) ‚àù exp(-Œª||w||¬≤)
  - Lasso (L1) ‚ü∫ Laplace prior: p(w) ‚àù exp(-Œª||w||‚ÇÅ)

**–ü—Ä–æ–±–ª–µ–º–∞ 2: –ß–∏—Å–ª–µ–Ω–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è**

–û–±—ã—á–Ω–æ –Ω–µ—Ç –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è ‚Üí –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ –º–µ—Ç–æ–¥—ã:
- **Gradient Ascent**: Œ∏^(t+1) = Œ∏^(t) + Œ± ¬∑ ‚àá‚Ñì(Œ∏^(t))
- **Newton-Raphson** (–≤—Ç–æ—Ä–æ–π –ø–æ—Ä—è–¥–æ–∫)
- **EM algorithm** –¥–ª—è —Å–∫—Ä—ã—Ç—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö (GMM, HMM)

**–ü—Ä–æ–±–ª–µ–º–∞ 3: –°–≤–æ–π—Å—Ç–≤–∞ MLE**

**–ü–ª—é—Å—ã (–∞—Å–∏–º–ø—Ç–æ—Ç–∏—á–µ—Å–∫–∏ –ø—Ä–∏ n ‚Üí ‚àû):**
- **Consistency**: Œ∏ÃÇ_MLE ‚Üí Œ∏_true
- **Asymptotic Normality**: ‚àön(Œ∏ÃÇ_MLE - Œ∏_true) ‚Üí N(0, I^(-1)), –≥–¥–µ I ‚Äî Fisher Information
- **Efficiency**: –º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –∞—Å–∏–º–ø—Ç–æ—Ç–∏—á–µ—Å–∫–∞—è –¥–∏—Å–ø–µ—Ä—Å–∏—è —Å—Ä–µ–¥–∏ –Ω–µ—Å–º–µ—â–µ–Ω–Ω—ã—Ö –æ—Ü–µ–Ω–æ–∫

**–ú–∏–Ω—É—Å—ã:**
- –ú–æ–∂–µ—Ç –±—ã—Ç—å **—Å–º–µ—â–µ–Ω–Ω–æ–π** –ø—Ä–∏ –º–∞–ª—ã—Ö n
- –¢—Ä–µ–±—É–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ —É–∫–∞–∑–∞–Ω–∏—è **—Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è**

### 5) –°—Ä–∞–≤–Ω–µ–Ω–∏–µ MLE vs MAP vs Bayesian

| –ü–æ–¥—Ö–æ–¥ | –§–æ—Ä–º—É–ª–∞ | Prior | –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å |
|--------|---------|-------|-------------------|
| **MLE** | Œ∏ = argmax p(data\|Œ∏) | ‚ùå –ù–µ—Ç | –ú–Ω–æ–≥–æ –¥–∞–Ω–Ω—ã—Ö, –Ω–µ—Ç –∞–ø—Ä–∏–æ—Ä–Ω—ã—Ö –∑–Ω–∞–Ω–∏–π |
| **MAP** | Œ∏ = argmax [p(data\|Œ∏) ¬∑ p(Œ∏)] | ‚úÖ –û–¥–∏–Ω prior | –°—Ä–µ–¥–Ω–∏–µ –¥–∞–Ω–Ω—ã–µ, –µ—Å—Ç—å –ø—Ä–µ–¥–ø–æ–ª–æ–∂–µ–Ω–∏—è |
| **Full Bayesian** | p(Œ∏\|data) ‚àù p(data\|Œ∏) ¬∑ p(Œ∏) | ‚úÖ –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ Œ∏ | –ú–∞–ª—ã–µ –¥–∞–Ω–Ω—ã–µ, –Ω—É–∂–Ω–∞ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç—å |

**–°–≤—è–∑—å —Å —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–µ–π:**
```
MLE + L2 regularization = MAP —Å Gaussian prior
MLE + L1 regularization = MAP —Å Laplace prior
```

---

## üìù –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –∫–æ–¥ (sklearn)

### Ridge/Lasso/ElasticNet —Å hyperparameters

```python
from sklearn.linear_model import Ridge, Lasso, ElasticNet, RidgeCV, LassoCV
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# –ì–ò–ü–ï–†–ü–ê–†–ê–ú–ï–¢–†–´ –†–ï–ì–£–õ–Ø–†–ò–ó–ê–¶–ò–ò
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# alpha (Œª): —Å–∏–ª–∞ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏
#   - 0: –Ω–µ—Ç —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏ (OLS)
#   - 0.01-0.1: —Å–ª–∞–±–∞—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
#   - 1-10: —É–º–µ—Ä–µ–Ω–Ω–∞—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
#   - 100+: —Å–∏–ª—å–Ω–∞—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è (–∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã ‚Üí 0)
#
# l1_ratio (–¥–ª—è ElasticNet): –±–∞–ª–∞–Ω—Å L1/L2
#   - 0: —á–∏—Å—Ç—ã–π Ridge (L2)
#   - 1: —á–∏—Å—Ç—ã–π Lasso (L1)
#   - 0.5: —Ä–∞–≤–Ω—ã–π –±–∞–ª–∞–Ω—Å (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –Ω–∞—á–∞—Ç—å)
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

# –í–ê–ñ–ù–û: –í–°–ï–ì–î–ê –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞—Ç—å –¥–ª—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏!
pipeline_ridge = Pipeline([
    ('scaler', StandardScaler()),
    ('ridge', Ridge(alpha=1.0))
])

# –ê–≤—Ç–æ–ø–æ–¥–±–æ—Ä alpha —á–µ—Ä–µ–∑ CV
ridge_cv = RidgeCV(alphas=[0.01, 0.1, 1, 10, 100], cv=5)
ridge_cv.fit(X_train_scaled, y_train)
print(f"–õ—É—á—à–∏–π alpha: {ridge_cv.alpha_}")

# Lasso: alpha + max_iter (–º–æ–∂–µ—Ç –Ω–µ —Å–æ–π—Ç–∏—Å—å!)
lasso = Lasso(alpha=0.1, max_iter=10000, tol=1e-4)

# ElasticNet: –∫–æ–º–±–∏–Ω–∞—Ü–∏—è L1+L2
elastic = ElasticNet(alpha=0.1, l1_ratio=0.5, max_iter=10000)
```

### Logistic Regression —Å –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏

```python
from sklearn.linear_model import LogisticRegression

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# –ì–ò–ü–ï–†–ü–ê–†–ê–ú–ï–¢–†–´ LOGISTIC REGRESSION
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# C = 1/Œª (–û–ë–†–ê–¢–ù–ê–Ø —Å–∏–ª–∞ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏!)
#   - C=100: —Å–ª–∞–±–∞—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
#   - C=1: —É–º–µ—Ä–µ–Ω–Ω–∞—è (default)
#   - C=0.01: —Å–∏–ª—å–Ω–∞—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
#
# penalty: —Ç–∏–ø —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏
#   - 'l2': Ridge (default)
#   - 'l1': Lasso (—Ç–æ–ª—å–∫–æ —Å solver='saga' –∏–ª–∏ 'liblinear')
#   - 'elasticnet': –∫–æ–º–±–∏–Ω–∞—Ü–∏—è (—Ç–æ–ª—å–∫–æ solver='saga')
#   - 'none': –±–µ–∑ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏
#
# class_weight: –¥–ª—è –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞ –∫–ª–∞—Å—Å–æ–≤
#   - None: –≤—Å–µ –∫–ª–∞—Å—Å—ã —Ä–∞–≤–Ω—ã
#   - 'balanced': –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –≤–µ—Å–∞ ‚àù 1/—á–∞—Å—Ç–æ—Ç–∞_–∫–ª–∞—Å—Å–∞
#
# solver: –∞–ª–≥–æ—Ä–∏—Ç–º –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
#   - 'lbfgs': default, –±—ã—Å—Ç—Ä—ã–π –¥–ª—è L2
#   - 'saga': –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –≤—Å–µ penalty, —Ö–æ—Ä–æ—à –¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö
#   - 'liblinear': —Ö–æ—Ä–æ—à –¥–ª—è –º–∞–ª—ã—Ö –¥–∞–Ω–Ω—ã—Ö
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

# –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Å–ª—É—á–∞–π
logreg = LogisticRegression(C=1.0, penalty='l2', solver='lbfgs', max_iter=1000)

# –î–∏—Å–±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤
logreg_balanced = LogisticRegression(class_weight='balanced', C=0.5)

# L1 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è (feature selection)
logreg_l1 = LogisticRegression(penalty='l1', solver='saga', C=0.1, max_iter=5000)

# ElasticNet
logreg_elastic = LogisticRegression(
    penalty='elasticnet', solver='saga', 
    l1_ratio=0.5, C=0.5, max_iter=5000
)
```

### SVM —Å kernel trick

```python
from sklearn.svm import SVC, LinearSVC
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import GridSearchCV

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# –ì–ò–ü–ï–†–ü–ê–†–ê–ú–ï–¢–†–´ SVM
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# C: trade-off margin vs –æ—à–∏–±–∫–∏
#   - –ú–∞–ª—ã–π C (0.01): —à–∏—Ä–æ–∫–∏–π margin, –¥–æ–ø—É—Å–∫–∞–µ–º –æ—à–∏–±–∫–∏
#   - –ë–æ–ª—å—à–æ–π C (100): —É–∑–∫–∏–π margin, –º–∏–Ω–∏–º—É–º –æ—à–∏–±–æ–∫ (overfitting!)
#
# kernel: —Ç–∏–ø —è–¥—Ä–∞
#   - 'linear': –ª–∏–Ω–µ–π–Ω–∞—è –≥—Ä–∞–Ω–∏—Ü–∞, –±—ã—Å—Ç—Ä–æ
#   - 'rbf': –Ω–µ–ª–∏–Ω–µ–π–Ω–∞—è, —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –≤—ã–±–æ—Ä
#   - 'poly': –ø–æ–ª–∏–Ω–æ–º–∏–∞–ª—å–Ω–∞—è –≥—Ä–∞–Ω–∏—Ü–∞
#
# gamma (–¥–ª—è RBF): "—Ä–∞–¥–∏—É—Å –≤–ª–∏—è–Ω–∏—è" –∫–∞–∂–¥–æ–π —Ç–æ—á–∫–∏
#   - 'scale': 1/(n_features * X.var()) ‚Äî default, —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è
#   - 'auto': 1/n_features
#   - –ë–æ–ª—å—à–æ–π gamma: overfitting (–∫–∞–∂–¥–∞—è —Ç–æ—á–∫–∞ ‚Äî –æ—Å—Ç—Ä–æ–≤–æ–∫)
#   - –ú–∞–ª—ã–π gamma: underfitting (–ø–æ—á—Ç–∏ –ª–∏–Ω–µ–π–Ω–æ)
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

# –ö–†–ò–¢–ò–ß–ù–û: –≤—Å–µ–≥–¥–∞ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞—Ç—å –¥–ª—è SVM!
pipeline_svm = Pipeline([
    ('scaler', StandardScaler()),
    ('svm', SVC(kernel='rbf', C=1.0, gamma='scale'))
])

# GridSearch –¥–ª—è –ø–æ–¥–±–æ—Ä–∞ C –∏ gamma
param_grid = {
    'svm__C': [0.1, 1, 10, 100],
    'svm__gamma': ['scale', 0.01, 0.1, 1]
}
grid = GridSearchCV(pipeline_svm, param_grid, cv=5, scoring='accuracy')

# LinearSVC: –±—ã—Å—Ç—Ä–µ–µ –¥–ª—è linear kernel
linear_svm = LinearSVC(C=1.0, max_iter=10000)

# –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ (–Ω—É–∂–Ω–∞ –∫–∞–ª–∏–±—Ä–æ–≤–∫–∞!)
svm_proba = SVC(kernel='rbf', probability=True)  # –º–µ–¥–ª–µ–Ω–Ω–µ–µ!
```

### –¢–∞–±–ª–∏—Ü–∞ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤

| –ú–æ–¥–µ–ª—å | –ü–∞—Ä–∞–º–µ—Ç—Ä | Default | –î–∏–∞–ø–∞–∑–æ–Ω | –ö–æ–≥–¥–∞ –º–µ–Ω—è—Ç—å |
|--------|----------|---------|----------|--------------|
| **Ridge** | alpha | 1.0 | 0.01-100 | –ß–µ—Ä–µ–∑ RidgeCV |
| **Lasso** | alpha | 1.0 | 0.001-10 | LassoCV, max_iter‚Üë –µ—Å–ª–∏ –Ω–µ —Å—Ö–æ–¥–∏—Ç—Å—è |
| **LogReg** | C | 1.0 | 0.01-100 | C‚Üì –ø—Ä–∏ overfitting |
| | penalty | l2 | l1, l2, elasticnet | l1 –¥–ª—è feature selection |
| | class_weight | None | balanced | –ü—Ä–∏ –¥–∏—Å–±–∞–ª–∞–Ω—Å–µ –∫–ª–∞—Å—Å–æ–≤ |
| **SVM** | C | 1.0 | 0.01-1000 | GridSearchCV –æ–±—è–∑–∞—Ç–µ–ª–µ–Ω |
| | gamma | scale | 0.001-10 | –¢–æ–ª—å–∫–æ –¥–ª—è RBF |
| | kernel | rbf | linear, rbf, poly | linear –¥–ª—è –≤—ã—Å–æ–∫–æ—Ä–∞–∑–º–µ—Ä–Ω—ã—Ö |

---

## üéØ Q&A –¥–ª—è —ç–∫–∑–∞–º–µ–Ω–∞

**Q1: –ü–æ—á–µ–º—É L1 –¥–µ–ª–∞–µ—Ç feature selection, –∞ L2 –Ω–µ—Ç?**
> –ì–µ–æ–º–µ—Ç—Ä–∏—è: L1 constraint ‚Äî —Ä–æ–º–± —Å –æ—Å—Ç—Ä—ã–º–∏ —É–≥–ª–∞–º–∏ –Ω–∞ –æ—Å—è—Ö. –û–ø—Ç–∏–º–∞–ª—å–Ω–∞—è —Ç–æ—á–∫–∞ —á–∞—Å—Ç–æ –ø–æ–ø–∞–¥–∞–µ—Ç –Ω–∞ —É–≥–æ–ª, –≥–¥–µ —á–∞—Å—Ç—å –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–≤ = 0. L2 constraint ‚Äî –∫—Ä—É–≥, –∫–∞—Å–∞–Ω–∏–µ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –Ω–∞ –≥–ª–∞–¥–∫–æ–π –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–∏ ‚Üí –≤—Å–µ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã –Ω–µ–Ω—É–ª–µ–≤—ã–µ, –ø—Ä–æ—Å—Ç–æ –º–∞–ª–µ–Ω—å–∫–∏–µ.

**Q2: –ü–æ—á–µ–º—É C –≤ LogisticRegression ‚Äî —ç—Ç–æ 1/Œª?**
> –ò—Å—Ç–æ—Ä–∏—á–µ—Å–∫–æ–µ —Å–æ–≥–ª–∞—à–µ–Ω–∏–µ sklearn. C = 1/Œª, –ø–æ—ç—Ç–æ–º—É –±–æ–ª—å—à–æ–π C –æ–∑–Ω–∞—á–∞–µ—Ç —Å–ª–∞–±—É—é —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—é (–º–∞–ª–µ–Ω—å–∫–∏–π Œª), –∞ –º–∞–ª—ã–π C ‚Äî —Å–∏–ª—å–Ω—É—é —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—é.

**Q3: –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å SVM vs Logistic Regression?**
> LogReg: –Ω—É–∂–Ω—ã –∫–∞–ª–∏–±—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏, –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è, –±—ã—Å—Ç—Ä–æ–µ –æ–±—É—á–µ–Ω–∏–µ. SVM: –Ω–µ–ª–∏–Ω–µ–π–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (kernel), –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π margin –≤–∞–∂–Ω–µ–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π, —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∫ –≤—ã–±—Ä–æ—Å–∞–º (—Ç–æ–ª—å–∫–æ support vectors).

**Q4: –ß—Ç–æ —Ç–∞–∫–æ–µ kernel trick –∏ –ø–æ—á–µ–º—É –æ–Ω —Ä–∞–±–æ—Ç–∞–µ—Ç?**
> Kernel trick –∑–∞–º–µ–Ω—è–µ—Ç —è–≤–Ω–æ–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ œÜ(x)·µÄœÜ(x') (–≤ –≤—ã—Å–æ–∫–æ—Ä–∞–∑–º–µ—Ä–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ) –Ω–∞ K(x,x'). –†–∞–±–æ—Ç–∞–µ—Ç, –ø–æ—Ç–æ–º—É —á—Ç–æ SVM –∑–∞–≤–∏—Å–∏—Ç —Ç–æ–ª—å–∫–æ –æ—Ç —Å–∫–∞–ª—è—Ä–Ω—ã—Ö –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–π x·µÄx', –Ω–µ –æ—Ç —Å–∞–º–∏—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤. RBF kernel K(x,x') = exp(-Œ≥||x-x'||¬≤) –Ω–µ—è–≤–Ω–æ –æ—Ç–æ–±—Ä–∞–∂–∞–µ—Ç –≤ –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ–º–µ—Ä–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ!

**Q5: –°–≤—è–∑—å MLE –∏ MSE/LogLoss?**
> MSE ‚Äî —ç—Ç–æ MLE –ø—Ä–∏ –ø—Ä–µ–¥–ø–æ–ª–æ–∂–µ–Ω–∏–∏ Gaussian noise: y = Xw + Œµ, Œµ ~ N(0,œÉ¬≤). LogLoss ‚Äî —ç—Ç–æ MLE –¥–ª—è Bernoulli —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è: y ~ Bernoulli(œÉ(w·µÄx)). –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è = –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ prior (MAP –≤–º–µ—Å—Ç–æ MLE).

---

## –†–µ–∑—é–º–µ –º–æ–¥—É–ª—è

–õ–∏–Ω–µ–π–Ω—ã–µ –º–æ–¥–µ–ª–∏ ‚Äî —ç—Ç–æ **—Ä–∞–±–æ—á–∏–µ –ª–æ—à–∞–¥–∫–∏ ML**:
- **–†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è**: L1 (sparsity) vs L2 (shrinkage), –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–π —Å–º—ã—Å–ª
- **Logistic Regression**: –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏, Log Loss, –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è —á–µ—Ä–µ–∑ odds
- **SVM**: –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π margin, kernel trick –¥–ª—è –Ω–µ–ª–∏–Ω–µ–π–Ω–æ—Å—Ç–∏
- **MLE**: —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π framework, —Å–≤—è–∑—å —Å OLS, LogReg, —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–µ–π

–ü–æ–Ω–∏–º–∞–Ω–∏–µ —ç—Ç–∏—Ö –º–µ—Ç–æ–¥–æ–≤ –∫—Ä–∏—Ç–∏—á–Ω–æ, –æ–Ω–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –≤–µ–∑–¥–µ –æ—Ç –∫—Ä–µ–¥–∏—Ç–Ω–æ–≥–æ —Å–∫–æ—Ä–∏–Ω–≥–∞ –¥–æ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–π –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏!
