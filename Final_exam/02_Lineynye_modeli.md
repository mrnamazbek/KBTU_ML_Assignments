# Модуль 2: Линейные модели

## 2.1 Регуляризация (Regularization): Ridge (L2) и Lasso (L1)

### 1) Интуиция и суть

**Простыми словами (ELI5):**  
Представь, что ты строишь модель самолета из конструктора. Если использовать ВСЕ детали из коробки (все признаки), самолет получится слишком сложным и развалится при первом полете (переобучение). Регуляризация — это правило: "используй как можно меньше деталей, но чтобы самолет летал". 

- **L2 (Ridge)**: делает все детали меньше (уменьшает все коэффициенты)
- **L1 (Lasso)**: выбрасывает ненужные детали совсем (обнуляет коэффициенты)

**Академически:**  
Регуляризация — это добавление **штрафа (penalty)** к функции потерь за сложность модели. Цель — предотвратить переобучение, контролируя величину коэффициентов.

**Физический смысл:**  
Большие коэффициенты → модель сильно реагирует на малые изменения признаков → высокая **variance**. Регуляризация "сжимает" коэффициенты, делая модель более устойчивой.

### 2) Зачем и когда (Business Value)

**Какую проблему решает:**
- **Переобучение** (overfitting) при большом числе признаков
- **Мультиколлинеарность** (сильно коррелированные признаки)
- **Feature selection** (L1 автоматически выбирает важные признаки)
- **Численная стабильность** (Ridge делает (X^T X)^(-1) более устойчивой)

**Где это индустриальный стандарт:**
- **Финансы**: кредитный скоринг (много коррелированных признаков)
- **Геномика**: p >> n (признаков больше, чем объектов)
- **NLP**: text classification (огромное число признаков — слова)
- **Рекомендательные системы**: matrix factorization (Ridge для regularization)

**Когда применять нельзя:**
- ❌ Мало признаков и нет переобучения (незачем)
- ❌ Все признаки важны и некоррелированы (Lasso может удалить нужные)
- ❌ Нужна точная интерпретация коэффициентов (регуляризация их "размывает")

### 3) Математическое ядро

**Ridge Regression (L2):**

**Функция потерь:**
```
L(β) = ||y - Xβ||² + λ||β||²₂
     = Σ(y_i - x_i^T β)² + λ Σ β_j²
       |_______________|   |_____|
            MSE           L2 penalty
```

**Аналитическое решение:**
```
β̂_Ridge = (X^T X + λI)^(-1) X^T y
```

Добавление λI к X^T X гарантирует **обратимость** даже при мультиколлинеарности!

**Lasso Regression (L1):**

**Функция потерь:**
```
L(β) = ||y - Xβ||² + λ||β||₁
     = Σ(y_i - x_i^T β)² + λ Σ |β_j|
       |_______________|   |______|
            MSE           L1 penalty
```

**Решение:**  
Нет аналитической формулы! Используются итеративные алгоритмы:
- **Coordinate Descent**
- **Proximal Gradient Descent**

**Elastic Net (комбинация L1 и L2):**

```
L(β) = ||y - Xβ||² + λ₁||β||₁ + λ₂||β||²₂
```

Сочетает преимущества обоих: feature selection (L1) + устойчивость к мультиколлинеарности (L2).

### 4) Middle-level нюансы

**Проблема 1: Геометрический смысл L1 vs L2**

**L2 (Ridge):**
```
Constraint region: ||β||²₂ ≤ t  (круг/сфера)
```

Решение — точка касания эллипса (contour lines MSE) и круга (L2 constraint).  
Круг гладкий → решения редко попадают на оси → **коэффициенты малые, но ненулевые**.

**L1 (Lasso):**
```
Constraint region: ||β||₁ ≤ t  (ромб/гиперромб)
```

Ромб имеет **острые углы на осях** → решения часто попадают на углы → **некоторые коэффициенты = 0**.

**Визуально (2D):**

```
       β₂               L1 (Lasso)
        │    ╱╲
        │   ╱  ╲        ← Ромб
        │  ╱ ●  ╲       ● — точка касания
        │ ╱  ╲
        └─────────── β₁
        
       β₂               L2 (Ridge)
        │   ╭──╮
        │  │ ● │        ← Круг
        │   ╰──╯
        │
        └─────────── β₁
```

**Вывод:**  
L1 обнуляет коэффициенты (feature selection), L2 — только уменьшает.

**Проблема 2: Выбор λ (гиперпараметр регуляризации)**

**λ = 0**: нет регуляризации → обычная OLS → может переобучаться  
**λ → ∞**: все коэффициенты → 0 → модель предсказывает только среднее (underfitting)

**Метод подбора:**
- **Cross-Validation** (GridSearchCV, RidgeCV, LassoCV)
- **Regularization path**: построить график MSE(λ) и выбрать минимум на валидации

**Пример:**
```python
from sklearn.linear_model import RidgeCV
ridge = RidgeCV(alphas=[0.01, 0.1, 1, 10, 100], cv=5)
ridge.fit(X_train, y_train)
print(ridge.alpha_)  # оптимальное λ
```

**Проблема 3: Масштабирование признаков (КРИТИЧНО!)**

Регуляризация штрафует коэффициенты **по абсолютной величине**.  
Если признак X₁ ∈ [0, 1], а X₂ ∈ [0, 1000], то β₂ будет в ~1000 раз меньше β₁ просто из-за масштаба!

**Решение:**  
ВСЕГДА **стандартизировать** признаки перед регуляризацией:
```python
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_train)
```

**Проблема 4: Lasso нестабилен при мультиколлинеарности**

Если два признака сильно коррелированы, Lasso **случайно выберет один**, а другой обнулит.  
При малом изменении данных может выбрать другой признак!

**Пример:**
```
Признаки: площадь_кв_м и площадь_кв_футы (полностью коррелированы)
Lasso может выбрать любой из них, результат нестабилен
```

**Решение:**
- **Elastic Net**: α·L1 + (1-α)·L2, обычно α = 0.5
- Вручную удалить один из коррелированных признаков

### 5) Сравнение Ridge vs Lasso vs Elastic Net

| Метод | L1 penalty | L2 penalty | Feature Selection | Мультиколлинеарность | Когда использовать |
|-------|------------|------------|-------------------|---------------------|-------------------|
| **OLS** | ❌ | ❌ | ❌ | ❌ Чувствителен | Мало признаков, нет корреляций |
| **Ridge (L2)** | ❌ | ✅ | ❌ Все признаки остаются | ✅ Устойчив | Много коррелированных признаков, все важны |
| **Lasso (L1)** | ✅ | ❌ | ✅ Автоматический | ⚠️ Нестабилен | p >> n, нужна интерпретация, feature selection |
| **Elastic Net** | ✅ | ✅ | ✅ Автоматический | ✅ Устойчив | Комбинация: feature selection + группы коррелированных признаков |

**Рекомендации:**
- **Ridge**: когда нужны все признаки, есть мультиколлинеарность
- **Lasso**: когда нужна разреженность (sparsity), feature selection
- **Elastic Net**: большинство реальных задач (золотая середина)

---

## 2.2 Логистическая регрессия (Logistic Regression)

### 1) Интуиция и суть

**Простыми словами (ELI5):**  
Ты на собеседовании и решаешь: взять кандидата или нет. У тебя есть его характеристики (опыт, образование, навыки). Логистическая регрессия превращает эти характеристики в **вероятность** от 0 до 1: например, 0.85 = "скорее всего подходит". Если вероятность > 0.5, берем кандидата.

**Академически:**  
Логистическая регрессия — это **линейная модель для бинарной классификации**, которая моделирует вероятность принадлежности к классу 1 через **логистическую функцию (сигмоиду)**. Несмотря на название "регрессия", это классификатор!

**Физический смысл:**  
Линейная комбинация признаков (z = w^T x) может быть любым числом, но вероятность должна быть в [0, 1]. Сигмоида "сжимает" z в (0, 1).

### 2) Зачем и когда (Business Value)

**Какую проблему решает:**
- **Бинарная классификация** с вероятностными оценками
- **Интерпретация важности** признаков (odds ratios)
- **Калибровка**: вероятности близки к реальным частотам

**Где это индустриальный стандарт:**
- **Финансы**: вероятность дефолта по кредиту
- **Медицина**: вероятность заболевания (диагностика)
- **Маркетинг**: вероятность покупки (click-through rate, CTR)
- **Fraud detection**: вероятность мошенничества

**Когда применять нельзя:**
- ❌ Нелинейные границы решений (используйте kernel SVM, tree-based)
- ❌ Мультиклассовая классификация (используйте One-vs-Rest или Multinomial Logistic)
- ❌ Очень несбалансированные классы без коррекции (нужны class weights или resampling)

### 3) Математическое ядро

**Логистическая функция (сигмоида):**
```
σ(z) = 1 / (1 + e^(-z)) = e^z / (1 + e^z)
```

**Свойства:**
- σ(z) ∈ (0, 1) для любого z ∈ ℝ
- σ(0) = 0.5
- σ(z) → 1 при z → ∞
- σ(z) → 0 при z → -∞
- **Симметрия**: σ(-z) = 1 - σ(z)

**Модель:**
```
z = w^T x + b  (линейная комбинация)
P(y=1|x) = σ(z) = 1 / (1 + e^(-(w^T x + b)))
```

**Предсказание:**
```
ŷ = 1, если P(y=1|x) ≥ 0.5
ŷ = 0, иначе
```

**Функция потерь: Log Loss (Binary Cross-Entropy)**

```
L(w) = -1/n Σ [y_i log(p_i) + (1-y_i) log(1-p_i)]
```

где p_i = σ(w^T x_i) — предсказанная вероятность.

**Смысл слагаемых:**
- Если y = 1: loss = -log(p) → минимум при p → 1
- Если y = 0: loss = -log(1-p) → минимум при p → 0

**Оптимизация:**  
Нет аналитического решения! Используются итеративные методы:
- **Gradient Descent**
- **Newton-Raphson** (быстрее, но нужна Hessian матрица)
- **L-BFGS, SAG, SAGA** (для больших данных)

**Градиент:**
```
∇L(w) = 1/n X^T (σ(Xw) - y)
```

Удивительно похож на OLS!

### 4) Middle-level нюансы

**Проблема 1: Интерпретация коэффициентов**

**Odds (шансы):**
```
Odds = P(y=1) / P(y=0) = e^(w^T x)
```

**Odds Ratio при изменении x_j на 1:**
```
OR = e^(w_j)
```

**Пример:**
- w_опыт = 0.5
- OR = e^0.5 ≈ 1.65
- **Интерпретация**: при увеличении опыта на 1 год **шансы** принятия увеличиваются в 1.65 раза

**Проблема 2: Регуляризация**

Аналогично линейной регрессии:
- **L2 (Ridge)**: penalty = λ||w||²
- **L1 (Lasso)**: penalty = λ||w||₁

В sklearn:
```python
from sklearn.linear_model import LogisticRegression
# C = 1/λ (чем меньше C, тем сильнее регуляризация!)
model = LogisticRegression(penalty='l2', C=1.0)
```

**Проблема 3: Несбалансированные классы**

Если класс 0: 95%, класс 1: 5%, модель может научиться предсказывать всегда 0 (accuracy = 95%!).

**Решения:**

1. **Class weights:**
```python
LogisticRegression(class_weight='balanced')
```
Автоматически: weight_class_i = n / (2 · n_class_i)

2. **Resampling:**
- Oversampling меньшинства (SMOTE)
- Undersampling большинства

3. **Adjust threshold:**
```python
# Вместо 0.5 используем 0.2 для класса 1
y_pred = (proba[:, 1] > 0.2).astype(int)
```

**Проблема 4: Decision Boundary**

Граница решений: σ(w^T x + b) = 0.5
⟺ w^T x + b = 0 (гиперплоскость!)

**Логистическая регрессия = линейный классификатор**.

Для нелинейных границ:
- Polynomial features: x, x², x³, x₁x₂, ...
- Kernel trick (не поддерживается напрямую в LogReg, используйте SVM)

### 5) Сравнение с альтернативами

| Метод | Выход | Граница решений | Вероятности | Обучение | Когда использовать |
|-------|-------|----------------|-------------|----------|-------------------|
| **Logistic Regression** | Вероятность | Линейная | ✅ Калибровка хорошая | Быстро (gradient descent) | Линейно разделимые, нужны вероятности |
| **SVM (linear)** | Расстояние до границы | Линейная | ⚠️ Некалиброванные | Медленнее (QP solver) | Нужна максимальная margin, выбросы |
| **Naive Bayes** | Вероятность | Нелинейная (если признаки независимы) | ⚠️ Слишком уверенные | Очень быстро | Малые данные, text classification |
| **Random Forest** | Голосование деревьев | Нелинейная | ⚠️ Некалиброванные | Медленнее | Нелинейные зависимости, feature importance |

---

## 2.3 Support Vector Machines (SVM)

### 1) Интуиция и суть

**Простыми словами (ELI5):**  
Представь, что у тебя есть яблоки и апельсины на столе, и нужно провести между ними разделительную линию. SVM не просто проводит любую линию, а ищет такую, которая **максимально далеко** от ближайших яблок и апельсинов. Эти ближайшие фрукты — "support vectors" (опорные векторы), они и определяют линию.

**Академически:**  
SVM — это метод классификации, который строит **оптимальную разделяющую гиперплоскость** с **максимальным отступом (margin)** между классами. Использует **ядерный трюк (kernel trick)** для нелинейных границ.

**Физический смысл:**  
Интуиция: чем больше margin, тем более "уверенно" разделены классы → лучше обобщение на новые данные.

### 2) Зачем и когда (Business Value)

**Какую проблему решает:**
- Классификация с **максимальной margin** (лучше обобщение)
- **Нелинейные границы** через kernel trick (без явного создания polynomial features!)
- Устойчивость к **выбросам** (только support vectors влияют на решение)

**Где это индустриальный стандарт:**
- **Bioinformatics**: классификация белков, генов
- **Computer Vision**: распознавание образов (до эры deep learning)
- **Text classification**: spam detection (с linear kernel)
- **Финансы**: fraud detection

**Когда применять нельзя:**
- ❌ Очень большие данные (O(n³) для обучения → используйте linear SVM или LogReg)
- ❌ Нужны вероятности (SVM дает расстояния, не вероятности; калибровка нужна)
- ❌ Много шума в данных (SVM чувствителен к выбросам без soft margin)

### 3) Математическое ядро

**Hard Margin SVM (линейно разделимые данные):**

**Цель:**  
Найти гиперплоскость w^T x + b = 0 с **максимальным margin**.

**Margin:**
```
margin = 2 / ||w||
```

**Оптимизационная задача:**
```
min  (1/2)||w||²
w,b

subject to: y_i(w^T x_i + b) ≥ 1,  ∀i
```

**Смысл constraint:**  
Все точки должны быть **корректно классифицированы** с отступом ≥ 1.

**Решение:**  
Используется **Lagrange multipliers** (двойственная задача):
```
max  Σ α_i - (1/2) Σ Σ α_i α_j y_i y_j x_i^T x_j
α

subject to: α_i ≥ 0, Σ α_i y_i = 0
```

**Support Vectors:**  
Точки с α_i > 0. Остальные точки не влияют на решение!

**Soft Margin SVM (для неразделимых данных):**

Добавляем **slack variables ξ_i** (допускаем ошибки):
```
min  (1/2)||w||² + C Σ ξ_i
w,b,ξ

subject to: y_i(w^T x_i + b) ≥ 1 - ξ_i,  ξ_i ≥ 0
```

**C (гиперпараметр):**
- **C велик**: жесткий margin, мало ошибок (может переобучаться)
- **C мал**: мягкий margin, допускаем ошибки (может недообучаться)

**Kernel Trick (нелинейные границы):**

**Идея:**  
Отобразить данные в **высокоразмерное пространство** φ(x), где они линейно разделимы.

**Проблема:**  
φ(x) может быть бесконечномерным → вычислительно невозможно!

**Решение — Kernel Trick:**  
Заменить x_i^T x_j на K(x_i, x_j) = φ(x_i)^T φ(x_j) (kernel function).  
Вычисляем скалярное произведение в высокоразмерном пространстве **без явного преобразования**!

**Популярные ядра:**

1. **Linear:** K(x, x') = x^T x'
2. **Polynomial:** K(x, x') = (γ x^T x' + c)^d
3. **RBF (Radial Basis Function, Gaussian):**
   ```
   K(x, x') = exp(-γ||x - x'||²)
   ```
   γ — гиперпараметр (чем больше γ, тем более локализованное влияние)
4. **Sigmoid:** K(x, x') = tanh(γ x^T x' + c)

**Решающая функция:**
```
f(x) = sign(Σ α_i y_i K(x_i, x) + b)
```

### 4) Middle-level нюансы

**Проблема 1: Выбор kernel и гиперпараметров**

| Kernel | Гиперпараметры | Когда использовать |
|--------|---------------|-------------------|
| **Linear** | C | Высокоразмерные данные (text, gene data), d >> n |
| **RBF** | C, γ | Универсальный, нелинейные зависимости |
| **Polynomial** | C, degree, γ | Известная полиномиальная зависимость |

**Подбор:**
- GridSearchCV по C и γ (для RBF)
- **Эмпирические правила для RBF:**
  - γ = 1 / (n_features · Var(X))
  - C ∈ [0.1, 1, 10, 100]

**Проблема 2: RBF kernel — риск переобучения**

**γ слишком велик:**  
Каждая точка — "остров" → модель запоминает обучающую выборку.

**γ слишком мал:**  
Kernel почти линейный → недообучение.

**Проблема 3: Масштабирование признаков (КРИТИЧНО для RBF!)**

RBF kernel зависит от **евклидова расстояния** ||x - x'||.  
Признаки с большим масштабом доминируют!

**Решение:**  
ВСЕГДА стандартизировать:
```python
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
```

**Проблема 4: Вычислительная сложность**

- **Обучение**: O(n² · d) до O(n³) для QP solver
- **Предсказание**: O(n_sv · d), где n_sv — число support vectors

Для больших данных:
- **LinearSVC** (liblinear library): оптимизирован для linear kernel
- **SGDClassifier** с loss='hinge': аппроксимация SVM через SGD

### 5) Сравнение с альтернативами

| Метод | Граница | Устойчивость к выбросам | Нелинейность | Вычислительная сложность | Вероятности |
|-------|---------|------------------------|-------------|--------------------------|-------------|
| **SVM (RBF)** | Нелинейная | ⚠️ Средняя (зависит от C) | ✅ Kernel trick | ❌ O(n²-n³) | ❌ (нужна калибровка) |
| **Logistic Regression** | Линейная | ⚠️ Чувствителен к выбросам | ❌ (без polynomial features) | ✅ O(n·d) | ✅ Калиброванные |
| **Random Forest** | Нелинейная | ✅ Устойчив (ансамбль) | ✅ Автоматически | ⚠️ O(n·log n · d · trees) | ⚠️ Некалиброванные |
| **k-NN** | Нелинейная | ❌ Очень чувствителен | ✅ Локальность | ❌ O(n·d) на предсказание | ⚠️ Локальные вероятности |

**Рекомендации:**
- **Малые/средние данные, нелинейная задача**: SVM с RBF
- **Высокоразмерные, разреженные (text)**: SVM с linear kernel
- **Нужны вероятности, интерпретация**: Logistic Regression
- **Большие данные, нелинейность**: Random Forest, XGBoost

---

## 2.4 Maximum Likelihood Estimation (MLE)

### 1) Интуиция и суть

**Простыми словами (ELI5):**  
У тебя есть монетка, и ты подбросил ее 10 раз: 7 орлов, 3 решки. Какая вероятность выпадения орла? MLE говорит: "выбери такую вероятность, при которой наблюдаемые данные (7 орлов, 3 решки) были бы **наиболее вероятны**". Ответ: p = 7/10 = 0.7.

**Академически:**  
MLE — это метод оценки параметров статистической модели путем максимизации **функции правдоподобия (likelihood function)** — вероятности наблюдаемых данных при данных параметрах.

**Физический смысл:**  
Мы ищем параметры модели, которые "лучше всего объясняют" наблюдаемые данные.

### 2) Зачем и когда (Business Value)

**Какую проблему решает:**
- **Оценка параметров** любых вероятностных моделей
- Универсальный framework для **статистического вывода**
- **Теоретическое обоснование** многих ML алгоритмов

**Где это фундамент:**
- **Linear Regression** (MLE с Gaussian noise = OLS!)
- **Logistic Regression** (MLE с Bernoulli распределением = Log Loss)
- **Naive Bayes** (MLE для оценки P(X|Y))
- **GMM (Gaussian Mixture Models)** (EM algorithm = итеративный MLE)

**Связь с ML:**  
Минимизация функции потерь часто эквивалентна MLE для определенного распределения!

### 3) Математическое ядро

**Likelihood Function:**

Пусть данные {x_1, ..., x_n} независимы и одинаково распределены (i.i.d.) с плотностью p(x|θ).

**Likelihood:**
```
L(θ | x_1, ..., x_n) = Π p(x_i | θ)
```

**Log-Likelihood (удобнее для оптимизации):**
```
ℓ(θ) = log L(θ) = Σ log p(x_i | θ)
```

**MLE:**
```
θ̂_MLE = argmax ℓ(θ)
          θ
```

**Пример 1: OLS как MLE**

**Предположение:**  
y_i = w^T x_i + ε_i,  где ε_i ∼ N(0, σ²)

**Likelihood:**
```
p(y_i | x_i, w) = (1/√(2πσ²)) exp(-(y_i - w^T x_i)² / (2σ²))
```

**Log-Likelihood:**
```
ℓ(w) = Σ [-(y_i - w^T x_i)² / (2σ²) - log(√(2πσ²))]
     ∝ -Σ (y_i - w^T x_i)²  (убираем константы)
```

**MLE:**
```
w_MLE = argmax ℓ(w) = argmin Σ (y_i - w^T x_i)²
        w              w
```

**Это MSE!** OLS = MLE для Gaussian noise.

**Пример 2: Logistic Regression как MLE**

**Предположение:**  
y_i ∼ Bernoulli(p_i),  где p_i = σ(w^T x_i)

**Likelihood:**
```
p(y_i | x_i, w) = p_i^(y_i) · (1 - p_i)^(1 - y_i)
```

**Log-Likelihood:**
```
ℓ(w) = Σ [y_i log(p_i) + (1 - y_i) log(1 - p_i)]
```

**MLE:**
```
w_MLE = argmax ℓ(w)
        w
```

**Это минимизация Log Loss!**

### 4) Middle-level нюансы

**Проблема 1: MLE может переобучаться**

**Пример:**  
Подбросили монетку 3 раза, все орлы.  
MLE: p̂ = 3/3 = 1.0 (уверены, что монета всегда орел!)

Это **overfitting**. Мало данных → MLE неустойчива.

**Решение:**
- **MAP (Maximum A Posteriori)**: добавляем prior (байесовский подход):
  ```
  θ_MAP = argmax [log p(data|θ) + log p(θ)]
          θ
  ```
- **Регуляризация в ML = prior в MLE!**
  - Ridge (L2) ⟺ Gaussian prior: p(w) ∝ exp(-λ||w||²)
  - Lasso (L1) ⟺ Laplace prior: p(w) ∝ exp(-λ||w||₁)

**Проблема 2: Численная оптимизация**

Обычно нет аналитического решения → используются итеративные методы:
- **Gradient Ascent**: θ^(t+1) = θ^(t) + α · ∇ℓ(θ^(t))
- **Newton-Raphson** (второй порядок)
- **EM algorithm** для скрытых переменных (GMM, HMM)

**Проблема 3: Свойства MLE**

**Плюсы (асимптотически при n → ∞):**
- **Consistency**: θ̂_MLE → θ_true
- **Asymptotic Normality**: √n(θ̂_MLE - θ_true) → N(0, I^(-1)), где I — Fisher Information
- **Efficiency**: минимальная асимптотическая дисперсия среди несмещенных оценок

**Минусы:**
- Может быть **смещенной** при малых n
- Требует корректного указания **распределения**

### 5) Сравнение MLE vs MAP vs Bayesian

| Подход | Формула | Prior | Когда использовать |
|--------|---------|-------|-------------------|
| **MLE** | θ = argmax p(data\|θ) | ❌ Нет | Много данных, нет априорных знаний |
| **MAP** | θ = argmax [p(data\|θ) · p(θ)] | ✅ Один prior | Средние данные, есть предположения |
| **Full Bayesian** | p(θ\|data) ∝ p(data\|θ) · p(θ) | ✅ Распределение на θ | Малые данные, нужна неопределенность |

**Связь с регуляризацией:**
```
MLE + L2 regularization = MAP с Gaussian prior
MLE + L1 regularization = MAP с Laplace prior
```

---

## Резюме модуля

Линейные модели — это **рабочие лошадки ML**:
- **Регуляризация**: L1 (sparsity) vs L2 (shrinkage), геометрический смысл
- **Logistic Regression**: вероятности, Log Loss, интерпретация через odds
- **SVM**: максимальный margin, kernel trick для нелинейности
- **MLE**: универсальный framework, связь с OLS, LogReg, регуляризацией

Понимание этих методов критично, они используются везде от кредитного скоринга до медицинской диагностики!
