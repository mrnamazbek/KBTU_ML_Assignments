# Модуль 6: Нейронные сети (Neural Networks)

## 6.1 Backpropagation (обратное распространение ошибки)

### 1) Интуиция и суть

**Простыми словами (ELI5):**  
Представь цепочку людей, передающих мяч. В конце мяч попал мимо цели. Чтобы исправить ситуацию, мы идем **назад**: последний человек корректирует бросок, предпоследний корректирует передачу ему, и так далее до начала. Каждый корректирует свою часть ошибки.

**Академически:**  
Backpropagation — это алгоритм эффективного вычисления **градиентов функции потерь** по параметрам сети с использованием **chain rule** (правило дифференцирования сложной функции). Основа обучения нейросетей.

**Физический смысл:**  
Мы распространяем ошибку с выхода сети **обратно** к весам, вычисляя вклад каждого веса в итоговую ошибку.

### 2) Зачем и когда (Business Value)

**Какую проблему решает:**
- **Обучение** глубоких нейросетей (десятки/сотни слоев)
- **Эффективное вычисление градиентов** для всех параметров за O(параметры)
- Без backprop обучение сетей было бы невозможно!

**Где используется:**
- **Везде в Deep Learning**: CV, NLP, Speech, Reinforcement Learning
- **Фреймворки**: PyTorch, TensorFlow — автоматический backprop!

**Критичность:**  
Это **фундамент** современного ML. Без понимания backprop невозможно работать с нейросетями.

### 3) Математическое ядро

**Прямой проход (Forward Pass):**

**Простая сеть (2 слоя):**
```
Input: x
Hidden: h = σ(W₁x + b₁)
Output: ŷ = σ(W₂h + b₂)
Loss: L = (1/2)(y - ŷ)²
```

**Обратный проход (Backward Pass) — Chain Rule:**

**Цель:** вычислить ∂L/∂W₁, ∂L/∂W₂, ∂L/∂b₁, ∂L/∂b₂

**Выходной слой:**
```
∂L/∂ŷ = -(y - ŷ)  (производная MSE)
```

**Для W₂:**
```
∂L/∂W₂ = ∂L/∂ŷ · ∂ŷ/∂W₂
       = ∂L/∂ŷ · σ'(W₂h + b₂) · h
```

**Для скрытого слоя:**
```
∂L/∂h = ∂L/∂ŷ · ∂ŷ/∂h
      = ∂L/∂ŷ · σ'(W₂h + b₂) · W₂
```

**Для W₁:**
```
∂L/∂W₁ = ∂L/∂h · ∂h/∂W₁
       = ∂L/∂h · σ'(W₁x + b₁) · x
```

**Общий паттерн (Chain Rule):**
```
∂L/∂w = ∂L/∂output · ∂output/∂w
```

**Обновление параметров (Gradient Descent):**
```
W₁ ← W₁ - α · ∂L/∂W₁
W₂ ← W₂ - α · ∂L/∂W₂
```

### 4) Middle-level нюансы

**Проблема 1: Vanishing Gradients (затухание градиентов)**

**Причина:**  
При глубоких сетях градиент = произведение многих производных активаций:
```
∂L/∂W₁ = ∂L/∂ŷ · σ'(z_n) · W_n · σ'(z_{n-1}) · ... · σ'(z_1) · x
```

**Если σ' < 1** (например, sigmoid: σ' ≤ 0.25), то:
```
(0.25)^10 ≈ 10^(-6)  ← градиент исчезает!
```

**Последствия:**
- Ранние слои **почти не обучаются** (градиенты ≈ 0)
- Сеть застревает (saturation)

**Решения:**
1. **ReLU activation**: ReLU'(z) = 1 (для z > 0) → нет затухания
2. **Batch Normalization**: нормализует активации
3. **Residual connections** (ResNet): y = F(x) + x (skip connections)
4. **Better initialization**: Xavier, He initialization

**Проблема 2: Exploding Gradients (взрыв градиентов)**

**Обратная проблема:**  
Если производные велики, градиент растет экспоненциально.

**Последствия:**
- Веса становятся огромными
- NaN в вычислениях

**Решения:**
1. **Gradient Clipping**: огранчиваем ||∇|| ≤ threshold
2. **Learning rate scheduler**: уменьшаем α со временем
3. **Batch Normalization**

**Проблема 3: Вычислительная эффективность backprop**

**Наивный подход:**  
Вычислять ∂L/∂w для каждого w отдельно → O(параметры²)

**Backprop (dynamic programming):**  
Сохраняем промежуточные градиенты (∂L/∂h, ...) и переиспользуем → O(параметры)

**Computational graph:**  
Современные фреймворки (PyTorch, TF) строят граф вычислений и автоматически применяют backprop!

### 5) Сравнение методов оптимизации

| Метод | Gradient | Сложность per iteration | Когда использовать |
|-------|----------|-------------------------|-------------------|
| **Numerical gradients** | Приближенные (finite differences) | O(параметры²) | Только для отладки (gradient check) |
| **Backpropagation** | ✅ Точные | ✅ O(параметры) | Всегда для обучения |

---

## 6.2 Функции активации (Activation Functions)

### 1) Интуиция и суть

**Простыми словами (ELI5):**  
Функция активации решает: "насколько сильно этот нейрон должен 'активироваться' (влиять на следующий слой)?" Без нее нейросеть — это просто набор линейных преобразований (матричных умножений), а композиция линейных функций = линейная функция. Нелинейность нужна для сложных паттернов!

**Академически:**  
Activation function — это **нелинейная функция**, применяемая к выходу нейрона, позволяющая сети аппроксимировать **произвольные нелинейные зависимости**.

**Физический смысл:**  
Вводим **нелинейность** → сеть может моделировать сложные функции.

### 2) Зачем и когда (Business Value)

**Какую проблему решает:**
- **Нелинейное моделирование**: без активаций сеть = линейная регрессия
- **Разные свойства**: разные активации для разных задач

**Где используется:**
- **Hidden layers**: ReLU (стандарт)
- **Output layer**: зависит от задачи (Sigmoid для binary classification, Softmax для multi-class)

### 3) Математическое ядро

**Популярные активации:**

**1. Sigmoid:**
```
σ(z) = 1 / (1 + e^(-z))
σ'(z) = σ(z) · (1 - σ(z))
```

**Свойства:**
- Выход: (0, 1) — интерпретация как вероятность
- Производная: σ' ≤ 0.25 → **vanishing gradient**!

**2. Tanh (гиперболический тангенс):**
```
tanh(z) = (e^z - e^(-z)) / (e^z + e^(-z))
tanh'(z) = 1 - tanh²(z)
```

**Свойства:**
- Выход: (-1, 1) — zero-centered (лучше, чем sigmoid)
- Производная: tanh' ≤ 1 → все равно **vanishing gradient**

**3. ReLU (Rectified Linear Unit):**
```
ReLU(z) = max(0, z)
ReLU'(z) = {1, если z > 0; 0, если z ≤ 0}
```

**Свойства:**
- ✅ Нет vanishing gradient (для z > 0)
- ✅ Быстрые вычисления
- ❌ "Dead ReLU" (нейроны с z < 0 навсегда "умирают")

**4. Leaky ReLU:**
```
Leaky ReLU(z) = max(αz, z),  α ≈ 0.01
```

**Исправляет "dead ReLU"**: градиент ≠ 0 даже при z < 0.

**5. ELU (Exponential Linear Unit):**
```
ELU(z) = {z, если z > 0; α(e^z - 1), если z ≤ 0}
```

**Свойства:**
- Сглаженный ReLU
- Производная непрерывна

**6. Softmax (для output layer, multi-class):**
```
Softmax(z_i) = e^(z_i) / Σ e^(z_j)
```

**Свойства:**
- Σ Softmax(z_i) = 1 → интерпретация как вероятности
- Используется с Categorical Cross-Entropy loss

### 4) Middle-level нюансы

**Проблема 1: Sigmoid vs Tanh vs ReLU**

| Activation | Range | Zero-centered | Vanishing gradient | Dead neurons | Когда использовать |
|-----------|-------|---------------|-------------------|-------------|-------------------|
| **Sigmoid** | (0, 1) | ❌ | ✅ Проблема | ❌ | Output (binary classification) |
| **Tanh** | (-1, 1) | ✅ | ⚠️ Меньше, чем sigmoid | ❌ | RNN (исторически) |
| **ReLU** | [0, ∞) | ❌ | ✅ Нет (для z>0) | ⚠️ Да | **Hidden layers (стандарт!)** |
| **Leaky ReLU** | (-∞, ∞) | ⚠️ | ✅ Нет | ✅ Нет | Когда много dead ReLU |

**Рекомендация:**  
**Используйте ReLU по умолчанию**. Если проблема с dead neurons → Leaky ReLU или ELU.

**Проблема 2: Dead ReLU Problem**

**Причина:**  
Если z = Wx + b < 0 для всех примеров, то ReLU(z) = 0 → градиент 0 → веса не обновляются → нейрон "умер".

**Пример:**
```
Большой отрицательный bias b << 0
→ z < 0 всегда
→ ReLU = 0, градиент = 0
→ нейрон не обучается
```

**Решения:**
1. **Leaky ReLU** (градиент αz при z < 0)
2. **ReLU initialization**: He initialization (веса ∼ N(0, 2/n_in))
3. **Lower learning rate**: не убивать нейроны большими шагами

**Проблема 3: Output Layer Activation**

| Задача | Output activation | Loss function |
|--------|------------------|---------------|
| **Binary classification** | Sigmoid | Binary Cross-Entropy |
| **Multi-class (single label)** | Softmax | Categorical Cross-Entropy |
| **Multi-label (multiple labels)** | Sigmoid (per class) | Binary Cross-Entropy (per class) |
| **Regression** | Linear (нет активации) | MSE |

### 5) Сравнение активаций

**Hidden layers:**
- **Default**: ReLU
- **Dead neurons**: Leaky ReLU, PReLU, ELU
- **Smoothness важна**: ELU, GELU

**Output layer:**
- **Binary**: Sigmoid
- **Multi-class**: Softmax
- **Regression**: Linear

---

## 6.3 Инициализация весов (Weight Initialization)

### 1) Интуиция и суть

**Простыми словами (ELI5):**  
Представь, что ты учишься стрелять по мишени, начиная с **первого выстрела**. Если начальная позиция плохая (например, ствол смотрит в землю), ты потратишь много времени, просто чтобы добраться до мишени. Хорошая инициализация = хорошая стартовая позиция.

**Академически:**  
Weight initialization — это выбор начальных значений весов сети. Хорошая инициализация критична для **быстрой сходимости** и избежания **vanishing/exploding gradients**.

### 2) Зачем и когда (Business Value)

**Какую проблему решает:**
- **Плохая сходимость**: неправильная инициализация → медленное обучение
- **Vanishing/exploding gradients**: уже с первой итерации

**Где используется:**
- **Везде в нейросетях**: правильная инициализация — must-have

### 3) Математическое ядро

**Плохие стратегии:**

**1. Все нули:**
```
W = 0
```
**Проблема:** все нейроны вычисляют одно и то же → симметрия не нарушается → сеть не обучается!

**2. Слишком большие значения:**
```
W ∼ N(0, 1)  (большая дисперсия)
```
**Проблема:** активации → огромные → насыщение (saturation sigmoid/tanh) → vanishing gradients.

**Хорошие стратегии:**

**Xavier (Glorot) Initialization (для sigmoid/tanh):**
```
W ∼ N(0, 2 / (n_in + n_out))
```
или
```
W ∼ Uniform[-√(6/(n_in + n_out)), √(6/(n_in + n_out))]
```

**Идея:** сохранить дисперсию активаций примерно одинаковой на всех слоях.

**He Initialization (для ReLU):**
```
W ∼ N(0, 2 / n_in)
```

**Почему 2/n_in, а не 1/n_in:**  
ReLU обнуляет половину активаций → нужна большая дисперсия для компенсации.

### 4) Middle-level нюансы

**Xavier vs He:**

| Activation | Initialization | Формула |
|-----------|---------------|---------|
| **Sigmoid, Tanh** | Xavier | Var(W) = 2/(n_in + n_out) |
| **ReLU, Leaky ReLU** | He | Var(W) = 2/n_in |

**Проблема Batch Normalization:**  
При использовании Batch Norm инициализация менее критична (BN нормализует активации).

### 5) Рекомендации

- **ReLU**: He initialization
- **Sigmoid/Tanh**: Xavier initialization
- **С Batch Norm**: инициализация менее важна, но все равно используйте He/Xavier

---

## 6.4 Оптимизаторы: SGD, Adam

### 1) Интуиция и суть

**Простыми словами (ELI5):**  
Оптимизатор — это **способ шагать** к минимуму функции потерь.
- **SGD**: идешь прямо вниз по градиенту (может застрять в узких оврагах)
- **Momentum**: как шар, катящийся вниз (набирает инерцию, преодолевает небольшие препятствия)
- **Adam**: умный шар, который еще и адаптирует размер шага для каждого направления

**Академически:**  
Optimizer — это алгоритм обновления параметров на основе градиентов для минимизации функции потерь.

### 2) Математическое ядро

**1. SGD (Stochastic Gradient Descent):**
```
θ_{t+1} = θ_t - α · ∇L(θ_t)
```

**Проблемы:**
- Медленная сходимость в ravines (узкие овраги)
- Фиксированный learning rate для всех параметров

**2. SGD with Momentum:**
```
v_t = β · v_{t-1} + (1 - β) · ∇L(θ_t)
θ_{t+1} = θ_t - α · v_t
```

**Идея:** сглаживаем градиент, используя **exponential moving average**.  
**Эффект:** ускорение в направлении постоянного градиента, сглаживание колебаний.

**3. RMSprop:**
```
s_t = β · s_{t-1} + (1 - β) · (∇L(θ_t))²
θ_{t+1} = θ_t - α · ∇L(θ_t) / √(s_t + ε)
```

**Идея:** адаптивный learning rate для каждого параметра (больший шаг для малых градиентов).

**4. Adam (Adaptive Moment Estimation):**

Комбинация Momentum + RMSprop:
```
m_t = β₁ · m_{t-1} + (1 - β₁) · ∇L(θ_t)  (first moment — momentum)
v_t = β₂ · v_{t-1} + (1 - β₂) · (∇L(θ_t))²  (second moment — RMSprop)

m̂_t = m_t / (1 - β₁^t)  (bias correction)
v̂_t = v_t / (1 - β₂^t)

θ_{t+1} = θ_t - α · m̂_t / (√v̂_t + ε)
```

**Гиперпараметры (стандартные):**
- α = 0.001 (learning rate)
- β₁ = 0.9
- β₂ = 0.999
- ε = 10^(-8)

### 3) Middle-level нюансы

**Сравнение:**

| Optimizer | Скорость | Стабильность | Гиперпараметры | Когда использовать |
|-----------|----------|-------------|---------------|-------------------|
| **SGD** | ⚠️ Медленно | ✅ Стабильно | α | Простые задачи, хорошая инициализация |
| **SGD + Momentum** | ✅ Быстрее | ✅ Стабильно | α, β | Классические CV задачи |
| **Adam** | ✅✅ Быстро | ⚠️ Может переобучаться | α, β₁, β₂ | **Default для большинства задач** |
| **AdamW** | ✅✅ Быстро | ✅ Лучше, чем Adam | α, β₁, β₂, weight_decay | Modern default |

**Рекомендации:**
- **Default**: Adam или AdamW (с weight decay)
- **Computer Vision** (ResNet, etc.): SGD + Momentum часто лучше
- **NLP (Transformers)**: Adam с cosine lr scheduler

### 4) Сравнение SGD vs Adam

**SGD:**
- ✅ Лучше обобщение (generalization) при правильной настройке
- ❌ Требует tuning learning rate
- Лучше для **долгого обучения** (сотни эпох)

**Adam:**
- ✅ Быстрая сходимость
- ✅ Мало чувствителен к α
- ⚠️ Может переобучаться (используйте weight decay → AdamW)
- Лучше для **быстрых экспериментов**

---

## Резюме модуля

Нейросети — это **универсальные аппроксиматоры**:
- **Backpropagation**: chain rule для эффективного вычисления градиентов, vanishing/exploding gradients
- **Activation functions**: ReLU (default), Sigmoid (output binary), Softmax (output multi-class)
- **Инициализация**: He для ReLU, Xavier для sigmoid/tanh
- **Оптимизаторы**: Adam (default, быстро), SGD+Momentum (лучше generalization)

Понимание этих основ критично для работы с любыми нейросетями!
