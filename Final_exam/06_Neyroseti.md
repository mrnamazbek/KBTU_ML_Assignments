# –ú–æ–¥—É–ª—å 6: –ù–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ (Neural Networks)

> **–ó–∞—á–µ–º —ç—Ç–æ—Ç –º–æ–¥—É–ª—å?**  
> –ù–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ ‚Äî —ç—Ç–æ **—É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–µ –∞–ø–ø—Ä–æ–∫—Å–∏–º–∞—Ç–æ—Ä—ã**. –¢–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏ –æ–Ω–∏ –º–æ–≥—É—Ç –≤—ã—É—á–∏—Ç—å *–ª—é–±—É—é* —Ñ—É–Ω–∫—Ü–∏—é, –∫–∞–∫–æ–π –±—ã —Å–ª–æ–∂–Ω–æ–π –æ–Ω–∞ –Ω–∏ –±—ã–ª–∞.
> - **–ì–∏–±–∫–æ—Å—Ç—å**: –æ—Ç —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –ª–∏—Ü –¥–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –±–µ—Å–ø–∏–ª–æ—Ç–Ω–∏–∫–∞–º–∏ ‚Äî –≤—Å—ë —ç—Ç–æ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏.
> - **End-to-End**: —Å–µ—Ç–∏ —Å–∞–º–∏ –∏–∑–≤–ª–µ–∫–∞—é—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ –¥–∞–Ω–Ω—ã—Ö (Feature Extraction), –∏–∑–±–∞–≤–ª—è—è –Ω–∞—Å –æ—Ç —Ä—É—á–Ω–æ–≥–æ –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Ñ–∏—á–µ–π.
> - **–ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å**: —á–µ–º –±–æ–ª—å—à–µ –¥–∞–Ω–Ω—ã—Ö –∏ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –º–æ—â–Ω–æ—Å—Ç–µ–π (GPU), —Ç–µ–º —É–º–Ω–µ–µ —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è —Å–µ—Ç—å.

---

## 6.1 Backpropagation (–æ–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –æ—à–∏–±–∫–∏)

### 1) –ò–Ω—Ç—É–∏—Ü–∏—è –∏ —Å—É—Ç—å

**–ü—Ä–æ—Å—Ç—ã–º–∏ —Å–ª–æ–≤–∞–º–∏ (ELI5):**  
–ü—Ä–µ–¥—Å—Ç–∞–≤—å —Ü–µ–ø–æ—á–∫—É –ª—é–¥–µ–π, –ø–µ—Ä–µ–¥–∞—é—â–∏—Ö –º—è—á. –í –∫–æ–Ω—Ü–µ –º—è—á –ø–æ–ø–∞–ª –º–∏–º–æ —Ü–µ–ª–∏. –ß—Ç–æ–±—ã –∏—Å–ø—Ä–∞–≤–∏—Ç—å —Å–∏—Ç—É–∞—Ü–∏—é, –º—ã –∏–¥–µ–º **–Ω–∞–∑–∞–¥**: –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–µ–ª–æ–≤–µ–∫ –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ—Ç –±—Ä–æ—Å–æ–∫, –ø—Ä–µ–¥–ø–æ—Å–ª–µ–¥–Ω–∏–π –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ—Ç –ø–µ—Ä–µ–¥–∞—á—É –µ–º—É, –∏ —Ç–∞–∫ –¥–∞–ª–µ–µ –¥–æ –Ω–∞—á–∞–ª–∞. –ö–∞–∂–¥—ã–π –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ—Ç —Å–≤–æ—é —á–∞—Å—Ç—å –æ—à–∏–±–∫–∏.

**–ê–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏:**  
Backpropagation ‚Äî —ç—Ç–æ –∞–ª–≥–æ—Ä–∏—Ç–º —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è **–≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å** –ø–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º —Å–µ—Ç–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º **chain rule** (–ø—Ä–∞–≤–∏–ª–æ –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä–æ–≤–∞–Ω–∏—è —Å–ª–æ–∂–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏). –û—Å–Ω–æ–≤–∞ –æ–±—É—á–µ–Ω–∏—è –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π.

**–§–∏–∑–∏—á–µ—Å–∫–∏–π —Å–º—ã—Å–ª:**  
–ú—ã —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω—è–µ–º –æ—à–∏–±–∫—É —Å –≤—ã—Ö–æ–¥–∞ —Å–µ—Ç–∏ **–æ–±—Ä–∞—Ç–Ω–æ** –∫ –≤–µ—Å–∞–º, –≤—ã—á–∏—Å–ª—è—è –≤–∫–ª–∞–¥ –∫–∞–∂–¥–æ–≥–æ –≤–µ—Å–∞ –≤ –∏—Ç–æ–≥–æ–≤—É—é –æ—à–∏–±–∫—É.

### 2) –ó–∞—á–µ–º –∏ –∫–æ–≥–¥–∞ (Business Value)

**–ö–∞–∫—É—é –ø—Ä–æ–±–ª–µ–º—É —Ä–µ—à–∞–µ—Ç:**
- **–û–±—É—á–µ–Ω–∏–µ** –≥–ª—É–±–æ–∫–∏—Ö –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π (–¥–µ—Å—è—Ç–∫–∏/—Å–æ—Ç–Ω–∏ —Å–ª–æ–µ–≤)
- **–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤** –¥–ª—è –≤—Å–µ—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∑–∞ O(–ø–∞—Ä–∞–º–µ—Ç—Ä—ã)
- –ë–µ–∑ backprop –æ–±—É—á–µ–Ω–∏–µ —Å–µ—Ç–µ–π –±—ã–ª–æ –±—ã –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ!

**–ì–¥–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è:**
- **–í–µ–∑–¥–µ –≤ Deep Learning**: CV, NLP, Speech, Reinforcement Learning
- **–§—Ä–µ–π–º–≤–æ—Ä–∫–∏**: PyTorch, TensorFlow ‚Äî –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π backprop!

**–ö—Ä–∏—Ç–∏—á–Ω–æ—Å—Ç—å:**  
–≠—Ç–æ **—Ñ—É–Ω–¥–∞–º–µ–Ω—Ç** —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ ML. –ë–µ–∑ –ø–æ–Ω–∏–º–∞–Ω–∏—è backprop –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ —Ä–∞–±–æ—Ç–∞—Ç—å —Å –Ω–µ–π—Ä–æ—Å–µ—Ç—è–º–∏.

### 3) –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —è–¥—Ä–æ

**–ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥ (Forward Pass) –¥–ª—è —Å–ª–æ—è:**
$$
\mathbf{h} = \sigma(\mathbf{W} \mathbf{x} + \mathbf{b})
$$

**–û–±—Ä–∞—Ç–Ω—ã–π –ø—Ä–æ—Ö–æ–¥ (Backward Pass) ‚Äî Chain Rule:**
–î–ª—è –≤–µ—Å–∞ $w$:
$$
\frac{\partial L}{\partial w} = \frac{\partial L}{\partial \text{output}} \cdot \frac{\partial \text{output}}{\partial w}
$$

**–ì—Ä–∞–¥–∏–µ–Ω—Ç –¥–ª—è –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Å–ª–æ—è (MSE):**
$$
\frac{\partial L}{\partial \hat{y}} = -(\mathbf{y} - \hat{\mathbf{y}})
$$

**–ì—Ä–∞–¥–∏–µ–Ω—Ç –¥–ª—è –≤–µ—Å–æ–≤ —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–ª–æ—è (—á–µ—Ä–µ–∑ –≥—Ä–∞–¥–∏–µ–Ω—Ç —Å–ª–µ–¥—É—é—â–µ–≥–æ):**
$$
\frac{\partial L}{\partial \mathbf{W}_1} = \underbrace{\frac{\partial L}{\partial \mathbf{h}_2}}_{\text{–æ—à–∏–±–∫–∞ —Å–≤—ã—à–µ}} \cdot \underbrace{\sigma'(\mathbf{z}_1)}_{\text{–∞–∫—Ç–∏–≤–∞—Ü–∏—è}} \cdot \underbrace{\mathbf{x}^T}_{\text{–≤—Ö–æ–¥}}
$$

> [!CAUTION]
> **Production Warning: Vanishing Gradients (–ó–∞—Ç—É—Ö–∞–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤)**  
> –ï—Å–ª–∏ –≤—ã –∏—Å–ø–æ–ª—å–∑—É–µ—Ç–µ `Sigmoid` –≤ —Å–µ—Ç–∏ –≥–ª—É–±–∂–µ 5 —Å–ª–æ–µ–≤, –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –ø—Ä–∏ —É–º–Ω–æ–∂–µ–Ω–∏–∏ $0.25 \cdot 0.25 \dots$ —Å—Ç–∞–Ω—É—Ç –Ω–∞—Å—Ç–æ–ª—å–∫–æ –º–∞–ª—ã–º–∏ ($\approx 0$), —á—Ç–æ –≤–µ—Å–∞ –ø–µ—Ä–≤—ã—Ö —Å–ª–æ–µ–≤ –ø–µ—Ä–µ—Å—Ç–∞–Ω—É—Ç –æ–±–Ω–æ–≤–ª—è—Ç—å—Å—è. –ú–æ–¥–µ–ª—å –ø–µ—Ä–µ—Å—Ç–∞–Ω–µ—Ç —É—á–∏—Ç—å—Å—è. **–†–µ—à–µ–Ω–∏–µ**: –≤—Å–µ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ `ReLU` –¥–ª—è —Å–∫—Ä—ã—Ç—ã—Ö —Å–ª–æ–µ–≤ –∏ `He initialization`.

> [!TIP]
> **–ê–Ω–∞–ª–æ–≥–∏—è: –°–ª–æ–º–∞–Ω–Ω—ã–π —Ç–µ–ª–µ—Ñ–æ–Ω —Å —É—á–∏—Ç–µ–ª–µ–º**  
> –ü—Ä–µ–¥—Å—Ç–∞–≤—å—Ç–µ —Ü–µ–ø–æ—á–∫—É –ª—é–¥–µ–π, –ø–µ—Ä–µ–¥–∞—é—â–∏—Ö —Å–æ–æ–±—â–µ–Ω–∏–µ. –ü–æ—Å–ª–µ–¥–Ω–∏–π –≥–æ–≤–æ—Ä–∏—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç —É—á–∏—Ç–µ–ª—é. –£—á–∏—Ç–µ–ª—å –≥–æ–≤–æ—Ä–∏—Ç: "–û—à–∏–±–∫–∞ ‚Äî 10 –±–∞–ª–ª–æ–≤". –ü–æ—Å–ª–µ–¥–Ω–∏–π —á–µ–ª–æ–≤–µ–∫ –ø–æ–Ω–∏–º–∞–µ—Ç —Å–≤–æ—é –≤–∏–Ω—É –∏ –≥–æ–≤–æ—Ä–∏—Ç –ø—Ä–µ–¥–ø–æ—Å–ª–µ–¥–Ω–µ–º—É: "–¢—ã –º–Ω–µ –ø–µ—Ä–µ–¥–∞–ª —Å–ª–∏—à–∫–æ–º –≥—Ä–æ–º–∫–æ, –ø–æ—ç—Ç–æ–º—É —è –æ—à–∏–±—Å—è –Ω–∞ 5 –±–∞–ª–ª–æ–≤". –ü—Ä–µ–¥–ø–æ—Å–ª–µ–¥–Ω–∏–π –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ—Ç —Å–µ–±—è –∏ –ø–µ—Ä–µ–¥–∞–µ—Ç –≤–∏–Ω—É –¥–∞–ª—å—à–µ. –¢–∞–∫ "–≤–∏–Ω–∞" (–≥—Ä–∞–¥–∏–µ–Ω—Ç) –¥–æ—Ö–æ–¥–∏—Ç –¥–æ —Å–∞–º–æ–≥–æ –ø–µ—Ä–≤–æ–≥–æ —á–µ–ª–æ–≤–µ–∫–∞.

### 4) Middle-level –Ω—é–∞–Ω—Å—ã

**–ü—Ä–æ–±–ª–µ–º–∞ 1: Vanishing Gradients (–∑–∞—Ç—É—Ö–∞–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤)**

**–ü—Ä–∏—á–∏–Ω–∞:**  
–ü—Ä–∏ –≥–ª—É–±–æ–∫–∏—Ö —Å–µ—Ç—è—Ö –≥—Ä–∞–¥–∏–µ–Ω—Ç = –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ –º–Ω–æ–≥–∏—Ö –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã—Ö –∞–∫—Ç–∏–≤–∞—Ü–∏–π:
$$
\frac{\partial L}{\partial \mathbf{W}_1} = \frac{\partial L}{\partial \hat{y}} \cdot \sigma'(z_n) \cdot \mathbf{W}_n \cdot \sigma'(z_{n-1}) \cdot \dots \cdot \sigma'(z_1) \cdot \mathbf{x}
$$

$$
(0.25)^{10} \approx 10^{-6} \quad \leftarrow \text{–≥—Ä–∞–¥–∏–µ–Ω—Ç –∏—Å—á–µ–∑–∞–µ—Ç!}
$$

**–ü–æ—Å–ª–µ–¥—Å—Ç–≤–∏—è:**
- –†–∞–Ω–Ω–∏–µ —Å–ª–æ–∏ **–ø–æ—á—Ç–∏ –Ω–µ –æ–±—É—á–∞—é—Ç—Å—è** (–≥—Ä–∞–¥–∏–µ–Ω—Ç—ã ‚âà 0)
- –°–µ—Ç—å –∑–∞—Å—Ç—Ä–µ–≤–∞–µ—Ç (saturation)

**–†–µ—à–µ–Ω–∏—è:**
1. **ReLU activation**: $\text{ReLU}'(z) = 1$ (–¥–ª—è $z > 0$) ‚Üí –Ω–µ—Ç –∑–∞—Ç—É—Ö–∞–Ω–∏—è
2. **Batch Normalization**: –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –∞–∫—Ç–∏–≤–∞—Ü–∏–∏
3. **Residual connections** (ResNet): y = F(x) + x (skip connections)
4. **Better initialization**: Xavier, He initialization

**–ü—Ä–æ–±–ª–µ–º–∞ 2: Exploding Gradients (–≤–∑—Ä—ã–≤ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤)**

**–û–±—Ä–∞—Ç–Ω–∞—è –ø—Ä–æ–±–ª–µ–º–∞:**  
–ï—Å–ª–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã–µ –≤–µ–ª–∏–∫–∏, –≥—Ä–∞–¥–∏–µ–Ω—Ç —Ä–∞—Å—Ç–µ—Ç —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–æ.

**–ü–æ—Å–ª–µ–¥—Å—Ç–≤–∏—è:**
- –í–µ—Å–∞ —Å—Ç–∞–Ω–æ–≤—è—Ç—Å—è –æ–≥—Ä–æ–º–Ω—ã–º–∏
- NaN –≤ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è—Ö

**–†–µ—à–µ–Ω–∏—è:**
1. **Gradient Clipping**: –æ–≥—Ä–∞–Ω—á–∏–≤–∞–µ–º ||‚àá|| ‚â§ threshold
2. **Learning rate scheduler**: —É–º–µ–Ω—å—à–∞–µ–º Œ± —Å–æ –≤—Ä–µ–º–µ–Ω–µ–º
3. **Batch Normalization**

**–ü—Ä–æ–±–ª–µ–º–∞ 3: –í—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–∞—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å backprop**

**–ù–∞–∏–≤–Ω—ã–π –ø–æ–¥—Ö–æ–¥:**  
–í—ã—á–∏—Å–ª—è—Ç—å ‚àÇL/‚àÇw –¥–ª—è –∫–∞–∂–¥–æ–≥–æ w –æ—Ç–¥–µ–ª—å–Ω–æ ‚Üí O(–ø–∞—Ä–∞–º–µ—Ç—Ä—ã¬≤)

**Backprop (dynamic programming):**  
–°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã (‚àÇL/‚àÇh, ...) –∏ –ø–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º ‚Üí O(–ø–∞—Ä–∞–º–µ—Ç—Ä—ã)

**Computational graph:**  
–°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∏ (PyTorch, TF) —Å—Ç—Ä–æ—è—Ç –≥—Ä–∞—Ñ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø—Ä–∏–º–µ–Ω—è—é—Ç backprop!

### 5) –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç–æ–¥–æ–≤ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏

| –ú–µ—Ç–æ–¥ | Gradient | –°–ª–æ–∂–Ω–æ—Å—Ç—å per iteration | –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å |
|-------|----------|-------------------------|-------------------|
| **Numerical gradients** | –ü—Ä–∏–±–ª–∏–∂–µ–Ω–Ω—ã–µ (finite differences) | O(–ø–∞—Ä–∞–º–µ—Ç—Ä—ã¬≤) | –¢–æ–ª—å–∫–æ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏ (gradient check) |
| **Backpropagation** | ‚úÖ –¢–æ—á–Ω—ã–µ | ‚úÖ O(–ø–∞—Ä–∞–º–µ—Ç—Ä—ã) | –í—Å–µ–≥–¥–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è |

---

## 6.2 –§—É–Ω–∫—Ü–∏–∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ (Activation Functions)

### 1) –ò–Ω—Ç—É–∏—Ü–∏—è –∏ —Å—É—Ç—å

**–ü—Ä–æ—Å—Ç—ã–º–∏ —Å–ª–æ–≤–∞–º–∏ (ELI5):**  
–§—É–Ω–∫—Ü–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ —Ä–µ—à–∞–µ—Ç: "–Ω–∞—Å–∫–æ–ª—å–∫–æ —Å–∏–ª—å–Ω–æ —ç—Ç–æ—Ç –Ω–µ–π—Ä–æ–Ω –¥–æ–ª–∂–µ–Ω '–∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞—Ç—å—Å—è' (–≤–ª–∏—è—Ç—å –Ω–∞ —Å–ª–µ–¥—É—é—â–∏–π —Å–ª–æ–π)?" –ë–µ–∑ –Ω–µ–µ –Ω–µ–π—Ä–æ—Å–µ—Ç—å ‚Äî —ç—Ç–æ –ø—Ä–æ—Å—Ç–æ –Ω–∞–±–æ—Ä –ª–∏–Ω–µ–π–Ω—ã—Ö –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–π (–º–∞—Ç—Ä–∏—á–Ω—ã—Ö —É–º–Ω–æ–∂–µ–Ω–∏–π), –∞ –∫–æ–º–ø–æ–∑–∏—Ü–∏—è –ª–∏–Ω–µ–π–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π = –ª–∏–Ω–µ–π–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è. –ù–µ–ª–∏–Ω–µ–π–Ω–æ—Å—Ç—å –Ω—É–∂–Ω–∞ –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤!

**–ê–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏:**  
Activation function ‚Äî —ç—Ç–æ **–Ω–µ–ª–∏–Ω–µ–π–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è**, –ø—Ä–∏–º–µ–Ω—è–µ–º–∞—è –∫ –≤—ã—Ö–æ–¥—É –Ω–µ–π—Ä–æ–Ω–∞, –ø–æ–∑–≤–æ–ª—è—é—â–∞—è —Å–µ—Ç–∏ –∞–ø–ø—Ä–æ–∫—Å–∏–º–∏—Ä–æ–≤–∞—Ç—å **–ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã–µ –Ω–µ–ª–∏–Ω–µ–π–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏**.

**–§–∏–∑–∏—á–µ—Å–∫–∏–π —Å–º—ã—Å–ª:**  
–í–≤–æ–¥–∏–º **–Ω–µ–ª–∏–Ω–µ–π–Ω–æ—Å—Ç—å** ‚Üí —Å–µ—Ç—å –º–æ–∂–µ—Ç –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞—Ç—å —Å–ª–æ–∂–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏.

### 2) –ó–∞—á–µ–º –∏ –∫–æ–≥–¥–∞ (Business Value)

**–ö–∞–∫—É—é –ø—Ä–æ–±–ª–µ–º—É —Ä–µ—à–∞–µ—Ç:**
- **–ù–µ–ª–∏–Ω–µ–π–Ω–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ**: –±–µ–∑ –∞–∫—Ç–∏–≤–∞—Ü–∏–π —Å–µ—Ç—å = –ª–∏–Ω–µ–π–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è
- **–†–∞–∑–Ω—ã–µ —Å–≤–æ–π—Å—Ç–≤–∞**: —Ä–∞–∑–Ω—ã–µ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –∑–∞–¥–∞—á

**–ì–¥–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è:**
- **Hidden layers**: ReLU (—Å—Ç–∞–Ω–¥–∞—Ä—Ç)
- **Output layer**: –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –∑–∞–¥–∞—á–∏ (Sigmoid –¥–ª—è binary classification, Softmax –¥–ª—è multi-class)

### 3) –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —è–¥—Ä–æ

**–ü–æ–ø—É–ª—è—Ä–Ω—ã–µ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏:**

**1. Sigmoid:**
$$
\sigma(z) = \frac{1}{1 + e^{-z}} \quad \text{range: } (0, 1)
$$

**2. ReLU (Rectified Linear Unit):**
$$
\text{ReLU}(z) = \max(0, z) \quad \text{range: } [0, \infty)
$$

**3. Softmax (–¥–ª—è –º—É–ª—å—Ç–∏–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏):**
$$
\text{Softmax}(z_i) = \frac{e^{z_i}}{\sum_{j} e^{z_j}} \quad \sum \text{Softmax} = 1
$$

**4. Binary Cross-Entropy (BCE):**
$$
L = -\frac{1}{N} \sum [y_i \log(p_i) + (1-y_i) \log(1-p_i)]
$$

> [!IMPORTANT]
> **Dead ReLU Problem**  
> –ï—Å–ª–∏ –≤–µ—Å–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã —Ç–∞–∫, —á—Ç–æ $\mathbf{W}\mathbf{x} + \mathbf{b}$ –≤—Å–µ–≥–¥–∞ –º–µ–Ω—å—à–µ –Ω—É–ª—è, –Ω–µ–π—Ä–æ–Ω –≤—ã–¥–∞–µ—Ç 0 –∏ –∏–º–µ–µ—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç 0. –û–Ω "—É–º–∏—Ä–∞–µ—Ç" –∏ –±–æ–ª—å—à–µ –Ω–∏–∫–æ–≥–¥–∞ –Ω–µ –æ–±–Ω–æ–≤–∏—Ç—Å—è. –ï—Å–ª–∏ —É –≤–∞—Å 50% —Å–µ—Ç–∏ –≤—ã–¥–∞–µ—Ç –Ω—É–ª–∏ ‚Äî –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é –≤–µ—Å–æ–≤ –∏–ª–∏ –ø–æ–ø—Ä–æ–±—É–π—Ç–µ `Leaky ReLU` ($\max(0.01z, z)$).

> [!TIP]
> **–ê–Ω–∞–ª–æ–≥–∏—è: –ë–∏–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –Ω–µ–π—Ä–æ–Ω**  
> –§—É–Ω–∫—Ü–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ ‚Äî —ç—Ç–æ –ø–æ—Ä–æ–≥ —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏—è. –ï—Å–ª–∏ —Å–∏–≥–Ω–∞–ª –æ—Ç –¥—Ä—É–≥–∏—Ö –Ω–µ–π—Ä–æ–Ω–æ–≤ —Å–ª–∞–±—ã–π, –Ω–µ–π—Ä–æ–Ω "–º–æ–ª—á–∏—Ç" (0). –ö–∞–∫ —Ç–æ–ª—å–∫–æ —Å–∏–≥–Ω–∞–ª –ø—Ä–µ–≤—ã—à–∞–µ—Ç –ø–æ—Ä–æ–≥, –Ω–µ–π—Ä–æ–Ω "—Å—Ç—Ä–µ–ª—è–µ—Ç" –∏–º–ø—É–ª—å—Å–æ–º –≤ —Å–ª–µ–¥—É—é—â–∏–π —Å–ª–æ–π. ReLU ‚Äî —Å–∞–º–∞—è –ø—Ä–æ—Å—Ç–∞—è –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –∏–º–∏—Ç–∞—Ü–∏—è —ç—Ç–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞.
### 4) Middle-level –Ω—é–∞–Ω—Å—ã

**–ü—Ä–æ–±–ª–µ–º–∞ 1: Sigmoid vs Tanh vs ReLU**

| Activation | Range | Zero-centered | Vanishing gradient | Dead neurons | –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å |
|-----------|-------|---------------|-------------------|-------------|-------------------|
| **Sigmoid** | (0, 1) | ‚ùå | ‚úÖ –ü—Ä–æ–±–ª–µ–º–∞ | ‚ùå | Output (binary classification) |
| **Tanh** | (-1, 1) | ‚úÖ | ‚ö†Ô∏è –ú–µ–Ω—å—à–µ, —á–µ–º sigmoid | ‚ùå | RNN (–∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏) |
| **ReLU** | [0, ‚àû) | ‚ùå | ‚úÖ –ù–µ—Ç (–¥–ª—è z>0) | ‚ö†Ô∏è –î–∞ | **Hidden layers (—Å—Ç–∞–Ω–¥–∞—Ä—Ç!)** |
| **Leaky ReLU** | (-‚àû, ‚àû) | ‚ö†Ô∏è | ‚úÖ –ù–µ—Ç | ‚úÖ –ù–µ—Ç | –ö–æ–≥–¥–∞ –º–Ω–æ–≥–æ dead ReLU |

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è:**  
**–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ ReLU –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é**. –ï—Å–ª–∏ –ø—Ä–æ–±–ª–µ–º–∞ —Å dead neurons ‚Üí Leaky ReLU –∏–ª–∏ ELU.

**–ü—Ä–æ–±–ª–µ–º–∞ 2: Dead ReLU Problem**

**–ü—Ä–∏—á–∏–Ω–∞:**  
–ï—Å–ª–∏ z = Wx + b < 0 –¥–ª—è –≤—Å–µ—Ö –ø—Ä–∏–º–µ—Ä–æ–≤, —Ç–æ ReLU(z) = 0 ‚Üí –≥—Ä–∞–¥–∏–µ–Ω—Ç 0 ‚Üí –≤–µ—Å–∞ –Ω–µ –æ–±–Ω–æ–≤–ª—è—é—Ç—Å—è ‚Üí –Ω–µ–π—Ä–æ–Ω "—É–º–µ—Ä".

**–ü—Ä–∏–º–µ—Ä:**
```
–ë–æ–ª—å—à–æ–π –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–π bias b << 0
‚Üí z < 0 –≤—Å–µ–≥–¥–∞
‚Üí ReLU = 0, –≥—Ä–∞–¥–∏–µ–Ω—Ç = 0
‚Üí –Ω–µ–π—Ä–æ–Ω –Ω–µ –æ–±—É—á–∞–µ—Ç—Å—è
```

**–†–µ—à–µ–Ω–∏—è:**
1. **Leaky ReLU** (–≥—Ä–∞–¥–∏–µ–Ω—Ç Œ±z –ø—Ä–∏ z < 0)
2. **ReLU initialization**: He initialization (–≤–µ—Å–∞ ‚àº N(0, 2/n_in))
3. **Lower learning rate**: –Ω–µ —É–±–∏–≤–∞—Ç—å –Ω–µ–π—Ä–æ–Ω—ã –±–æ–ª—å—à–∏–º–∏ —à–∞–≥–∞–º–∏

**–ü—Ä–æ–±–ª–µ–º–∞ 3: Output Layer Activation**

| –ó–∞–¥–∞—á–∞ | Output activation | Loss function |
|--------|------------------|---------------|
| **Binary classification** | Sigmoid | Binary Cross-Entropy |
| **Multi-class (single label)** | Softmax | Categorical Cross-Entropy |
| **Multi-label (multiple labels)** | Sigmoid (per class) | Binary Cross-Entropy (per class) |
| **Regression** | Linear (–Ω–µ—Ç –∞–∫—Ç–∏–≤–∞—Ü–∏–∏) | MSE |

### 5) –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∞–∫—Ç–∏–≤–∞—Ü–∏–π

**Hidden layers:**
- **Default**: ReLU
- **Dead neurons**: Leaky ReLU, PReLU, ELU
- **Smoothness –≤–∞–∂–Ω–∞**: ELU, GELU

**Output layer:**
- **Binary**: Sigmoid
- **Multi-class**: Softmax
- **Regression**: Linear

---

## 6.3 –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤ (Weight Initialization)

### 1) –ò–Ω—Ç—É–∏—Ü–∏—è –∏ —Å—É—Ç—å

**–ü—Ä–æ—Å—Ç—ã–º–∏ —Å–ª–æ–≤–∞–º–∏ (ELI5):**  
–ü—Ä–µ–¥—Å—Ç–∞–≤—å, —á—Ç–æ —Ç—ã —É—á–∏—à—å—Å—è —Å—Ç—Ä–µ–ª—è—Ç—å –ø–æ –º–∏—à–µ–Ω–∏, –Ω–∞—á–∏–Ω–∞—è —Å **–ø–µ—Ä–≤–æ–≥–æ –≤—ã—Å—Ç—Ä–µ–ª–∞**. –ï—Å–ª–∏ –Ω–∞—á–∞–ª—å–Ω–∞—è –ø–æ–∑–∏—Ü–∏—è –ø–ª–æ—Ö–∞—è (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Å—Ç–≤–æ–ª —Å–º–æ—Ç—Ä–∏—Ç –≤ –∑–µ–º–ª—é), —Ç—ã –ø–æ—Ç—Ä–∞—Ç–∏—à—å –º–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏, –ø—Ä–æ—Å—Ç–æ —á—Ç–æ–±—ã –¥–æ–±—Ä–∞—Ç—å—Å—è –¥–æ –º–∏—à–µ–Ω–∏. –•–æ—Ä–æ—à–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è = —Ö–æ—Ä–æ—à–∞—è —Å—Ç–∞—Ä—Ç–æ–≤–∞—è –ø–æ–∑–∏—Ü–∏—è.

**–ê–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏:**  
Weight initialization ‚Äî —ç—Ç–æ –≤—ã–±–æ—Ä –Ω–∞—á–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –≤–µ—Å–æ–≤ —Å–µ—Ç–∏. –•–æ—Ä–æ—à–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫—Ä–∏—Ç–∏—á–Ω–∞ –¥–ª—è **–±—ã—Å—Ç—Ä–æ–π —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏** –∏ –∏–∑–±–µ–∂–∞–Ω–∏—è **vanishing/exploding gradients**.

### 2) –ó–∞—á–µ–º –∏ –∫–æ–≥–¥–∞ (Business Value)

**–ö–∞–∫—É—é –ø—Ä–æ–±–ª–µ–º—É —Ä–µ—à–∞–µ—Ç:**
- **–ü–ª–æ—Ö–∞—è —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å**: –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ‚Üí –º–µ–¥–ª–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ
- **Vanishing/exploding gradients**: —É–∂–µ —Å –ø–µ—Ä–≤–æ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏

**–ì–¥–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è:**
- **–í–µ–∑–¥–µ –≤ –Ω–µ–π—Ä–æ—Å–µ—Ç—è—Ö**: –ø—Ä–∞–≤–∏–ª—å–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ‚Äî must-have

### 3) –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —è–¥—Ä–æ

**–ü–ª–æ—Ö–∏–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏:**

**1. –í—Å–µ –Ω—É–ª–∏:**
```
W = 0
```
**–ü—Ä–æ–±–ª–µ–º–∞:** –≤—Å–µ –Ω–µ–π—Ä–æ–Ω—ã –≤—ã—á–∏—Å–ª—è—é—Ç –æ–¥–Ω–æ –∏ —Ç–æ –∂–µ ‚Üí —Å–∏–º–º–µ—Ç—Ä–∏—è –Ω–µ –Ω–∞—Ä—É—à–∞–µ—Ç—Å—è ‚Üí —Å–µ—Ç—å –Ω–µ –æ–±—É—á–∞–µ—Ç—Å—è!

**2. –°–ª–∏—à–∫–æ–º –±–æ–ª—å—à–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è:**
```
W ‚àº N(0, 1)  (–±–æ–ª—å—à–∞—è –¥–∏—Å–ø–µ—Ä—Å–∏—è)
```
**–ü—Ä–æ–±–ª–µ–º–∞:** –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ ‚Üí –æ–≥—Ä–æ–º–Ω—ã–µ ‚Üí –Ω–∞—Å—ã—â–µ–Ω–∏–µ (saturation sigmoid/tanh) ‚Üí vanishing gradients.

**–•–æ—Ä–æ—à–∏–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏:**

**Xavier (Glorot) Initialization (–¥–ª—è sigmoid/tanh):**
```
W ‚àº N(0, 2 / (n_in + n_out))
```
–∏–ª–∏
```
W ‚àº Uniform[-‚àö(6/(n_in + n_out)), ‚àö(6/(n_in + n_out))]
```

**–ò–¥–µ—è:** —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –¥–∏—Å–ø–µ—Ä—Å–∏—é –∞–∫—Ç–∏–≤–∞—Ü–∏–π –ø—Ä–∏–º–µ—Ä–Ω–æ –æ–¥–∏–Ω–∞–∫–æ–≤–æ–π –Ω–∞ –≤—Å–µ—Ö —Å–ª–æ—è—Ö.

**He Initialization (–¥–ª—è ReLU):**
```
W ‚àº N(0, 2 / n_in)
```

**–ü–æ—á–µ–º—É 2/n_in, –∞ –Ω–µ 1/n_in:**  
ReLU –æ–±–Ω—É–ª—è–µ—Ç –ø–æ–ª–æ–≤–∏–Ω—É –∞–∫—Ç–∏–≤–∞—Ü–∏–π ‚Üí –Ω—É–∂–Ω–∞ –±–æ–ª—å—à–∞—è –¥–∏—Å–ø–µ—Ä—Å–∏—è –¥–ª—è –∫–æ–º–ø–µ–Ω—Å–∞—Ü–∏–∏.

### 4) Middle-level –Ω—é–∞–Ω—Å—ã

**Xavier vs He:**

| Activation | Initialization | –§–æ—Ä–º—É–ª–∞ |
|-----------|---------------|---------|
| **Sigmoid, Tanh** | Xavier | Var(W) = 2/(n_in + n_out) |
| **ReLU, Leaky ReLU** | He | Var(W) = 2/n_in |

**–ü—Ä–æ–±–ª–µ–º–∞ Batch Normalization:**  
–ü—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ Batch Norm –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–µ–Ω–µ–µ –∫—Ä–∏—Ç–∏—á–Ω–∞ (BN –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –∞–∫—Ç–∏–≤–∞—Ü–∏–∏).

### 5) –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏

- **ReLU**: He initialization
- **Sigmoid/Tanh**: Xavier initialization
- **–° Batch Norm**: –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–µ–Ω–µ–µ –≤–∞–∂–Ω–∞, –Ω–æ –≤—Å–µ —Ä–∞–≤–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ He/Xavier

---

## 6.4 –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä—ã: SGD, Adam

### 1) –ò–Ω—Ç—É–∏—Ü–∏—è –∏ —Å—É—Ç—å

**–ü—Ä–æ—Å—Ç—ã–º–∏ —Å–ª–æ–≤–∞–º–∏ (ELI5):**  
–û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä ‚Äî —ç—Ç–æ **—Å–ø–æ—Å–æ–± —à–∞–≥–∞—Ç—å** –∫ –º–∏–Ω–∏–º—É–º—É —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å.
- **SGD**: –∏–¥–µ—à—å –ø—Ä—è–º–æ –≤–Ω–∏–∑ –ø–æ –≥—Ä–∞–¥–∏–µ–Ω—Ç—É (–º–æ–∂–µ—Ç –∑–∞—Å—Ç—Ä—è—Ç—å –≤ —É–∑–∫–∏—Ö –æ–≤—Ä–∞–≥–∞—Ö)
- **Momentum**: –∫–∞–∫ —à–∞—Ä, –∫–∞—Ç—è—â–∏–π—Å—è –≤–Ω–∏–∑ (–Ω–∞–±–∏—Ä–∞–µ—Ç –∏–Ω–µ—Ä—Ü–∏—é, –ø—Ä–µ–æ–¥–æ–ª–µ–≤–∞–µ—Ç –Ω–µ–±–æ–ª—å—à–∏–µ –ø—Ä–µ–ø—è—Ç—Å—Ç–≤–∏—è)
- **Adam**: —É–º–Ω—ã–π —à–∞—Ä, –∫–æ—Ç–æ—Ä—ã–π –µ—â–µ –∏ –∞–¥–∞–ø—Ç–∏—Ä—É–µ—Ç —Ä–∞–∑–º–µ—Ä —à–∞–≥–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è

**–ê–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏:**  
Optimizer ‚Äî —ç—Ç–æ –∞–ª–≥–æ—Ä–∏—Ç–º –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –¥–ª—è –º–∏–Ω–∏–º–∏–∑–∞—Ü–∏–∏ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å.

### 2) –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —è–¥—Ä–æ

**1. SGD (Stochastic Gradient Descent):**
```
Œ∏_{t+1} = Œ∏_t - Œ± ¬∑ ‚àáL(Œ∏_t)
```

**–ü—Ä–æ–±–ª–µ–º—ã:**
- –ú–µ–¥–ª–µ–Ω–Ω–∞—è —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å –≤ ravines (—É–∑–∫–∏–µ –æ–≤—Ä–∞–≥–∏)
- –§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π learning rate –¥–ª—è –≤—Å–µ—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤

**2. SGD with Momentum:**
```
v_t = Œ≤ ¬∑ v_{t-1} + (1 - Œ≤) ¬∑ ‚àáL(Œ∏_t)
Œ∏_{t+1} = Œ∏_t - Œ± ¬∑ v_t
```

**–ò–¥–µ—è:** —Å–≥–ª–∞–∂–∏–≤–∞–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç, –∏—Å–ø–æ–ª—å–∑—É—è **exponential moving average**.  
**–≠—Ñ—Ñ–µ–∫—Ç:** —É—Å–∫–æ—Ä–µ–Ω–∏–µ –≤ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–∏ –ø–æ—Å—Ç–æ—è–Ω–Ω–æ–≥–æ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞, —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ –∫–æ–ª–µ–±–∞–Ω–∏–π.

**3. RMSprop:**
```
s_t = Œ≤ ¬∑ s_{t-1} + (1 - Œ≤) ¬∑ (‚àáL(Œ∏_t))¬≤
Œ∏_{t+1} = Œ∏_t - Œ± ¬∑ ‚àáL(Œ∏_t) / ‚àö(s_t + Œµ)
```

**–ò–¥–µ—è:** –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π learning rate –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ (–±–æ–ª—å—à–∏–π —à–∞–≥ –¥–ª—è –º–∞–ª—ã—Ö –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤).

**4. Adam (Adaptive Moment Estimation):**

–ö–æ–º–±–∏–Ω–∞—Ü–∏—è Momentum + RMSprop:
```
m_t = Œ≤‚ÇÅ ¬∑ m_{t-1} + (1 - Œ≤‚ÇÅ) ¬∑ ‚àáL(Œ∏_t)  (first moment ‚Äî momentum)
v_t = Œ≤‚ÇÇ ¬∑ v_{t-1} + (1 - Œ≤‚ÇÇ) ¬∑ (‚àáL(Œ∏_t))¬≤  (second moment ‚Äî RMSprop)

mÃÇ_t = m_t / (1 - Œ≤‚ÇÅ^t)  (bias correction)
vÃÇ_t = v_t / (1 - Œ≤‚ÇÇ^t)

Œ∏_{t+1} = Œ∏_t - Œ± ¬∑ mÃÇ_t / (‚àövÃÇ_t + Œµ)
```

**–ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã (—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ):**
- Œ± = 0.001 (learning rate)
- Œ≤‚ÇÅ = 0.9
- Œ≤‚ÇÇ = 0.999
- Œµ = 10^(-8)

### 3) Middle-level –Ω—é–∞–Ω—Å—ã

**–°—Ä–∞–≤–Ω–µ–Ω–∏–µ:**

| Optimizer | –°–∫–æ—Ä–æ—Å—Ç—å | –°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å | –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã | –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å |
|-----------|----------|-------------|---------------|-------------------|
| **SGD** | ‚ö†Ô∏è –ú–µ–¥–ª–µ–Ω–Ω–æ | ‚úÖ –°—Ç–∞–±–∏–ª—å–Ω–æ | Œ± | –ü—Ä–æ—Å—Ç—ã–µ –∑–∞–¥–∞—á–∏, —Ö–æ—Ä–æ—à–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è |
| **SGD + Momentum** | ‚úÖ –ë—ã—Å—Ç—Ä–µ–µ | ‚úÖ –°—Ç–∞–±–∏–ª—å–Ω–æ | Œ±, Œ≤ | –ö–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–µ CV –∑–∞–¥–∞—á–∏ |
| **Adam** | ‚úÖ‚úÖ –ë—ã—Å—Ç—Ä–æ | ‚ö†Ô∏è –ú–æ–∂–µ—Ç –ø–µ—Ä–µ–æ–±—É—á–∞—Ç—å—Å—è | Œ±, Œ≤‚ÇÅ, Œ≤‚ÇÇ | **Default –¥–ª—è –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–∞ –∑–∞–¥–∞—á** |
| **AdamW** | ‚úÖ‚úÖ –ë—ã—Å—Ç—Ä–æ | ‚úÖ –õ—É—á—à–µ, —á–µ–º Adam | Œ±, Œ≤‚ÇÅ, Œ≤‚ÇÇ, weight_decay | Modern default |

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:**
- **Default**: Adam –∏–ª–∏ AdamW (—Å weight decay)
- **Computer Vision** (ResNet, etc.): SGD + Momentum —á–∞—Å—Ç–æ –ª—É—á—à–µ
- **NLP (Transformers)**: Adam —Å cosine lr scheduler

### 4) –°—Ä–∞–≤–Ω–µ–Ω–∏–µ SGD vs Adam

**SGD:**
- ‚úÖ –õ—É—á—à–µ –æ–±–æ–±—â–µ–Ω–∏–µ (generalization) –ø—Ä–∏ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–µ
- ‚ùå –¢—Ä–µ–±—É–µ—Ç tuning learning rate
- –õ—É—á—à–µ –¥–ª—è **–¥–æ–ª–≥–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è** (—Å–æ—Ç–Ω–∏ —ç–ø–æ—Ö)

**Adam:**
- ‚úÖ –ë—ã—Å—Ç—Ä–∞—è —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å
- ‚úÖ –ú–∞–ª–æ —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª–µ–Ω –∫ Œ±
- ‚ö†Ô∏è –ú–æ–∂–µ—Ç –ø–µ—Ä–µ–æ–±—É—á–∞—Ç—å—Å—è (–∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ weight decay ‚Üí AdamW)
- –õ—É—á—à–µ –¥–ª—è **–±—ã—Å—Ç—Ä—ã—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤**

---

---

## üìù –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –∫–æ–¥ (PyTorch)

### –°–æ–∑–¥–∞–Ω–∏–µ –∏ –æ–±—É—á–µ–Ω–∏–µ –ø—Ä–æ—Å—Ç–æ–π –Ω–µ–π—Ä–æ—Å–µ—Ç–∏

```python
import torch
import torch.nn as nn
import torch.optim as optim

# 1. –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã
class SimpleNet(nn.Module):
    def __init__(self, input_size, hidden_size, num_classes):
        super(SimpleNet, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)  # –ü–æ–ª–Ω—ã–π —Å–ª–æ–π 1
        self.relu = nn.ReLU()                          # –ê–∫—Ç–∏–≤–∞—Ü–∏—è
        self.fc2 = nn.Linear(hidden_size, num_classes) # –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π
        
    def forward(self, x):
        out = self.fc1(x)
        out = self.relu(out)
        out = self.fc2(out)
        return out  # –ë–µ–∑ Softmax, —Ç.–∫. CrossEntropyLoss –¥–µ–ª–∞–µ—Ç —ç—Ç–æ –≤–Ω—É—Ç—Ä–∏!

# 2. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
model = SimpleNet(input_size=10, hidden_size=5, num_classes=2)

# 3. –§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å –∏ –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.01)

# 4. –ü—Ä–æ—Ö–æ–¥ –æ–±—É—á–µ–Ω–∏—è (–æ–¥–Ω–∞ –∏—Ç–µ—Ä–∞—Ü–∏—è)
dummy_data = torch.randn(1, 10)         # batch_size=1
dummy_target = torch.tensor([1])        # –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –∫–ª–∞—Å—Å

# A. Forward pass
output = model(dummy_data)
loss = criterion(output, dummy_target)

# B. Backward pass
optimizer.zero_grad()  # –û–±–Ω—É–ª–∏—Ç—å —Å—Ç–∞—Ä—ã–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã! –ö–†–ò–¢–ò–ß–ù–û!
loss.backward()        # –í—ã—á–∏—Å–ª–∏—Ç—å –Ω–æ–≤—ã–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã (backprop)
optimizer.step()       # –û–±–Ω–æ–≤–∏—Ç—å –≤–µ—Å–∞ (w = w - lr * grad)

print(f"Loss: {loss.item():.4f}")
```

### –¢–∞–±–ª–∏—Ü–∞: –§—É–Ω–∫—Ü–∏–∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏

| –ê–∫—Ç–∏–≤–∞—Ü–∏—è | –§–æ—Ä–º—É–ª–∞ | Range | –ì–¥–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å | –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ |
|-----------|---------|-------|------------------|------------|
| **ReLU** | max(0, x) | [0, ‚àû) | Hidden layers (Default) | –ë—ã—Å—Ç—Ä–∞—è, –Ω–µ—Ç –∑–∞—Ç—É—Ö–∞–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ (–¥–ª—è x>0) |
| **Sigmoid** | 1 / (1 + e‚ÅªÀ£) | (0, 1) | Output (Binary Class) | "–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å", –∑–∞—Ç—É—Ö–∞–µ—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç |
| **Tanh** | (eÀ£ - e‚ÅªÀ£) / ... | (-1, 1) | RNN, Hidden (—Ä–µ–¥–∫–æ) | –¶–µ–Ω—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∞ –≤ 0 |
| **Softmax** | eÀ£ / Œ£eÀ£ | (0, 1) | Output (Multi-class) | –°—É–º–º–∞ –≤—ã—Ö–æ–¥–æ–≤ = 1 |

---

## üéØ Q&A –¥–ª—è —ç–∫–∑–∞–º–µ–Ω–∞

**Q1: –ß—Ç–æ —Ç–∞–∫–æ–µ Backpropagation —Å–≤–æ–∏–º–∏ —Å–ª–æ–≤–∞–º–∏?**
> –≠—Ç–æ —Å–ø–æ—Å–æ–± —Å–∫–∞–∑–∞—Ç—å –∫–∞–∂–¥–æ–º—É –≤–µ—Å—É –≤ —Å–µ—Ç–∏, –Ω–∞—Å–∫–æ–ª—å–∫–æ —Å–∏–ª—å–Ω–æ –æ–Ω –≤–∏–Ω–æ–≤–∞—Ç –≤ –æ—à–∏–±–∫–µ. –ú—ã —Å—á–∏—Ç–∞–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –æ—Ç –∫–æ–Ω—Ü–∞ —Å–µ—Ç–∏ (loss) –∫ –Ω–∞—á–∞–ª—É (input), –∏—Å–ø–æ–ª—å–∑—É—è Chain Rule, –∏ –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ–º –≤–µ—Å–∞, —á—Ç–æ–±—ã —É–º–µ–Ω—å—à–∏—Ç—å –æ—à–∏–±–∫—É.

**Q2: –ü–æ—á–µ–º—É –Ω—É–∂–Ω–∞ –Ω–µ–ª–∏–Ω–µ–π–Ω–æ—Å—Ç—å (activation functions)?**
> –ë–µ–∑ –Ω–µ–ª–∏–Ω–µ–π–Ω–æ—Å—Ç–µ–π –≤—Å—è –Ω–µ–π—Ä–æ—Å–µ—Ç—å (—Å–∫–æ–ª—å–∫–æ –±—ã —Å–ª–æ–µ–≤ –Ω–∏ –±—ã–ª–æ) —Å—Ö–ª–æ–ø–Ω–µ—Ç—Å—è –≤ –æ–¥–Ω—É –ª–∏–Ω–µ–π–Ω—É—é —Ä–µ–≥—Ä–µ—Å—Å–∏—é (W2 * (W1 * x) = W_new * x). –ù–µ–ª–∏–Ω–µ–π–Ω–æ—Å—Ç–∏ (ReLU, Sigmoid) –ø–æ–∑–≤–æ–ª—è—é—Ç –∞–ø–ø—Ä–æ–∫—Å–∏–º–∏—Ä–æ–≤–∞—Ç—å —Å–ª–æ–∂–Ω—ã–µ –∫—Ä–∏–≤—ã–µ.

**Q3: –ß—Ç–æ —Ç–∞–∫–æ–µ Vanishing Gradient –∏ –∫–∞–∫ –µ–≥–æ —Ä–µ—à–∏—Ç—å?**
> –ü—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ Sigmoid/Tanh –≤ –≥–ª—É–±–æ–∫–∏—Ö —Å–µ—Ç—è—Ö –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã —Å—Ç–∞–Ω–æ–≤—è—Ç—Å—è –æ—á–µ–Ω—å –º–∞–ª–µ–Ω—å–∫–∏–º–∏ –ø—Ä–∏ –ø–µ—Ä–µ–¥–∞—á–µ –Ω–∞–∑–∞–¥ (—É–º–Ω–æ–∂–µ–Ω–∏–µ —á–∏—Å–µ–ª < 1). –ü–µ—Ä–≤—ã–µ —Å–ª–æ–∏ –ø–µ—Ä–µ—Å—Ç–∞—é—Ç –æ–±—É—á–∞—Ç—å—Å—è.
> **–†–µ—à–µ–Ω–∏–µ:** –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å ReLU (–≥—Ä–∞–¥–∏–µ–Ω—Ç = 1), Batch Normalization, Residual Connections (ResNet).

**Q4: –ü–æ—á–µ–º—É Adam –æ–±—ã—á–Ω–æ –ª—É—á—à–µ SGD?**
> SGD –ø—Ä–æ—Å—Ç–æ –∏–¥–µ—Ç –≤ —Å—Ç–æ—Ä–æ–Ω—É –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞. Adam (Adaptive Moment Estimation) —É—á–∏—Ç—ã–≤–∞–µ—Ç:
> 1. –ò–Ω–µ—Ä—Ü–∏—é (Momentum) ‚Äî –ø—Ä–æ–¥–æ–ª–∂–∞–µ—Ç –¥–≤–∏–≥–∞—Ç—å—Å—è –≤ —Ç–æ–º –∂–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–∏.
> 2. –ê–¥–∞–ø—Ç–∏–≤–Ω–æ—Å—Ç—å —à–∞–≥–∞ (RMSProp) ‚Äî –¥–µ–ª–∞–µ—Ç —Ä–∞–∑–Ω—ã–µ —à–∞–≥–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.
> –≠—Ç–æ –¥–µ–ª–∞–µ—Ç —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å –±—ã—Å—Ç—Ä–µ–µ –∏ —É—Å—Ç–æ–π—á–∏–≤–µ–µ.

---

## –†–µ–∑—é–º–µ –º–æ–¥—É–ª—è

–ù–µ–π—Ä–æ—Å–µ—Ç–∏ ‚Äî —ç—Ç–æ **—É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–µ –∞–ø–ø—Ä–æ–∫—Å–∏–º–∞—Ç–æ—Ä—ã**:
- **Backpropagation**: chain rule –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤, vanishing/exploding gradients
- **Activation functions**: ReLU (default), Sigmoid (output binary), Softmax (output multi-class)
- **–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è**: He –¥–ª—è ReLU, Xavier –¥–ª—è sigmoid/tanh
- **–û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä—ã**: Adam (default, –±—ã—Å—Ç—Ä–æ), SGD+Momentum (–ª—É—á—à–µ generalization)

–ü–æ–Ω–∏–º–∞–Ω–∏–µ —ç—Ç–∏—Ö –æ—Å–Ω–æ–≤ –∫—Ä–∏—Ç–∏—á–Ω–æ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –ª—é–±—ã–º–∏ –Ω–µ–π—Ä–æ—Å–µ—Ç—è–º–∏!
