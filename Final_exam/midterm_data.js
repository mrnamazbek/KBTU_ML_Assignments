/**
 * MASTER DATA REPOSITORY: KBTU ML COURSE 2025
 * Structure: 
 * - question: String
 * - options: Array (Optional for 'fill')
 * - correct: Number (Index) or String (for 'fill')
 * - type: 'choice' | 'fill'
 * - topic: String (Used for Weak Topic Analysis)
 * - set: 'midterm' | 'quiz2' | 'quiz3' | 'quiz4' | 'bonus'
 * - optionExplanations: { en: Array, ru: Array }
 */

const masterQuizData = [
    // --- MIDTERM EXAM (50 Questions) ---
    {
        question: "The main goal of hyperparameter tuning is to:",
        options: ["Change data labels", "Add new features", "Measure bias and variance", "Optimize model settings before training"],
        correct: 3,
        topic: "Hyperparameters",
        set: "midterm",
        optionExplanations: {
            en: [
                "‚ùå **Labels**: This is data preparation, not model tuning.",
                "‚ùå **Features**: This is Feature Engineering.",
                "‚ùå **Measure**: This is Evaluation. Tuning *manages* bias/variance but isn't the measurement itself.",
                "‚úÖ **Optimize settings**: Correct! These are parameters like 'k' in k-NN that you set before training starts."
            ],
            ru: [
                "‚ùå **–ú–µ—Ç–∫–∏**: –≠—Ç–æ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö, –∞ –Ω–µ —Ç—é–Ω–∏–Ω–≥ –º–æ–¥–µ–ª–∏.",
                "‚ùå **–ü—Ä–∏–∑–Ω–∞–∫–∏**: –≠—Ç–æ –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (Feature Engineering).",
                "‚ùå **–ò–∑–º–µ—Ä–µ–Ω–∏–µ**: –≠—Ç–æ –æ—Ü–µ–Ω–∫–∞ (Evaluation). –¢—é–Ω–∏–Ω–≥ –ø–æ–º–æ–≥–∞–µ—Ç *—É–ø—Ä–∞–≤–ª—è—Ç—å* –æ—à–∏–±–∫–∞–º–∏, –Ω–æ —Å–∞–º –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –∏–∑–º–µ—Ä–µ–Ω–∏–µ–º.",
                "‚úÖ **–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è**: –í–µ—Ä–Ω–æ! –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã (–∫–∞–∫ 'k' –≤ k-NN) ‚Äî —ç—Ç–æ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –º—ã –∑–∞–¥–∞–µ–º –î–û –Ω–∞—á–∞–ª–∞ –æ–±—É—á–µ–Ω–∏—è."
            ]
        }
    },
    {
        question: "In the California Housing analysis, the correlation between median_income and median_house_value was found to be the strongest. This suggests that:",
        options: ["Median income is irrelevant", "Median income is a key feature with strong linear relationship", "Use only categorical features", "Correlation implies causation"],
        correct: 1,
        topic: "Feature Analysis",
        set: "midterm",
        optionExplanations: {
            en: [
                "‚ùå **Irrelevant**: High correlation makes it the *most* relevant predictor.",
                "‚úÖ **Key feature**: Correct! High Pearson correlation means houses tend to cost more where people earn more.",
                "‚ùå **Only categorical**: Ignoring strong numeric data would make the model less accurate.",
                "‚ùå **Causation**: ‚ö†Ô∏è Danger! Correlation shows they move together, but doesn't prove one *causes* the other."
            ],
            ru: [
                "‚ùå **–ù–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ**: –í—ã—Å–æ–∫–∞—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è –¥–µ–ª–∞–µ—Ç —ç—Ç–æ—Ç –ø—Ä–∏–∑–Ω–∞–∫ *—Å–∞–º—ã–º* –≤–∞–∂–Ω—ã–º.",
                "‚úÖ **–ö–ª—é—á–µ–≤–æ–π –ø—Ä–∏–∑–Ω–∞–∫**: –í–µ—Ä–Ω–æ! –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –ü–∏—Ä—Å–æ–Ω–∞ –≥–æ–≤–æ—Ä–∏—Ç, —á—Ç–æ —Ü–µ–Ω–∞ –¥–æ–º–∞ —Ä–∞—Å—Ç–µ—Ç –≤–º–µ—Å—Ç–µ —Å –¥–æ—Ö–æ–¥–æ–º –∂–∏—Ç–µ–ª–µ–π.",
                "‚ùå **–¢–æ–ª—å–∫–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏**: –ò–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–∏–ª—å–Ω—ã—Ö —á–∏—Å–ª–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Å–Ω–∏–∑–∏—Ç —Ç–æ—á–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏.",
                "‚ùå **–ü—Ä–∏—á–∏–Ω–Ω–æ—Å—Ç—å**: ‚ö†Ô∏è –û–ø–∞—Å–Ω–æ! –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å–≤—è–∑—å, –Ω–æ –Ω–µ –¥–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –æ–¥–Ω–æ —è–≤–ª—è–µ—Ç—Å—è –ø—Ä–∏—á–∏–Ω–æ–π –¥—Ä—É–≥–æ–≥–æ."
            ]
        }
    },
    {
        question: "Create a histogram for all numerical attributes in a DataFrame named housing: housing.__()",
        type: "fill",
        answer: "hist",
        topic: "Visualization",
        set: "midterm",
        optionExplanations: {
            en: ["‚úÖ **hist()**: Automatically generates a grid of histograms for every numerical column in Pandas."],
            ru: ["‚úÖ **hist()**: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å—Ç—Ä–æ–∏—Ç —Å–µ—Ç–∫—É –≥–∏—Å—Ç–æ–≥—Ä–∞–º–º –¥–ª—è –∫–∞–∂–¥–æ–π —á–∏—Å–ª–æ–≤–æ–π –∫–æ–ª–æ–Ω–∫–∏ –≤ Pandas."]
        }
    },
    {
        question: "The argument c in matplotlib's scatter plot defines:",
        options: ["Labels", "Color mapping", "Background", "Axis color"],
        correct: 1,
        topic: "Visualization",
        set: "midterm",
        optionExplanations: {
            en: [
                "‚ùå **Labels**: Labels use the `label` parameter.",
                "‚úÖ **Color mapping**: Correct! Used to represent a third variable (like price) via a color scale.",
                "‚ùå **Background**: Background is controlled via axis properties.",
                "‚ùå **Axis**: Axis colors use tick or spine parameters."
            ],
            ru: [
                "‚ùå **–ú–µ—Ç–∫–∏**: –î–ª—è —ç—Ç–æ–≥–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø–∞—Ä–∞–º–µ—Ç—Ä `label`.",
                "‚úÖ **–¶–≤–µ—Ç**: –í–µ—Ä–Ω–æ! –ü–æ–∑–≤–æ–ª—è–µ—Ç –æ—Ç–æ–±—Ä–∞–∑–∏—Ç—å —Ç—Ä–µ—Ç—å—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Ü–µ–Ω—É) —á–µ—Ä–µ–∑ —Ü–≤–µ—Ç–æ–≤—É—é –ø–∞–ª–∏—Ç—Ä—É.",
                "‚ùå **–§–æ–Ω**: –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç—Å—è —á–µ—Ä–µ–∑ —Å–≤–æ–π—Å—Ç–≤–∞ –æ—Å–µ–π.",
                "‚ùå **–û—Å–∏**: –¶–≤–µ—Ç –æ—Å–µ–π –º–µ–Ω—è–µ—Ç—Å—è —á–µ—Ä–µ–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Ç–∏–∫–æ–≤ –∏–ª–∏ –≥—Ä–∞–Ω–∏—Ü."
            ]
        }
    },
    {
        question: "In markdown, # before text represents:",
        options: ["Bold", "Hyperlink", "Heading", "Italics"],
        correct: 2,
        topic: "Tools",
        set: "midterm",
        optionExplanations: {
            en: ["‚ùå **Bold**: Uses `**` or `__`.", "‚ùå **Link**: Uses `[text](url)`.", "‚úÖ **Heading**: Correct! One `#` is H1, `##` is H2, and so on.", "‚ùå **Italics**: Uses `*` or `_`."],
            ru: ["‚ùå **–ñ–∏—Ä–Ω—ã–π**: –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è `**`.", "‚ùå **–°—Å—ã–ª–∫–∞**: –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è `[—Ç–µ–∫—Å—Ç](—Å—Å—ã–ª–∫–∞)`.", "‚úÖ **–ó–∞–≥–æ–ª–æ–≤–æ–∫**: –í–µ—Ä–Ω–æ! –° –æ–¥–Ω–æ–π `#` –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è –∑–∞–≥–æ–ª–æ–≤–æ–∫ –ø–µ—Ä–≤–æ–≥–æ —É—Ä–æ–≤–Ω—è.", "‚ùå **–ö—É—Ä—Å–∏–≤**: –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è `*`."]
        }
    },
    {
        question: "Association Rule Learning is best described as:",
        options: ["Finding co-occurrence patterns", "Predicting future values", "Identifying anomalies", "Clustering customers"],
        correct: 0,
        topic: "ML Types",
        set: "midterm",
        optionExplanations: {
            en: [
                "‚úÖ **Patterns**: Correct! Finds 'if X, then Y' relationships in transaction data (Market Basket Analysis).",
                "‚ùå **Predicting**: This is Regression/Forecasting.",
                "‚ùå **Anomalies**: This is Anomaly Detection (looking for RARE events).",
                "‚ùå **Clustering**: Groups people/objects based on similarity, not rules between items."
            ],
            ru: [
                "‚úÖ **–ü–∞—Ç—Ç–µ—Ä–Ω—ã**: –í–µ—Ä–Ω–æ! –ò—â–µ—Ç —Å–≤—è–∑–∏ —Ç–∏–ø–∞ '–µ—Å–ª–∏ X, —Ç–æ Y' –≤ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—è—Ö (–∞–Ω–∞–ª–∏–∑ –ø–æ–∫—É–ø–∞—Ç–µ–ª—å—Å–∫–æ–π –∫–æ—Ä–∑–∏–Ω—ã).",
                "‚ùå **–ü—Ä–æ–≥–Ω–æ–∑**: –≠—Ç–æ —Ä–µ–≥—Ä–µ—Å—Å–∏—è –∏–ª–∏ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä—è–¥–æ–≤.",
                "‚ùå **–ê–Ω–æ–º–∞–ª–∏–∏**: –≠—Ç–æ –ø–æ–∏—Å–∫ —Ä–µ–¥–∫–∏—Ö –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–π (Anomaly Detection).",
                "‚ùå **–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è**: –ì—Ä—É–ø–ø–∏—Ä—É–µ—Ç –æ–±—ä–µ–∫—Ç—ã –ø–æ —Å—Ö–æ–∂–µ—Å—Ç–∏, –∞ –Ω–µ –∏—â–µ—Ç –ø—Ä–∞–≤–∏–ª–∞ –º–µ–∂–¥—É –ø—Ä–µ–¥–º–µ—Ç–∞–º–∏."
            ]
        }
    },
    {
        question: "The MAE (Mean Absolute Error) measures:",
        options: ["Squared error", "Average absolute difference", "Variance", "Root squared residuals"],
        correct: 1,
        topic: "Metrics",
        set: "midterm",
        optionExplanations: {
            en: [
                "‚ùå **Squared**: That's MSE (Mean Squared Error).",
                "‚úÖ **Absolute**: Correct! It takes the average of the absolute errors ($|y_{true} - y_{pred}|$). It is robust to outliers.",
                "‚ùå **Variance**: Variance measures spread, not error magnitude.",
                "‚ùå **Root**: That's RMSE."
            ],
            ru: [
                "‚ùå **–ö–≤–∞–¥—Ä–∞—Ç**: –≠—Ç–æ MSE.",
                "‚úÖ **–ê–±—Å–æ–ª—é—Ç–Ω–∞—è**: –í–µ—Ä–Ω–æ! –£—Å—Ä–µ–¥–Ω—è–µ—Ç –º–æ–¥—É–ª–∏ –æ—à–∏–±–æ–∫. –û–Ω–∞ –º–µ–Ω–µ–µ —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–∞ –∫ –≤—ã–±—Ä–æ—Å–∞–º, —á–µ–º —Ä–µ–≥—Ä–µ—Å—Å–∏—è —Å –∫–≤–∞–¥—Ä–∞—Ç–∞–º–∏.",
                "‚ùå **–î–∏—Å–ø–µ—Ä—Å–∏—è**: –ò–∑–º–µ—Ä—è–µ—Ç —Ä–∞–∑–±—Ä–æ—Å –¥–∞–Ω–Ω—ã—Ö, –∞ –Ω–µ –≤–µ–ª–∏—á–∏–Ω—É –æ—à–∏–±–∫–∏.",
                "‚ùå **–ö–æ—Ä–µ–Ω—å**: –≠—Ç–æ RMSE."
            ]
        }
    },
    {
        question: "In supervised learning, the model never uses labeled data during training.",
        options: ["True", "False"],
        correct: 1,
        topic: "Basics",
        set: "midterm",
        optionExplanations: {
            en: ["‚ùå **True**: No, Supervised Learning REQUIRES labels (answers) to learn.", "‚úÖ **False**: Correct! Labels are the teacher's guide in Supervised Learning."],
            ru: ["‚ùå **–ü—Ä–∞–≤–¥–∞**: –ù–µ—Ç, –æ–±—É—á–µ–Ω–∏–µ —Å —É—á–∏—Ç–µ–ª–µ–º –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û —Ç—Ä–µ–±—É–µ—Ç –º–µ—Ç–æ–∫ (–æ—Ç–≤–µ—Ç–æ–≤).", "‚úÖ **–õ–æ–∂—å**: –í–µ—Ä–Ω–æ! –ú–µ—Ç–∫–∏ ‚Äî —ç—Ç–æ –ø–æ–¥—Å–∫–∞–∑–∫–∏ —É—á–∏—Ç–µ–ª—è –≤ —ç—Ç–æ–º —Ç–∏–ø–µ –æ–±—É—á–µ–Ω–∏—è."]
        }
    },
    {
        question: "In the example of detecting tumors in brain scans, which ML task is used?",
        options: ["Semantic segmentation", "Regression", "Reinforcement learning", "Clustering"],
        correct: 0,
        topic: "Applications",
        set: "midterm",
        optionExplanations: {
            en: [
                "‚úÖ **Segmentation**: Correct! This involves identifying which pixels in an image belong to a tumor.",
                "‚ùå **Regression**: Predicts a continuous number (like price), not shapes on an image.",
                "‚ùå **Reinforcement**: Learns through trial and error reward systems (like a robot in a maze).",
                "‚ùå **Clustering**: Often lacks predefined labels; tumors require precise, labeled medical data."
            ],
            ru: [
                "‚úÖ **–°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è**: –í–µ—Ä–Ω–æ! –≠—Ç–æ –ø—Ä–æ—Ü–µ—Å—Å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–æ–≥–æ, –∫–∞–∫–∏–µ –∏–º–µ–Ω–Ω–æ –ø–∏–∫—Å–µ–ª–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –æ—Ç–Ω–æ—Å—è—Ç—Å—è –∫ –æ–ø—É—Ö–æ–ª–∏.",
                "‚ùå **–†–µ–≥—Ä–µ—Å—Å–∏—è**: –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç —á–∏—Å–ª–æ (—Ü–µ–Ω—É, –≤–µ—Å), –∞ –Ω–µ —Ñ–æ—Ä–º—ã –Ω–∞ –∫–∞—Ä—Ç–∏–Ω–∫–µ.",
                "‚ùå **Reinforcement**: –û–±—É—á–µ–Ω–∏–µ –∑–∞ –Ω–∞–≥—Ä–∞–¥—ã (–∫–∞–∫ –ò–ò –≤ –∏–≥—Ä–∞—Ö).",
                "‚ùå **–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è**: –û–±—ã—á–Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –±–µ–∑ —É—á–∏—Ç–µ–ª–µ–π, —Ç–æ–≥–¥–∞ –∫–∞–∫ –≤ –º–µ–¥–∏—Ü–∏–Ω–µ –Ω—É–∂–Ω—ã —Ç–æ—á–Ω—ã–µ —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã."
            ]
        }
    },
    {
        question: "To calculate correlation matrix in pandas: corr_matrix = df.___()",
        type: "fill",
        answer: "corr",
        topic: "Pandas",
        set: "midterm",
        optionExplanations: {
            en: ["‚úÖ **corr()**: Fast method to compute the pairwise correlation of columns."],
            ru: ["‚úÖ **corr()**: –ë—ã—Å—Ç—Ä—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –º–µ–∂–¥—É –≤—Å–µ–º–∏ –∫–æ–ª–æ–Ω–∫–∞–º–∏ —Ç–∞–±–ª–∏—Ü—ã."]
        }
    },
    {
        question: "The scatterplot matrix for feature relationships is created using:",
        options: ["sns.pairplot()", "plt.show()", "df.plot_matrix()", "np.plot()"],
        correct: 0,
        topic: "Visualization",
        set: "midterm",
        optionExplanations: {
            en: [
                "‚úÖ **pairplot()**: Correct! A Seaborn function that plots every numeric feature against every other numeric feature.",
                "‚ùå **show()**: Only displays current plots, doesn't create a matrix.",
                "‚ùå **plot_matrix()**: Not a standard Pandas method.",
                "‚ùå **np.plot()**: Numpy doesn't have a high-level plotting method for DataFrames."
            ],
            ru: [
                "‚úÖ **pairplot()**: –í–µ—Ä–Ω–æ! –§—É–Ω–∫—Ü–∏—è Seaborn, –∫–æ—Ç–æ—Ä–∞—è —Å—Ç—Ä–æ–∏—Ç –≥—Ä–∞—Ñ–∏–∫–∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –∫–∞–∂–¥–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞ –æ—Ç –∫–∞–∂–¥–æ–≥–æ.",
                "‚ùå **show()**: –¢–æ–ª—å–∫–æ –≤—ã–≤–æ–¥–∏—Ç –≥—Ä–∞—Ñ–∏–∫ –Ω–∞ —ç–∫—Ä–∞–Ω, –Ω–æ –Ω–µ —Å–æ–∑–¥–∞–µ—Ç –µ–≥–æ.",
                "‚ùå **plot_matrix()**: –ù–µ —è–≤–ª—è–µ—Ç—Å—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–º –º–µ—Ç–æ–¥–æ–º –≤ Pandas.",
                "‚ùå **np.plot()**: –£ Numpy –Ω–µ—Ç —Ç–∞–∫–∏—Ö –≤—ã—Å–æ–∫–æ—É—Ä–æ–≤–Ω–µ–≤—ã—Ö –º–µ—Ç–æ–¥–æ–≤ –¥–ª—è —Ç–∞–±–ª–∏—Ü."
            ]
        }
    },
    {
        question: "Which of the following pairs correctly matches ML types?",
        options: ["Image classification -> Supervised", "Spam filtering -> Unsupervised", "Market segmentation -> Reinforcement", "Speech recognition -> Clustering"],
        correct: 0,
        topic: "Definitions",
        set: "midterm",
        optionExplanations: {
            en: [
                "‚úÖ **Classification**: Correct! We have labeled images (Cat/Dog) used for training.",
                "‚ùå **Spam**: This is Supervised (Spam/Not Spam labels).",
                "‚ùå **Segmentation**: This is Unsupervised (Clustering users).",
                "‚ùå **Speech**: Typically Supervised (Sound -> Text labels)."
            ],
            ru: [
                "‚úÖ **–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è**: –í–µ—Ä–Ω–æ! –£ –Ω–∞—Å –µ—Å—Ç—å —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã–µ —Ñ–æ—Ç–æ (–ö–æ—Ç/–ü—ë—Å) –¥–ª—è –æ–±—É—á–µ–Ω–∏—è.",
                "‚ùå **–°–ø–∞–º**: –≠—Ç–æ –æ–±—É—á–µ–Ω–∏–µ —Å —É—á–∏—Ç–µ–ª–µ–º (–º–µ—Ç–∫–∏ –°–ø–∞–º/–ù–µ —Å–ø–∞–º).",
                "‚ùå **–°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è**: –≠—Ç–æ –æ–±—É—á–µ–Ω–∏–µ –ë–ï–ó —É—á–∏—Ç–µ–ª—è (–∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è).",
                "‚ùå **–†–µ—á—å**: –û–±—ã—á–Ω–æ —Å —É—á–∏—Ç–µ–ª–µ–º (–ó–≤—É–∫ -> –¢–µ–∫—Å—Ç)."
            ]
        }
    },
    {
        question: "In the happiness vs. GDP example, the cost function measures:",
        options: ["Model error", "Accuracy", "Parameters count", "Samples count"],
        correct: 0,
        topic: "Math",
        set: "midterm",
        optionExplanations: {
            en: [
                "‚úÖ **Model error**: Correct! The cost function computes the 'distance' between the line and the actual data points.",
                "‚ùå **Accuracy**: Accuracy is usually used in classification; cost functions typically use error magnitude.",
                "‚ùå **Parameters**: Bias and weight are tuned *using* the cost function, they aren't measured by it.",
                "‚ùå **Samples**: This is just the dataset size."
            ],
            ru: [
                "‚úÖ **–û—à–∏–±–∫–∞ –º–æ–¥–µ–ª–∏**: –í–µ—Ä–Ω–æ! –§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å –≤—ã—á–∏—Å–ª—è–µ—Ç ¬´—Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ¬ª –º–µ–∂–¥—É –ª–∏–Ω–∏–µ–π –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∏ —Ä–µ–∞–ª—å–Ω—ã–º–∏ —Ç–æ—á–∫–∞–º–∏.",
                "‚ùå **–¢–æ—á–Ω–æ—Å—Ç—å (Accuracy)**: –û–±—ã—á–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏, –∞ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –∏–∑–º–µ—Ä—è—é—Ç –≤–µ–ª–∏—á–∏–Ω—É –æ—à–∏–±–∫–∏.",
                "‚ùå **–ü–∞—Ä–∞–º–µ—Ç—Ä—ã**: –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏ –ø–æ–¥–±–∏—Ä–∞—é—Ç—Å—è *—Å –ø–æ–º–æ—â—å—é* —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å.",
                "‚ùå **–ü—Ä–∏–º–µ—Ä—ã**: –≠—Ç–æ –ø—Ä–æ—Å—Ç–æ —Ä–∞–∑–º–µ—Ä –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö."
            ]
        }
    },
    {
        question: "Create a scatter plot in matplotlib: plt.__(x, y)",
        type: "fill",
        answer: "scatter",
        topic: "Visualization",
        set: "midterm",
        optionExplanations: { en: ["‚úÖ **scatter()**: Standard function for creating point charts."], ru: ["‚úÖ **scatter()**: –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Ç–æ—á–µ—á–Ω—ã—Ö –¥–∏–∞–≥—Ä–∞–º–º."] }
    },
    {
        question: "Underfitting occurs when:",
        options: ["Model is too simple", "Model is too complex", "Data is perfect", "Test data is missing"],
        correct: 0,
        topic: "Bias-Variance",
        set: "midterm",
        optionExplanations: {
            en: [
                "‚úÖ **Too simple**: Correct! High Bias. Model fails to learn the basic patterns.",
                "‚ùå **Complex**: This causes Overfitting (High Variance).",
                "‚ùå **Perfect**: If data is perfect and model still fails, it's underfitting, but the data itself is fine.",
                "‚ùå **Missing**: This prevents evaluation but doesn't cause underfitting."
            ],
            ru: [
                "‚úÖ **–°–ª–∏—à–∫–æ–º –ø—Ä–æ—Å—Ç–∞—è**: –í–µ—Ä–Ω–æ! –í—ã—Å–æ–∫–æ–µ —Å–º–µ—â–µ–Ω–∏–µ (High Bias). –ú–æ–¥–µ–ª—å –Ω–µ –≤–∏–¥–∏—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∞–Ω–Ω—ã—Ö.",
                "‚ùå **–°–ª–æ–∂–Ω–∞—è**: –≠—Ç–æ –ø—Ä–∏—á–∏–Ω–∞ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è (Overfitting).",
                "‚ùå **–ò–¥–µ–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ**: –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã–µ —Ö–æ—Ä–æ—à–∏, –∞ –º–æ–¥–µ–ª—å ‚Äî –Ω–µ—Ç, –∑–Ω–∞—á–∏—Ç –æ–Ω–∞ —Å–ª–∏—à–∫–æ–º —Å–ª–∞–±–∞—è.",
                "‚ùå **–û—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –¥–∞–Ω–Ω—ã—Ö**: –≠—Ç–æ –º–µ—à–∞–µ—Ç –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –º–æ–¥–µ–ª—å, –Ω–æ –Ω–µ –¥–µ–ª–∞–µ—Ç –µ—ë —Å–ª–∞–±–æ–π —Å–∞–º–∞ –ø–æ —Å–µ–±–µ."
            ]
        }
    },
    {
        question: "According to Tom Mitchell's definition, a program learns from experience E with respect to task T and performance measure P if:",
        options: ["Accuracy decreases", "Performance on T improves with E", "Uses labeled data", "High power"],
        correct: 1,
        topic: "Definitions",
        set: "midterm",
        optionExplanations: {
            en: ["‚úÖ **Improves**: Correct! The essence of learning is that the score P on task T gets better as we provide more experience E.", "‚ùå **Decreases**: That's the opposite of learning!"],
            ru: ["‚úÖ **–£–ª—É—á—à–∞–µ—Ç—Å—è**: –í–µ—Ä–Ω–æ! –°—É—Ç—å –æ–±—É—á–µ–Ω–∏—è –≤ —Ç–æ–º, —á—Ç–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç P –≤ –∑–∞–¥–∞—á–µ T —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è –ª—É—á—à–µ —Å –Ω–∞–∫–æ–ø–ª–µ–Ω–∏–µ–º –æ–ø—ã—Ç–∞ E.", "‚ùå **–£—Ö—É–¥—à–∞–µ—Ç—Å—è**: –≠—Ç–æ –ø–æ–ª–Ω–∞—è –ø—Ä–æ—Ç–∏–≤–æ–ø–æ–ª–æ–∂–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—é!"]
        }
    },
    {
        question: "In supervised learning, the algorithm tries to find patterns without any target labels.",
        options: ["True", "False"],
        correct: 1,
        topic: "Basics",
        set: "midterm",
        optionExplanations: {
            en: ["‚úÖ **False**: That describes *Unsupervised* Learning. Supervised Learning always needs labels."],
            ru: ["‚úÖ **–õ–æ–∂—å**: –≠—Ç–æ –æ–ø–∏—Å–∞–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è *–ë–µ–∑ —É—á–∏—Ç–µ–ª—è*. –û–±—É—á–µ–Ω–∏–µ '—Å —É—á–∏—Ç–µ–ª–µ–º' –≤—Å–µ–≥–¥–∞ —Ç—Ä–µ–±—É–µ—Ç –º–µ—Ç–æ–∫."]
        }
    },
    {
        question: "Which of the following is an example of supervised learning?",
        options: ["Predicting house prices using labeled data", "Grouping customers", "Detecting anomalies", "PCA"],
        correct: 0,
        topic: "Applications",
        set: "midterm",
        optionExplanations: {
            en: [
                "‚úÖ **House prices**: Correct! We have historical prices (labels) to guide the model.",
                "‚ùå **Grouping/Anomalies/PCA**: All these are typically Unsupervised Learning tasks."
            ],
            ru: [
                "‚úÖ **–¶–µ–Ω—ã –Ω–∞ –¥–æ–º–∞**: –í–µ—Ä–Ω–æ! –£ –Ω–∞—Å –µ—Å—Ç—å –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ —Ü–µ–Ω—ã (–º–µ—Ç–∫–∏) –¥–ª—è –æ–±—É—á–µ–Ω–∏—è.",
                "‚ùå **–ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞/–ê–Ω–æ–º–∞–ª–∏–∏/PCA**: –í—Å–µ —ç—Ç–æ –ø—Ä–∏–º–µ—Ä—ã –∑–∞–¥–∞—á –æ–±—É—á–µ–Ω–∏—è –ë–ï–ó —É—á–∏—Ç–µ–ª—è."
            ]
        }
    },
    {
        question: "The k in k-Nearest Neighbors represents:",
        options: ["Model complexity", "Number of features", "Number of clusters", "Number of neighbors used for prediction"],
        correct: 3,
        topic: "Models",
        set: "midterm",
        optionExplanations: {
            en: ["‚úÖ **Neighbors**: Correct! It looks at the nearest $k$ points in the feature space to make a decision."],
            ru: ["‚úÖ **–°–æ—Å–µ–¥–∏**: –í–µ—Ä–Ω–æ! –ú–æ–¥–µ–ª—å —Å–º–æ—Ç—Ä–∏—Ç –Ω–∞ $k$ –±–ª–∏–∂–∞–π—à–∏—Ö —Ç–æ—á–µ–∫ –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏—è."]
        }
    },
    {
        question: "Batch learning means:",
        options: ["Continuous learning", "Learning from data in one large step", "Deletes old data", "Cannot be retrained"],
        correct: 1,
        topic: "ML Types",
        set: "midterm",
        optionExplanations: {
            en: [
                "‚úÖ **Large step**: Correct! It trains on all available data at once and is then deployed (no learning while online).",
                "‚ùå **Continuous**: This is Online/Incremental Learning."
            ],
            ru: [
                "‚úÖ **–û–¥–∏–Ω —à–∞–≥**: –í–µ—Ä–Ω–æ! –ú–æ–¥–µ–ª—å –æ–±—É—á–∞–µ—Ç—Å—è –Ω–∞ –≤—Å–µ—Ö –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Å—Ä–∞–∑—É, –∞ –∑–∞—Ç–µ–º –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –±–µ–∑ –¥–æ–æ–±—É—á–µ–Ω–∏—è –≤ '–∂–∏–≤–æ–º' —Ä–µ–∂–∏–º–µ.",
                "‚ùå **–ù–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–µ**: –≠—Ç–æ Online –∏–ª–∏ Incremental Learning."
            ]
        }
    },
    {
        question: "To fit a k-Nearest Neighbors model: from sclearn.neighbors import ___",
        type: "fill",
        answer: "KNeighborsClassifier",
        topic: "Scikit-Learn",
        set: "midterm",
        optionExplanations: { en: ["‚úÖ **KNeighborsClassifier**: The standard class name in scikit-learn for k-NN Classification."], ru: ["‚úÖ **KNeighborsClassifier**: –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –∏–º—è –∫–ª–∞—Å—Å–∞ –≤ sklearn –¥–ª—è k-NN –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏."] }
    },
    {
        question: "The accuracy of a kNN model in scikit-learn can be computed using:",
        options: ["knn.fit()", "knn.test()", "knn.validate()", "knn.score()"],
        correct: 3,
        topic: "Scikit-Learn",
        set: "midterm",
        optionExplanations: {
            en: [
                "‚ùå **fit()**: Used for training.",
                "‚úÖ **score()**: Correct! It returns the mean accuracy on the given test data and labels.",
                "‚ùå **test()/validate()**: These aren't standard estimator methods in scikit-learn."
            ],
            ru: [
                "‚ùå **fit()**: –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è.",
                "‚úÖ **score()**: –í–µ—Ä–Ω–æ! –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ä–µ–¥–Ω—é—é —Ç–æ—á–Ω–æ—Å—Ç—å (accuracy) –Ω–∞ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.",
                "‚ùå **test()/validate()**: –¢–∞–∫–∏—Ö —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤ —É –æ–±—ä–µ–∫—Ç–æ–≤-–æ—Ü–µ–Ω—â–∏–∫–æ–≤ –≤ sklearn –Ω–µ—Ç."
            ]
        }
    },
    {
        question: "Unsupervised learning is mainly used to:",
        options: ["Predict known outcomes", "Classify labeled data", "Train with rewards", "Find hidden patterns without labels"],
        correct: 3,
        topic: "Definitions",
        set: "midterm",
        optionExplanations: {
            en: [
                "‚úÖ **Hidden patterns**: Correct! Used for clustering, dimensionality reduction, and association rules.",
                "‚ùå **Predict/Classify**: This is Supervised Learning.",
                "‚ùå **Rewards**: This is Reinforcement Learning."
            ],
            ru: [
                "‚úÖ **–°–∫—Ä—ã—Ç—ã–µ –∑–∞–∫–æ–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç–∏**: –í–µ—Ä–Ω–æ! –ü—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –∏ –ø–æ–∏—Å–∫–∞ –∞—Å—Å–æ—Ü–∏–∞—Ü–∏–π.",
                "‚ùå **–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ/–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è**: –≠—Ç–æ –∑–∞–¥–∞—á–∏ –æ–±—É—á–µ–Ω–∏—è –° —É—á–∏—Ç–µ–ª–µ–º.",
                "‚ùå **–ù–∞–≥—Ä–∞–¥—ã**: –≠—Ç–æ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º (Reinforcement)."
            ]
        }
    },
    {
        question: "The main advantage of using pipelines is:",
        options: ["Reproducible workflows", "Removing model dependencies", "Training models faster", "Manual data processing"],
        correct: 0,
        topic: "Tools",
        set: "midterm",
        optionExplanations: {
            en: [
                "‚úÖ **Reproducible**: Correct! It prevents data leakage and ensures preprocessing is identical for train and test sets.",
                "‚ùå **Manual**: The whole point of pipelines is to AUTOMATE processing."
            ],
            ru: [
                "‚úÖ **–í–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç—å**: –í–µ—Ä–Ω–æ! –ü–∞–π–ø–ª–∞–π–Ω—ã –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞—é—Ç —É—Ç–µ—á–∫—É –¥–∞–Ω–Ω—ã—Ö –∏ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä—É—é—Ç –≤—Å—é —Ü–µ–ø–æ—á–∫—É –æ–±—Ä–∞–±–æ—Ç–∫–∏.",
                "‚ùå **–†—É—á–Ω–∞—è**: –°—É—Ç—å –ø–∞–π–ø–ª–∞–π–Ω–æ–≤ ‚Äî –ê–í–¢–û–ú–ê–¢–ò–ó–ê–¶–ò–Ø, –∞ –Ω–µ —Ä—É—á–Ω–∞—è —Ä–∞–±–æ—Ç–∞."
            ]
        }
    },
    {
        question: "To load the Iris dataset from scikit-learn: from sklearn.datasets import ___",
        type: "fill",
        answer: "load_iris",
        topic: "Scikit-Learn",
        set: "midterm",
        optionExplanations: { en: ["‚úÖ **load_iris**: Standard function to fetch the classic flower dataset."], ru: ["‚úÖ **load_iris**: –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ —Å —Ü–≤–µ—Ç–∞–º–∏ –∏—Ä–∏—Å–∞."] }
    },
    {
        question: "Which of the following metrics penalizes large errors more heavily?",
        options: ["RMSE", "MAE", "R2", "Accuracy"],
        correct: 0,
        topic: "Metrics",
        set: "midterm",
        optionExplanations: {
            en: [
                "‚úÖ **RMSE**: Correct! It uses the square of errors, making outliers much more 'expensive' for the model.",
                "‚ùå **MAE**: Uses absolute values; treats all errors proportionally.",
                "‚ùå **R2/Accuracy**: Don't directly measure error magnitude in the same way."
            ],
            ru: [
                "‚úÖ **RMSE**: –í–µ—Ä–Ω–æ! –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –∫–æ—Ä–µ–Ω—å –∏–∑ —Å—É–º–º—ã –∫–≤–∞–¥—Ä–∞—Ç–æ–≤ –æ—à–∏–±–æ–∫, —á—Ç–æ –¥–µ–ª–∞–µ—Ç –≤—ã–±—Ä–æ—Å—ã –æ—á–µ–Ω—å '–¥–æ—Ä–æ–≥–∏–º–∏'.",
                "‚ùå **MAE**: –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –º–æ–¥—É–ª—å –æ—à–∏–±–∫–∏, –ø–æ—ç—Ç–æ–º—É –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –∫–æ –≤—Å–µ–º –æ—à–∏–±–∫–∞–º –ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª—å–Ω–æ.",
                "‚ùå **R2/Accuracy**: –ù–µ –∏–∑–º–µ—Ä—è—é—Ç –≤–µ–ª–∏—á–∏–Ω—É –æ—à–∏–±–∫–∏ –Ω–∞–ø—Ä—è–º—É—é —Ç–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º."
            ]
        }
    },
    {
        question: "In stratified sampling, each stratum must appear in the sample in the same proportion as in the population.",
        options: ["True", "False"],
        correct: 0,
        topic: "Sampling",
        set: "midterm",
        optionExplanations: {
            en: ["‚úÖ **True**: Correct! This is vital for small datasets to ensure the test set is representative."],
            ru: ["‚úÖ **–ü—Ä–∞–≤–¥–∞**: –í–µ—Ä–Ω–æ! –≠—Ç–æ –∫—Ä–∏—Ç–∏—á–Ω–æ –¥–ª—è –Ω–µ–±–æ–ª—å—à–∏—Ö –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö, —á—Ç–æ–±—ã —Ç–µ—Å—Ç –±—ã–ª –ø–æ—Ö–æ–∂ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã–π –º–∏—Ä."]
        }
    },
    {
        question: "During stratified sampling in the California Housing dataset, an additional column income_cat was created using pd.cut(). The main reason was to:",
        options: ["Detect missing values", "Convert numeric income to categorical bins", "Simplify correlations", "Improve model scaling"],
        correct: 1,
        topic: "Sampling",
        set: "midterm",
        optionExplanations: {
            en: [
                "‚úÖ **Bins**: Correct! Stratification needs categories (strata). We binned income to perform proportional sampling.",
                "‚ùå **Missing values**: `pd.cut` doesn't help with detection."
            ],
            ru: [
                "‚úÖ **–ë–∏–Ω—ã**: –í–µ—Ä–Ω–æ! –î–ª—è —Å—Ç—Ä–∞—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏ –Ω—É–∂–Ω—ã –∫–∞—Ç–µ–≥–æ—Ä–∏–∏. –ú—ã —Ä–∞–∑–±–∏–ª–∏ –¥–æ—Ö–æ–¥ –Ω–∞ –≥—Ä—É–ø–ø—ã –¥–ª—è –ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ –æ—Ç–±–æ—Ä–∞.",
                "‚ùå **–ü—Ä–æ–ø—É—Å–∫–∏**: `pd.cut` –Ω–µ –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö."
            ]
        }
    },
    {
        question: "The describe() method in pandas provides:",
        options: ["Only column names", "Graphical visualization", "Basic summary statistics", "Correlation coefficients"],
        correct: 2,
        topic: "Pandas",
        set: "midterm",
        optionExplanations: {
            en: [
                "‚úÖ **Statistics**: Correct! It gives count, mean, std, min, max, and quartiles.",
                "‚ùå **Visualization**: That would be `.hist()` or `.plot()`."
            ],
            ru: [
                "‚úÖ **–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞**: –í–µ—Ä–Ω–æ! –í—ã–¥–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ, —Å—Ä–µ–¥–Ω–µ–µ, —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ, –º–∏–Ω–∏–º—É–º, –º–∞–∫—Å–∏–º—É–º –∏ –∫–≤–∞—Ä—Ç–∏–ª–∏.",
                "‚ùå **–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è**: –≠—Ç–æ –¥–µ–ª–∞—é—Ç –º–µ—Ç–æ–¥—ã –≤—Ä–æ–¥–µ `.hist()` –∏–ª–∏ `.plot()`."
            ]
        }
    },
    {
        question: "Predict using a trained linear regression model: y_pred = model.___(X_test)",
        type: "fill",
        answer: "predict",
        topic: "Scikit-Learn",
        set: "midterm",
        optionExplanations: { en: ["‚úÖ **predict()**: Universal method in sklearn to generate outputs for new input data."], ru: ["‚úÖ **predict()**: –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –º–µ—Ç–æ–¥ –≤ sklearn –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –Ω–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö."] }
    },
    {
        question: "Machine Learning is best described as:",
        options: ["Explicit rules", "Programming computers to learn from data", "Statistics only", "Manual labeling"],
        correct: 1,
        topic: "Definitions",
        set: "midterm",
        optionExplanations: {
            en: ["‚úÖ **Learn from data**: Correct! Unlike traditional programming, the computer finds the 'rules' itself by looking at examples."],
            ru: ["‚úÖ **–û–±—É—á–µ–Ω–∏–µ –Ω–∞ –¥–∞–Ω–Ω—ã—Ö**: –í–µ—Ä–Ω–æ! –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç –æ–±—ã—á–Ω–æ–≥–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è, –∫–æ–º–ø—å—é—Ç–µ—Ä —Å–∞–º –Ω–∞—Ö–æ–¥–∏—Ç –ø—Ä–∞–≤–∏–ª–∞, –≥–ª—è–¥—è –Ω–∞ –ø—Ä–∏–º–µ—Ä—ã."]
        }
    },
    {
        question: "Which library provides the KNeighborsClassifier?",
        options: ["sklearn.metrics", "numpy", "pandas", "sklearn.neighbors"],
        correct: 3,
        topic: "Scikit-Learn",
        set: "midterm",
        optionExplanations: { en: ["‚úÖ **sklearn.neighbors**: This module contains neighbor-based algorithms."], ru: ["‚úÖ **sklearn.neighbors**: –≠—Ç–æ—Ç –º–æ–¥—É–ª—å —Å–æ–¥–µ—Ä–∂–∏—Ç –∞–ª–≥–æ—Ä–∏—Ç–º—ã, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞ –ø–æ–∏—Å–∫–µ –±–ª–∏–∂–∞–π—à–∏—Ö —Å–æ—Å–µ–¥–µ–π."] }
    },
    {
        question: "To make a prediction for a new sample X_new: prediction = knn.___(X_new)",
        type: "fill",
        answer: "predict",
        topic: "Scikit-Learn",
        set: "midterm",
        optionExplanations: { en: ["‚úÖ **predict()**: The standard method for applying the model to new samples."], ru: ["‚úÖ **predict()**: –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –∫ –Ω–æ–≤—ã–º –ø—Ä–∏–º–µ—Ä–∞–º."] }
    },
    {
        question: "Overfitting means the model performs well on the training set but poorly on unseen data.",
        options: ["True", "False"],
        correct: 0,
        topic: "Bias-Variance",
        set: "midterm",
        optionExplanations: {
            en: ["‚úÖ **True**: Correct! High Variance. The model memorized the training noise and can't generalize."],
            ru: ["‚úÖ **–ü—Ä–∞–≤–¥–∞**: –í–µ—Ä–Ω–æ! –í—ã—Å–æ–∫–∞—è –¥–∏—Å–ø–µ—Ä—Å–∏—è (High Variance). –ú–æ–¥–µ–ª—å –∑–∞–∑—É–±—Ä–∏–ª–∞ —à—É–º –∏–∑ –æ–±—É—á–µ–Ω–∏—è –∏ –Ω–µ –ø–æ–Ω–∏–º–∞–µ—Ç –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ."]
        }
    },
    {
        question: "The command model.fit(X, y) in scikit-learn:",
        options: ["Trains the model on data", "Tests the model", "Displays accuracy", "Splits data"],
        correct: 0,
        topic: "Scikit-Learn",
        set: "midterm",
        optionExplanations: {
            en: ["‚úÖ **Trains**: Correct! `fit` is where the algorithm 'learns' from features $X$ and labels $y$."],
            ru: ["‚úÖ **–û–±—É—á–∞–µ—Ç**: –í–µ—Ä–Ω–æ! –ú–µ—Ç–æ–¥ `fit` ‚Äî —ç—Ç–æ –º–æ–º–µ–Ω—Ç, –∫–æ–≥–¥–∞ –∞–ª–≥–æ—Ä–∏—Ç–º ¬´—É—á–∏—Ç—Å—è¬ª –Ω–∞ –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö $X$ –∏ –æ—Ç–≤–µ—Ç–∞—Ö $y$."]
        }
    },
    {
        question: "In supervised learning, the label set is also called the:",
        options: ["Predictor", "Feature", "Target", "Variable"],
        correct: 2,
        topic: "Terminology",
        set: "midterm",
        optionExplanations: {
            en: [
                "‚úÖ **Target**: Correct! Usually denoted as $y$. It's what we want to predict.",
                "‚ùå **Feature/Predictor**: That's $X$ (the inputs)."
            ],
            ru: [
                "‚úÖ **Target (–¶–µ–ª—å)**: –í–µ—Ä–Ω–æ! –û–±—ã—á–Ω–æ –æ–±–æ–∑–Ω–∞—á–∞–µ—Ç—Å—è —á–µ—Ä–µ–∑ $y$. –≠—Ç–æ —Ç–æ, —á—Ç–æ –º—ã —Ö–æ—Ç–∏–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å.",
                "‚ùå **–ü—Ä–∏–∑–Ω–∞–∫/–ü—Ä–µ–¥–∏–∫—Ç–æ—Ä**: –≠—Ç–æ –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ ($X$)."
            ]
        }
    },
    {
        question: "In regression, the target variable is typically:",
        options: ["Category", "Label index", "Continuous numeric value", "Probability"],
        correct: 2,
        topic: "Definitions",
        set: "midterm",
        optionExplanations: {
            en: [
                "‚úÖ **Continuous**: Correct! Regression is used for predicting quantities (like $253,401.50).",
                "‚ùå **Category**: This would be Classification."
            ],
            ru: [
                "‚úÖ **–ù–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–µ —á–∏—Å–ª–æ**: –í–µ—Ä–Ω–æ! –†–µ–≥—Ä–µ—Å—Å–∏—è –Ω—É–∂–Ω–∞ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤–µ–ª–∏—á–∏–Ω (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Ü–µ–Ω—ã –∫–≤–∞—Ä—Ç–∏—Ä—ã).",
                "‚ùå **–ö–∞—Ç–µ–≥–æ—Ä–∏—è**: –≠—Ç–æ –±—ã–ª–∞ –±—ã –∑–∞–¥–∞—á–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏."
            ]
        }
    },
    {
        question: "In Python's scikit-learn, which function is used to split data into training and test sets?",
        options: ["split_data()", "train_test_split()", "divide_set()", "random_split()"],
        correct: 1,
        topic: "Tools",
        set: "midterm",
        optionExplanations: { en: ["‚úÖ **train_test_split()**: The most common way to separate data for validation in sklearn."], ru: ["‚úÖ **train_test_split()**: –°–∞–º—ã–π –ø–æ–ø—É–ª—è—Ä–Ω—ã–π —Å–ø–æ—Å–æ–± —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ –∏ —Ç–µ—Å—Ç –≤ sklearn."] }
    },
    {
        question: "Overfitting occurs when:",
        options: ["Large dataset", "Simple model", "Zero learning rate", "Complex model fitting noise"],
        correct: 3,
        topic: "Bias-Variance",
        set: "midterm",
        optionExplanations: {
            en: ["‚úÖ **Complex fitting noise**: Correct! The model is too flexible and starts seeing patterns in random fluctuations."],
            ru: ["‚úÖ **–°–ª–æ–∂–Ω–∞—è –º–æ–¥–µ–ª—å, –≤—ã—É—á–∏–≤—à–∞—è —à—É–º**: –í–µ—Ä–Ω–æ! –ú–æ–¥–µ–ª—å —Å–ª–∏—à–∫–æ–º –≥–∏–±–∫–∞—è –∏ –Ω–∞—á–∞–ª–∞ –≤–∏–¥–µ—Ç—å –∑–∞–∫–æ–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç–∏ —Ç–∞–º, –≥–¥–µ –∏—Ö –Ω–µ—Ç (–≤ —Å–ª—É—á–∞–π–Ω–æ–º —à—É–º–µ)."]
        }
    },
    {
        question: "Which function in pandas displays the first few rows of a DataFrame?",
        options: ["df.show()", "df.top()", "df.head()", "df.print()"],
        correct: 2,
        topic: "Pandas",
        set: "midterm",
        optionExplanations: { en: ["‚úÖ **head()**: Returns the first 5 rows by default."], ru: ["‚úÖ **head()**: –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–µ—Ä–≤—ã–µ 5 —Å—Ç—Ä–æ–∫ —Ç–∞–±–ª–∏—Ü—ã."] }
    },
    {
        question: "Online learning is best suited for:",
        options: ["Static datasets", "Streaming or constantly updating data", "Small datasets only", "Models that never change"],
        correct: 1,
        topic: "ML Types",
        set: "midterm",
        optionExplanations: {
            en: ["‚úÖ **Streaming**: Correct! Online learning processes data incrementally as it arrives."],
            ru: ["‚úÖ **–ü–æ—Ç–æ–∫–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ**: –í–µ—Ä–Ω–æ! Online (–∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–µ) –æ–±—É—á–µ–Ω–∏–µ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –ø–æ –º–µ—Ä–µ –∏—Ö –ø–æ—Å—Ç—É–ø–ª–µ–Ω–∏—è."]
        }
    },
    {
        question: "Stratified sampling ensures that:",
        options: ["Random data selected", "Missing data imputed", "Normalized features", "Subgroups represented proportionally"],
        correct: 3,
        topic: "Sampling",
        set: "midterm",
        optionExplanations: {
            en: ["‚úÖ **Proportionally**: Correct! It keeps the same class balance in both training and test sets."],
            ru: ["‚úÖ **–ü—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª—å–Ω–æ**: –í–µ—Ä–Ω–æ! –ú–µ—Ç–æ–¥ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ç–æ—Ç –∂–µ –±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤ –∏ –≤ –æ–±—É—á–µ–Ω–∏–∏, –∏ –≤ —Ç–µ—Å—Ç–µ."]
        }
    },
    {
        question: "Import the Linear Regression model: from sklearn.linear_model import ___",
        type: "fill",
        answer: "LinearRegression",
        topic: "Scikit-Learn",
        set: "midterm",
        optionExplanations: { en: ["‚úÖ **LinearRegression**: The name of the standard Linear Regression class in sklearn."], ru: ["‚úÖ **LinearRegression**: –ò–º—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞ –ª–∏–Ω–µ–π–Ω–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –≤ sklearn."] }
    },
    {
        question: "Which markdown syntax correctly inserts an image?",
        options: ["<img>", "[image](url)", "# Image", "![text](url)"],
        correct: 3,
        topic: "Tools",
        set: "midterm",
        optionExplanations: {
            en: [
                "‚úÖ **![text](url)**: Correct! The `!` makes the difference between a text link and an embedded image.",
                "‚ùå **[text](url)**: This is just a hyperlink to a file."
            ],
            ru: [
                "‚úÖ **![—Ç–µ–∫—Å—Ç](—Å—Å—ã–ª–∫–∞)**: –í–µ—Ä–Ω–æ! –ó–Ω–∞–∫ `!` –æ—Ç–ª–∏—á–∞–µ—Ç –≤—Å—Ç—Ä–æ–µ–Ω–Ω—É—é –∫–∞—Ä—Ç–∏–Ω–∫—É –æ—Ç –æ–±—ã—á–Ω–æ–π —Ç–µ–∫—Å—Ç–æ–≤–æ–π —Å—Å—ã–ª–∫–∏.",
                "‚ùå **[—Ç–µ–∫—Å—Ç](—Å—Å—ã–ª–∫–∞)**: –≠—Ç–æ –ø—Ä–æ—Å—Ç–æ —Ç–µ–∫—Å—Ç–æ–≤–∞—è –≥–∏–ø–µ—Ä—Å—Å—ã–ª–∫–∞."
            ]
        }
    },
    {
        question: "What is the purpose of random_state=42 in train_test_split()?",
        options: ["Reproducibility", "Accuracy", "Remove randomness", "Perform stratification"],
        correct: 0,
        topic: "Tools",
        set: "midterm",
        optionExplanations: {
            en: ["‚úÖ **Reproducibility**: Correct! It ensures the random split is identical every time you run the code."],
            ru: ["‚úÖ **–í–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç—å**: –í–µ—Ä–Ω–æ! –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç, —á—Ç–æ —Å–ª—É—á–∞–π–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –±—É–¥–µ—Ç –æ–¥–∏–Ω–∞–∫–æ–≤—ã–º –ø—Ä–∏ –∫–∞–∂–¥–æ–º –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–µ –∫–æ–¥–∞."]
        }
    },
    {
        question: "In supervised learning, features are also known as:",
        options: ["Targets", "Labels", "Attributes or inputs", "Predictions"],
        correct: 2,
        topic: "Terminology",
        set: "midterm",
        optionExplanations: {
            en: [
                "‚úÖ **Attributes/Inputs**: Correct! Usually denoted as $X$. These are the columns used for making predictions.",
                "‚ùå **Labels**: Labels are the targets (y)."
            ],
            ru: [
                "‚úÖ **–ê—Ç—Ä–∏–±—É—Ç—ã –∏–ª–∏ –≤—Ö–æ–¥—ã**: –í–µ—Ä–Ω–æ! –û–±—ã—á–Ω–æ –æ–±–æ–∑–Ω–∞—á–∞—é—Ç—Å—è –∫–∞–∫ $X$. –≠—Ç–æ –∫–æ–ª–æ–Ω–∫–∏, –Ω–∞ –∫–æ—Ç–æ—Ä—ã—Ö –º—ã —Å—Ç—Ä–æ–∏–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ.",
                "‚ùå **–ú–µ—Ç–∫–∏ (Labels)**: –ú–µ—Ç–∫–∏ ‚Äî —ç—Ç–æ —Ü–µ–ª–∏ (y)."
            ]
        }
    },
    {
        question: "In the California Housing dataset, the target variable represents:",
        options: ["Median house value", "Population size", "Ocean proximity", "Median income"],
        correct: 0,
        topic: "Applications",
        set: "midterm",
        optionExplanations: {
            en: ["‚úÖ **Median value**: Correct! Our goal was to predict the price of houses in California districts."],
            ru: ["‚úÖ **–ú–µ–¥–∏–∞–Ω–Ω–∞—è —Ü–µ–Ω–∞**: –í–µ—Ä–Ω–æ! –ù–∞—à–µ–π —Ü–µ–ª—å—é –±—ã–ª–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å—Ç–æ–∏–º–æ—Å—Ç–∏ –¥–æ–º–æ–≤ –≤ —Ä–∞–π–æ–Ω–∞—Ö –ö–∞–ª–∏—Ñ–æ—Ä–Ω–∏–∏."]
        }
    },

    // --- QUIZ 2 (Encoding & Scaling) ---
    {
        question: "What happens if you use pd.get_dummies() on test data containing unseen categories?",
        options: ["Error", "New columns created", "Assigns NaN", "Ignores categories"],
        correct: 1,
        topic: "Encoding",
        set: "quiz2",
        optionExplanations: {
            en: [
                "‚ùå **Error**: Pandas doesn't complain, it just silently breaks your model's expected shape.",
                "‚úÖ **New columns**: Correct! This creates a mismatch between dimensions (train has 5, test might have 7), causing a crash later.",
                "üí° **Coach Tip**: Always use sklearn's `OneHotEncoder` with `handle_unknown='ignore'` for more safety!"
            ],
            ru: [
                "‚ùå **–û—à–∏–±–∫–∞**: Pandas –º–æ–ª—á–∏—Ç, –æ–Ω –ø—Ä–æ—Å—Ç–æ —Ç–∏—Ö–æ –º–µ–Ω—è–µ—Ç —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —Ç–∞–±–ª–∏—Ü—ã, —á—Ç–æ –ª–æ–º–∞–µ—Ç –º–æ–¥–µ–ª—å.",
                "‚úÖ **–ù–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏**: –í–µ—Ä–Ω–æ! –≠—Ç–æ —Å–æ–∑–¥–∞–µ—Ç –Ω–µ—Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∫–æ–ª–æ–Ω–æ–∫ (–≤ –æ–±—É—á–µ–Ω–∏–∏ 5, –∞ –≤ —Ç–µ—Å—Ç–µ —Å—Ç–∞–ª–æ 7).",
                "üí° **–°–æ–≤–µ—Ç**: –í—Å–µ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ `OneHotEncoder` –∏–∑ sklearn —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–º `handle_unknown='ignore'`."
            ]
        }
    },
    {
        question: "Which technique best handles heavily right-skewed features?",
        options: ["StandardScaler", "Log transformation", "One-hot encoding", "Normalization"],
        correct: 1,
        topic: "Transformation",
        set: "quiz2",
        optionExplanations: {
            en: [
                "‚ùå **StandardScaler**: Good for scale, but it doesn't fix the shape (skewness).",
                "‚úÖ **Log transformation**: Correct! It compresses large values more than small ones, making the distribution more 'normal'.",
                "üí° **Coach Tip**: Look at histograms! If there's a long tail to the right, think about Log or Root transforms."
            ],
            ru: [
                "‚ùå **StandardScaler**: –ò—Å–ø—Ä–∞–≤–ª—è–µ—Ç –º–∞—Å—à—Ç–∞–±, –Ω–æ –Ω–µ —Ñ–æ—Ä–º—É —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è (—Å–∫–æ—à–µ–Ω–Ω–æ—Å—Ç—å).",
                "‚úÖ **Log transformation**: –í–µ—Ä–Ω–æ! –°–∂–∏–º–∞–µ—Ç –±–æ–ª—å—à–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è —Å–∏–ª—å–Ω–µ–µ –º–∞–ª—ã—Ö, –¥–µ–ª–∞—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –±–æ–ª–µ–µ —Å–∏–º–º–µ—Ç—Ä–∏—á–Ω—ã–º.",
                "üí° **–°–æ–≤–µ—Ç**: –°–º–æ—Ç—Ä–∏ –Ω–∞ –≥–∏—Å—Ç–æ–≥—Ä–∞–º–º—É! –ï—Å–ª–∏ –≤–∏–¥–∏—à—å –¥–ª–∏–Ω–Ω—ã–π ¬´—Ö–≤–æ—Å—Ç¬ª —Å–ø—Ä–∞–≤–∞ ‚Äî –ø—Ä–æ–±—É–π –ª–æ–≥–∞—Ä–∏—Ñ–º."
            ]
        }
    },

    {
        question: "What happens if you use OneHotEncoder(handle_unknown='ignore') on unseen categories?",
        options: ["Errors", "Drops the sample", "Returns an all-zero row for that sample", "Guesses the closest category"],
        correct: 2,
        topic: "Encoding",
        set: "quiz2",
        optionExplanations: {
            en: [
                "‚ùå **Errors**: This only happens if you don't use 'ignore'.",
                "‚ùå **Drops**: It doesn't drop the row; it just changes the encoding.",
                "‚úÖ **All-zero row**: Correct! The model receives 0s for all known category columns, which is the safest way to handle unknown data in production.",
                "‚ùå **Guesses**: Models don't guess categories unless you explicitly use an Imputer or KNN."
            ],
            ru: [
                "‚ùå **–û—à–∏–±–∫–∞**: –≠—Ç–æ –ø—Ä–æ–∏–∑–æ–π–¥–µ—Ç —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –ù–ï –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å 'ignore'.",
                "‚ùå **–£–¥–∞–ª–µ–Ω–∏–µ**: –°—Ç—Ä–æ–∫–∞ –æ—Å—Ç–∞–µ—Ç—Å—è –Ω–∞ –º–µ—Å—Ç–µ, –º–µ–Ω—è–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –µ—ë –∫–æ–¥.",
                "‚úÖ **–í—Å–µ –Ω—É–ª–∏**: –í–µ—Ä–Ω–æ! –ü—Ä–æ–≥—Ä–∞–º–º–∞ –ø–æ–¥—Å—Ç–∞–≤–ª—è–µ—Ç 0 –≤–æ –≤—Å–µ –∫–æ–ª–æ–Ω–∫–∏ —ç—Ç–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏. –≠—Ç–æ —Å–∞–º—ã–π –±–µ–∑–æ–ø–∞—Å–Ω—ã–π —Å–ø–æ—Å–æ–± –æ–±—Ä–∞–±–æ—Ç–∫–∏ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.",
                "‚ùå **–£–≥–∞–¥—ã–≤–∞–Ω–∏–µ**: –ú–æ–¥–µ–ª–∏ –Ω–µ —É–≥–∞–¥—ã–≤–∞—é—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Å–∞–º–∏ –ø–æ —Å–µ–±–µ –±–µ–∑ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤."
            ]
        }
    },
    {
        question: "Bucketization (converting numbers to bins) always improves model performance.",
        options: ["True", "False"],
        correct: 1,
        topic: "Feature Engineering",
        set: "quiz2",
        optionExplanations: {
            en: [
                "‚ùå **True**: In ML, 'always' is almost never true!",
                "‚úÖ **False**: Correct! While bucketization helps linear models understand non-linear ranges, it also LOSES information within the bin. Sometimes raw numbers are better."
            ],
            ru: [
                "‚ùå **–ü—Ä–∞–≤–¥–∞**: –í –º–∞—à–∏–Ω–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏ —Å–ª–æ–≤–æ ¬´–≤—Å–µ–≥–¥–∞¬ª –ø–æ—á—Ç–∏ –Ω–∏–∫–æ–≥–¥–∞ –Ω–µ –±—ã–≤–∞–µ—Ç –≤–µ—Ä–Ω—ã–º!",
                "‚úÖ **–õ–æ–∂—å**: –í–µ—Ä–Ω–æ! –†–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ –≥—Ä—É–ø–ø—ã –ø–æ–º–æ–≥–∞–µ—Ç –ª–∏–Ω–µ–π–Ω—ã–º –º–æ–¥–µ–ª—è–º, –Ω–æ –ø—Ä–∏ —ç—Ç–æ–º –º—ã –¢–ï–†–Ø–ï–ú —Ç–æ—á–Ω–æ—Å—Ç—å –≤–Ω—É—Ç—Ä–∏ –∫–∞–∂–¥–æ–π –≥—Ä—É–ø–ø—ã."
            ]
        }
    },
    {
        question: "Standardization using StandardScaler() transforms data using:",
        options: ["(x-min)/(max-min)", "x/max", "(x-mean)/std", "log(x)"],
        correct: 2,
        topic: "Scaling",
        set: "quiz3",
        optionExplanations: {
            en: [
                "‚ùå **(x-min)**: That's normalization (MinMaxScaler).",
                "‚úÖ **(x-mean)/std**: Correct! This is the formula for a z-score. It ensures mean=0 and variance=1.",
                "üí° **Coach Tip**: Standardization handles outliers better than MinMax because it doesn't squish everything into a tiny [0,1] range."
            ],
            ru: [
                "‚ùå **(x-min)**: –≠—Ç–æ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è (MinMaxScaler).",
                "‚úÖ **(x-mean)/std**: –í–µ—Ä–Ω–æ! –≠—Ç–æ —Ñ–æ—Ä–º—É–ª–∞ z-–æ—Ü–µ–Ω–∫–∏. –°—Ä–µ–¥–Ω–µ–µ —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è 0, –∞ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ ‚Äî 1.",
                "üí° **–°–æ–≤–µ—Ç**: –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è –ª—É—á—à–µ –ø–µ—Ä–µ–Ω–æ—Å–∏—Ç –≤—ã–±—Ä–æ—Å—ã, —á–µ–º MinMax, —Ç–∞–∫ –∫–∞–∫ –Ω–µ –≤—Ç–∏—Å–∫–∏–≤–∞–µ—Ç –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –≤ —É–∑–∫–∏–π –¥–∏–∞–ø–∞–∑–æ–Ω [0,1]."
            ]
        }
    },

    {
        question: "Which scaler may distort data if the distribution contains outliers?",
        options: ["MinMaxScaler", "Normalizer", "RobustScaler", "StandardScaler"],
        correct: 0,
        topic: "Scaling",
        set: "quiz3",
        optionExplanations: {
            en: [
                "‚úÖ **MinMaxScaler**: Correct! Because it uses the absolute min and max, one single outlier at 1,000,000 will crush all your 'normal' data points into a tiny range like 0.0001.",
                "‚ùå **Normalizer**: Rescales rows, not features.",
                "‚ùå **RobustScaler**: Specifically designed to handle outliers by using the Interquartile Range.",
                "‚ùå **StandardScaler**: Sensitive to outliers, but not as extreme as MinMaxScaler (which has a hard 0-1 boundary)."
            ],
            ru: [
                "‚úÖ **MinMaxScaler**: –í–µ—Ä–Ω–æ! –¢–∞–∫ –∫–∞–∫ –æ–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–∏–Ω–∏–º—É–º –∏ –º–∞–∫—Å–∏–º—É–º, –æ–¥–∏–Ω –≤—ã–±—Ä–æ—Å –≤ –º–∏–ª–ª–∏–æ–Ω ¬´—Å–æ–∂–º–µ—Ç¬ª –≤—Å–µ –Ω–æ—Ä–º–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ –∫—Ä–æ—à–µ—á–Ω—ã–π –æ—Ç—Ä–µ–∑–æ–∫ –æ–∫–æ–ª–æ –Ω—É–ª—è.",
                "‚ùå **Normalizer**: –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ—Ç —Å—Ç—Ä–æ–∫–∏, –∞ –Ω–µ —Å—Ç–æ–ª–±—Ü—ã.",
                "‚ùå **RobustScaler**: –°–ø–µ—Ü–∏–∞–ª—å–Ω–æ —Å–æ–∑–¥–∞–Ω –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –≤—ã–±—Ä–æ—Å–∞–º–∏ —á–µ—Ä–µ–∑ –∫–≤–∞–Ω—Ç–∏–ª–∏.",
                "‚ùå **StandardScaler**: –¢–æ–∂–µ —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª–µ–Ω –∫ –≤—ã–±—Ä–æ—Å–∞–º, –Ω–æ –Ω–µ —Ç–∞–∫ –∫—Ä–∏—Ç–∏—á–Ω–æ, –∫–∞–∫ MinMax."
            ]
        }
    },
    {
        question: "Given RBF formula exp(‚àíŒ≥(x‚àím)¬≤), what happens if Œ≥ is very small?",
        options: ["Narrow peak", "Negative similarity", "Very wide similarity curve", "Similarity becomes exactly 1"],
        correct: 2,
        topic: "RBF Kernel",
        set: "quiz3",
        optionExplanations: {
            en: [
                "‚ùå **Narrow peak**: This happens when Gamma is LARGE. Influence is local.",
                "‚ùå **Negative**: Exponential functions are always positive.",
                "‚úÖ **Wide curve**: Correct! Small Gamma means one data point influences a huge area around it (global influence).",
                "‚ùå **Exactly 1**: This only happens if x exactly matches the landmark m."
            ],
            ru: [
                "‚ùå **–£–∑–∫–∏–π –ø–∏–∫**: –≠—Ç–æ –±—ã–≤–∞–µ—Ç –ø—Ä–∏ –ë–û–õ–¨–®–û–ô –ì–∞–º–º–µ. –í–ª–∏—è–Ω–∏–µ —Ç–æ–ª—å–∫–æ –Ω–∞ –±–ª–∏–∑–∫–∏—Ö —Å–æ—Å–µ–¥–µ–π.",
                "‚ùå **–û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å**: –≠–∫—Å–ø–æ–Ω–µ–Ω—Ç–∞ –≤—Å–µ–≥–¥–∞ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–∞.",
                "‚úÖ **–®–∏—Ä–æ–∫–∞—è –∫—Ä–∏–≤–∞—è**: –í–µ—Ä–Ω–æ! –ú–∞–ª–µ–Ω—å–∫–∞—è –ì–∞–º–º–∞ –æ–∑–Ω–∞—á–∞–µ—Ç, —á—Ç–æ —Ç–æ—á–∫–∞ –≤–ª–∏—è–µ—Ç –Ω–∞ –æ—á–µ–Ω—å –±–æ–ª—å—à—É—é –æ–±–ª–∞—Å—Ç—å –≤–æ–∫—Ä—É–≥ —Å–µ–±—è.",
                "‚ùå **–†–∞–≤–Ω–∞ 1**: –≠—Ç–æ –≤–æ–∑–º–æ–∂–Ω–æ —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —Ç–æ—á–∫–∞ x —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–º m."
            ]
        }
    },
    {
        question: "In a pipeline, which steps must be transformers?",
        options: ["All steps", "Only the final step", "All steps except the last", "None of them"],
        correct: 2,
        topic: "Pipelines",
        set: "quiz4",
        optionExplanations: {
            en: [
                "‚ùå **All**: The last step can be an estimator (like LinearRegression) which doesn't have a `transform()` method.",
                "‚úÖ **Except last**: Correct! Every intermediate step must be able to change the data so it can be passed to the next step.",
                "üí° **Coach Tip**: Think of a factory belt. Every station (transformer) modifies the product, until the final inspector (estimator) evaluates it."
            ],
            ru: [
                "‚ùå **–í—Å–µ**: –ü–æ—Å–ª–µ–¥–Ω–∏–º —à–∞–≥–æ–º –º–æ–∂–µ—Ç –±—ã—Ç—å –º–æ–¥–µ–ª—å (—Ä–µ–≥—Ä–µ—Å—Å–∏—è), —É –∫–æ—Ç–æ—Ä–æ–π –Ω–µ—Ç –º–µ—Ç–æ–¥–∞ `transform()`.",
                "‚úÖ **–í—Å–µ –∫—Ä–æ–º–µ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ**: –í–µ—Ä–Ω–æ! –ö–∞–∂–¥—ã–π –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–π —à–∞–≥ –¥–æ–ª–∂–µ–Ω —É–º–µ—Ç—å –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤—ã–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —à–∞–≥–∞.",
                "üí° **–°–æ–≤–µ—Ç**: –ü—Ä–µ–¥—Å—Ç–∞–≤—å –∫–æ–Ω–≤–µ–π–µ—Ä. –ö–∞–∂–¥–∞—è —Å—Ç–∞–Ω—Ü–∏—è (—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä) –º–µ–Ω—è–µ—Ç –¥–µ—Ç–∞–ª—å, –ø–æ–∫–∞ —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –∏–Ω—Å–ø–µ–∫—Ç–æ—Ä (–º–æ–¥–µ–ª—å) –Ω–µ –≤—ã–¥–∞—Å—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç."
            ]
        }
    },

    // --- BONUS QUIZ (Validation) ---
    {
        question: "What is the key advantage of using k-fold cross-validation?",
        options: ["Eliminates bias completely", "Always improves accuracy", "Reliable estimate of generalization", "Reduces features"],
        correct: 2,
        topic: "Validation",
        set: "bonus",
        optionExplanations: {
            en: [
                "‚ùå **Eliminates bias**: No technique 'eliminates' bias completely.",
                "‚úÖ **Reliable estimate**: Correct! By testing on every subset of the data, you get a much better idea of how the model will perform on unseen data.",
                "üí° **Coach Tip**: Use CV if your dataset is small. It makes much better use of your limited data points."
            ],
            ru: [
                "‚ùå **–£–±–∏—Ä–∞–µ—Ç —Å–º–µ—â–µ–Ω–∏–µ**: –ù–∏–∫–∞–∫–æ–π –º–µ—Ç–æ–¥ –Ω–µ —É–±–∏—Ä–∞–µ—Ç —Å–º–µ—â–µ–Ω–∏–µ –ø–æ–ª–Ω–æ—Å—Ç—å—é.",
                "‚úÖ **–ù–∞–¥–µ–∂–Ω–∞—è –æ—Ü–µ–Ω–∫–∞**: –í–µ—Ä–Ω–æ! –ü—Ä–æ–≤–µ—Ä—è—è –º–æ–¥–µ–ª—å –Ω–∞ –∫–∞–∂–¥–æ–º –∫—É—Å–æ—á–∫–µ –¥–∞–Ω–Ω—ã—Ö, —Ç—ã –ª—É—á—à–µ –ø–æ–Ω–∏–º–∞–µ—à—å –µ—ë –∏—Å—Ç–∏–Ω–Ω—É—é —Å–∏–ª—É.",
                "üí° **–°–æ–≤–µ—Ç**: –ò—Å–ø–æ–ª—å–∑—É–π –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—é, –µ—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –º–∞–ª–æ. –û–Ω–∞ –ø–æ–∑–≤–æ–ª—è–µ—Ç –≤—ã–∂–∞—Ç—å –º–∞–∫—Å–∏–º—É–º –∏–∑ –∏–º–µ—é—â–∏—Ö—Å—è –ø—Ä–∏–º–µ—Ä–æ–≤."
            ]
        }
    },
    // --- ADVANCED FINAL EXAM MODULE (Logic Boost) ---
    {
        question: "What happens if you use Standard Scaler on a feature that has a huge outlier (e.g., 10,000 when the mean is 50)?",
        options: ["The outlier is removed", "The mean becomes skewed and the range of normal data is squashed", "The outlier is automatically clipped to 1", "The model becomes 100% accurate"],
        correct: 1,
        topic: "Scaling Issues",
        set: "midterm", // Adding to midterm/final pool
        optionExplanations: {
            en: [
                "‚ùå **Removed**: Scalers don't remove data. They just transform values.",
                "‚úÖ **Squashed**: Correct! Standard scaler is very sensitive to outliers. One huge value will pull the mean up and make standard deviation huge, leaving normal points as tiny decimals close to zero.",
                "‚ùå **Clipped**: Clipping is a separate technique (like `winsorizing`).",
                "üí° **Coach Tip**: Use `RobustScaler` if you have nasty outliers‚Äîit uses median and IQR instead of mean and std!"
            ],
            ru: [
                "‚ùå **–£–¥–∞–ª–µ–Ω–∏–µ**: –°–∫–µ–π–ª–µ—Ä—ã –Ω–µ —É–¥–∞–ª—è—é—Ç –¥–∞–Ω–Ω—ã–µ, –æ–Ω–∏ —Ç–æ–ª—å–∫–æ –º–µ–Ω—è—é—Ç –∏—Ö –∑–Ω–∞—á–µ–Ω–∏—è.",
                "‚úÖ **–°–¥–∞–≤–ª–∏–≤–∞–Ω–∏–µ**: –í–µ—Ä–Ω–æ! StandardScaler —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª–µ–Ω –∫ –≤—ã–±—Ä–æ—Å–∞–º. –û–¥–Ω–æ –≥–∏–≥–∞–Ω—Ç—Å–∫–æ–µ —á–∏—Å–ª–æ —Ä–∞–∑–¥—É–µ—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ, –∏–∑-–∑–∞ —á–µ–≥–æ –æ–±—ã—á–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –ø—Ä–µ–≤—Ä–∞—Ç—è—Ç—Å—è –≤ –∫—Ä–æ—à–µ—á–Ω—ã–µ –¥—Ä–æ–±–Ω—ã–µ —á–∏—Å–ª–∞.",
                "‚ùå **–ö–ª–∏–ø–ø–∏–Ω–≥**: –≠—Ç–æ –æ—Ç–¥–µ–ª—å–Ω—ã–π –ø—Ä–∏–µ–º. –°–∫–µ–π–ª–µ—Ä—ã —Å–∞–º–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –æ–±—Ä–µ–∑–∞—é—Ç.",
                "üí° **–°–æ–≤–µ—Ç**: –ï—Å–ª–∏ –≤ –¥–∞–Ω–Ω—ã—Ö –µ—Å—Ç—å –≤—ã–±—Ä–æ—Å—ã, –∏—Å–ø–æ–ª—å–∑—É–π `RobustScaler` ‚Äî –æ–Ω —Ä–∞–±–æ—Ç–∞–µ—Ç —á–µ—Ä–µ–∑ –º–µ–¥–∏–∞–Ω—É –∏ –∫–≤–∞–Ω—Ç–∏–ª–∏."
            ]
        }
    },
    {
        question: "If your model has 100% accuracy on Training but 50% on Validation, you are likely witnessing:",
        options: ["Underfitting", "Perfect learning", "Data leakage", "Overfitting (High Variance)"],
        correct: 3,
        topic: "Bias-Variance",
        set: "midterm",
        optionExplanations: {
            en: [
                "‚ùå **Underfitting**: Underfitting gives BAD scores on both sets.",
                "‚ùå **Perfect**: No, 50% is basically random guessing.",
                "‚ùå **Data Leakage**: Leakage usually makes BOTH scores look too good.",
                "‚úÖ **Overfitting**: Correct! Your model memorized the specific details (and noise) of the training data but can't handle new data."
            ],
            ru: [
                "‚ùå **–ù–µ–¥–æ–æ–±—É—á–µ–Ω–∏–µ**: –í —ç—Ç–æ–º —Å–ª—É—á–∞–µ –ø–ª–æ—Ö–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –±—É–¥—É—Ç –≤–µ–∑–¥–µ.",
                "‚ùå **–ò–¥–µ–∞–ª**: –ù–µ—Ç, 50% ‚Äî —ç—Ç–æ —É—Ä–æ–≤–µ–Ω—å –ø–æ–¥–±—Ä–∞—Å—ã–≤–∞–Ω–∏—è –º–æ–Ω–µ—Ç–∫–∏.",
                "‚ùå **–£—Ç–µ—á–∫–∞**: –£—Ç–µ—á–∫–∞ –æ–±—ã—á–Ω–æ –¥–µ–ª–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ —Ç–µ—Å—Ç–µ —Å–ª–∏—à–∫–æ–º *—Ö–æ—Ä–æ—à–∏–º–∏*.",
                "‚úÖ **–ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ**: –í–µ—Ä–Ω–æ! –ú–æ–¥–µ–ª—å –ø—Ä–æ—Å—Ç–æ –∑–∞—É—á–∏–ª–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤–º–µ—Å—Ç–µ —Å –∏—Ö —à—É–º–æ–º –∏ –Ω–µ –ø–æ–Ω–∏–º–∞–µ—Ç –Ω–æ–≤—ã–µ –ø—Ä–∏–º–µ—Ä—ã."
            ]
        }
    },
    {
        question: "Which hyperparameter in RandomizedSearchCV controls how many total models will be tested?",
        type: "fill",
        answer: "n_iter",
        topic: "Validation",
        set: "bonus",
        optionExplanations: {
            en: ["‚úÖ **n_iter**: Correct! Unlike GridSearch (which tries every option), RandomizedSearch stops after $n\_iter$ random combinations."],
            ru: ["‚úÖ **n_iter**: –í–µ—Ä–Ω–æ! –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç GridSearch (–∫–æ—Ç–æ—Ä—ã–π –ø—Ä–æ–±—É–µ—Ç –≤—Å—ë), —Å–ª—É—á–∞–π–Ω—ã–π –ø–æ–∏—Å–∫ –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è –ø–æ—Å–ª–µ $n\_iter$ –ø–æ–ø—ã—Ç–æ–∫."]
        }
    },
    {
        question: "Why is log transformation applied to features with heavy-tailed distributions?",
        options: ["To increase variance", "To improve categorical encoding", "To reduce skewness and stabilize variance", "To create more outliers"],
        correct: 2,
        topic: "Feature Engineering",
        set: "quiz4",
        optionExplanations: {
            en: [
                "‚ùå **Increase variance**: Log transformation usually *reduces* the spread of large values.",
                "‚ùå **Categorical**: Log is for numerical data, not categories.",
                "‚úÖ **Reduce skewness**: Correct! It 'pulls' the long right tail of the distribution toward the center, making it easier for linear models to work with.",
                "‚ùå **Outliers**: It actually makes outliers LESS extreme."
            ],
            ru: [
                "‚ùå **–£–≤–µ–ª–∏—á–µ–Ω–∏–µ –¥–∏—Å–ø–µ—Ä—Å–∏–∏**: –õ–æ–≥–∞—Ä–∏—Ñ–º –æ–±—ã—á–Ω–æ *—É–º–µ–Ω—å—à–∞–µ—Ç* —Ä–∞–∑–±—Ä–æ—Å –±–æ–ª—å—à–∏—Ö –∑–Ω–∞—á–µ–Ω–∏–π.",
                "‚ùå **–ö–∞—Ç–µ–≥–æ—Ä–∏–∏**: –õ–æ–≥–∞—Ä–∏—Ñ–º –Ω—É–∂–µ–Ω –¥–ª—è —á–∏—Å–µ–ª, –∞ –Ω–µ –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π.",
                "‚úÖ **–£–º–µ–Ω—å—à–µ–Ω–∏–µ —Å–∫–æ—à–µ–Ω–Ω–æ—Å—Ç–∏**: –í–µ—Ä–Ω–æ! –û–Ω ¬´–ø–æ–¥—Ç—è–≥–∏–≤–∞–µ—Ç¬ª –¥–ª–∏–Ω–Ω—ã–π –ø—Ä–∞–≤—ã–π —Ö–≤–æ—Å—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫ —Ü–µ–Ω—Ç—Ä—É, –¥–µ–ª–∞—è –¥–∞–Ω–Ω—ã–µ –±–æ–ª–µ–µ –ø—Ä–∏–≥–æ–¥–Ω—ã–º–∏ –¥–ª—è –ª–∏–Ω–µ–π–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π.",
                "‚ùå **–í—ã–±—Ä–æ—Å—ã**: –û–Ω –¥–µ–ª–∞–µ—Ç –≤—ã–±—Ä–æ—Å—ã –ú–ï–ù–ï–ï –∑–∞–º–µ—Ç–Ω—ã–º–∏."
            ]
        }
    },
    {
        question: "The purpose of ratio-based feature engineering (e.g., bedrooms_ratio) is to:",
        options: ["Reduce memory", "Remove multicollinearity", "Reduce dimensionality", "Create more meaningful, normalized relationships"],
        correct: 3,
        topic: "Feature Engineering",
        set: "quiz4",
        optionExplanations: {
            en: [
                "‚úÖ **Normalized relationships**: Correct! 5 bedrooms might be a lot for a small house but little for a mansion. A ratio (bedrooms/rooms) tells a better story independent of total size.",
                "‚ùå **Multicollinearity**: If anything, ratios might *add* complexity to relationships."
            ],
            ru: [
                "‚úÖ **–°–º—ã—Å–ª–æ–≤—ã–µ —Å–≤—è–∑–∏**: –í–µ—Ä–Ω–æ! 5 —Å–ø–∞–ª–µ–Ω ‚Äî —ç—Ç–æ –º–Ω–æ–≥–æ –¥–ª—è –º–∞–ª–µ–Ω—å–∫–æ–≥–æ –¥–æ–º–∞, –Ω–æ –º–∞–ª–æ –¥–ª—è –æ—Å–æ–±–Ω—è–∫–∞. –û—Ç–Ω–æ—à–µ–Ω–∏–µ (—Å–ø–∞–ª—å–Ω–∏/–∫–æ–º–Ω–∞—Ç—ã) –¥–∞–µ—Ç –±–æ–ª–µ–µ —Ç–æ—á–Ω—É—é –∫–∞—Ä—Ç–∏–Ω—É –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –æ—Ç —Ä–∞–∑–º–µ—Ä–∞ –¥–æ–º–∞.",
                "‚ùå **–ú—É–ª—å—Ç–∏–∫–æ–ª–ª–∏–Ω–µ–∞—Ä–Ω–æ—Å—Ç—å**: –ù–∞–ø—Ä–æ—Ç–∏–≤, –Ω–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –º–æ–≥—É—Ç –¥–∞–∂–µ —É—Å–ª–æ–∂–Ω–∏—Ç—å —Å–≤—è–∑–∏ –º–µ–∂–¥—É –¥–∞–Ω–Ω—ã–º–∏."
            ]
        }
    },
    {
        question: "KMeans-based cluster similarity features can help linear models capture nonlinear geographic patterns.",
        options: ["True", "False"],
        correct: 0,
        topic: "ClusterSimilarity",
        set: "quiz4",
        optionExplanations: {
            en: [
                "‚úÖ **True**: Correct! By measuring distance to specific 'hubs' (clusters), the linear model can learn that being near 'Point A' is good, even if it's not a simple straight-line relationship across the map."
            ],
            ru: [
                "‚úÖ **–ü—Ä–∞–≤–¥–∞**: –í–µ—Ä–Ω–æ! –ò–∑–º–µ—Ä—è—è —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –¥–æ ¬´—Ü–µ–Ω—Ç—Ä–æ–≤¬ª (–∫–ª–∞—Å—Ç–µ—Ä–æ–≤), –ª–∏–Ω–µ–π–Ω–∞—è –º–æ–¥–µ–ª—å –º–æ–∂–µ—Ç –ø–æ–Ω—è—Ç—å, —á—Ç–æ –±–ª–∏–∑–æ—Å—Ç—å –∫ '–¢–æ—á–∫–µ –ê' –≤–∞–∂–Ω–∞, –¥–∞–∂–µ –µ—Å–ª–∏ –æ–±—â–∞—è –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –ø–æ –∫–∞—Ä—Ç–µ –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –ø—Ä—è–º–æ–π –ª–∏–Ω–∏–µ–π."
            ]
        }
    },
    {
        question: "What is the main limitation of GridSearchCV compared to RandomizedSearchCV?",
        options: ["Cannot use CV", "Only for classification", "Computationally expensive", "Cannot tune pipelines"],
        correct: 2,
        topic: "Validation",
        set: "bonus",
        optionExplanations: {
            en: [
                "‚úÖ **Expensive**: Correct! GridSearch tries EVERY possible combination. If you have 10 params with 10 values each, that's $10^{10}$ models! RandomizedSearch only tries $n$ random ones.",
                "‚ùå **Pipelines**: Both work perfectly with pipelines."
            ],
            ru: [
                "‚úÖ **–î–æ—Ä–æ–≥–æ–≤–∏–∑–Ω–∞**: –í–µ—Ä–Ω–æ! GridSearch –ø–µ—Ä–µ–±–∏—Ä–∞–µ—Ç –í–°–ï –≤–æ–∑–º–æ–∂–Ω—ã–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏. –ï—Å–ª–∏ —É —Ç–µ–±—è 10 –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –ø–æ 10 –∑–Ω–∞—á–µ–Ω–∏–π, —ç—Ç–æ $10^{10}$ –º–æ–¥–µ–ª–µ–π! RandomizedSearch –ø—Ä–æ–±—É–µ—Ç —Ç–æ–ª—å–∫–æ $n$ —Å–ª—É—á–∞–π–Ω—ã—Ö.",
                "‚ùå **–ü–∞–π–ø–ª–∞–π–Ω—ã**: –û–±–∞ –º–µ—Ç–æ–¥–∞ –æ—Ç–ª–∏—á–Ω–æ —Ä–∞–±–æ—Ç–∞—é—Ç —Å –ø–∞–π–ø–ª–∞–π–Ω–∞–º–∏."
            ]
        }
    },
    {
        question: "Which sklearn tool can automatically remove low-importance features based on a fitted model?",
        options: ["PCA", "SelectKBest", "SelectFromModel", "VarianceThreshold"],
        correct: 2,
        topic: "Feature Selection",
        set: "bonus",
        optionExplanations: {
            en: [
                "‚ùå **PCA**: Reduces dimensions by creating NEW features, not by picking existing ones.",
                "‚ùå **SelectKBest**: Uses statistical tests (independently of a model).",
                "‚úÖ **SelectFromModel**: Correct! It uses the weights (like `coef_` or `feature_importances_`) from a trained model to pick the best features.",
                "‚ùå **VarianceThreshold**: Only looks at the variance of a single feature, not its importance to a label."
            ],
            ru: [
                "‚ùå **PCA**: –£–º–µ–Ω—å—à–∞–µ—Ç —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å, —Å–æ–∑–¥–∞–≤–∞—è –ù–û–í–´–ï –ø—Ä–∏–∑–Ω–∞–∫–∏, –∞ –Ω–µ –≤—ã–±–∏—Ä–∞—è —Å—Ç–∞—Ä—ã–µ.",
                "‚ùå **SelectKBest**: –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ —Ç–µ—Å—Ç—ã (–Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –æ—Ç —Å–∞–º–æ–π –º–æ–¥–µ–ª–∏).",
                "‚úÖ **SelectFromModel**: –í–µ—Ä–Ω–æ! –≠—Ç–æ—Ç –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≤–µ—Å–∞ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ (`feature_importances_`), —á—Ç–æ–±—ã –æ—Å—Ç–∞–≤–∏—Ç—å —Ç–æ–ª—å–∫–æ –≤–∞–∂–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏.",
                "‚ùå **VarianceThreshold**: –°–º–æ—Ç—Ä–∏—Ç —Ç–æ–ª—å–∫–æ –Ω–∞ —Ä–∞–∑–±—Ä–æ—Å –æ–¥–Ω–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞, –Ω–µ —É—á–∏—Ç—ã–≤–∞—è –µ–≥–æ –≤–∞–∂–Ω–æ—Å—Ç—å –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è."
            ]
        }
    }
];
