# Модуль 8: Unsupervised Learning (Обучение без учителя)

## 8.1 K-means Clustering

### 1) Интуиция и суть

**Простыми словами (ELI5):**  
У тебя есть бусины разных цветов, перемешанные в куче. Ты хочешь разложить их на K групп (кластеров). K-means:
1. Случайно выбрать K "центров" (представителей групп)
2. Каждую бусину отнести к ближайшему центру
3. Пересчитать центры как среднее положение бусин в группе
4. Повторять 2-3 до сходимости

**Академически:**  
K-means — это **partition-based clustering** алгоритм, который разбивает данные на K кластеров путем **минимизации внутрикластерной дисперсии** (inertia). Итеративный алгоритм, чередующий **assignment step** (присваивание точек центрам) и **update step** (обновление центров).

**Физический смысл:**  
Минимизация суммарного квадратичного расстояния от точек до их центров кластеров.

### 2) Зачем и когда (Business Value)

**Какую проблему решает:**
- **Сегментация**: разделение объектов на группы без меток
- **Компрессия данных**: представление каждой точки через центр кластера
- **Feature engineering**: кластерная принадлежность как признак

**Где это индустриальный стандарт:**
- **Маркетинг**: сегментация клиентов (RFM-анализ)
- **Рекомендательные системы**: группировка пользователей/товаров
- **Computer Vision**: сегментация изображений, цветов
- **Геномика**: группировка генов с похожей экспрессией

**Когда применять нельзя:**
- ❌ Кластеры нелинейной формы (например, концентрические круги)
- ❌ Разные размеры/плотности кластеров
- ❌ Много выбросов (сильно влияют на центры)

### 3) Математическое ядро

**Цель:**  
Минимизировать **inertia** (within-cluster sum of squares):
```
J = Σ Σ ||x_i - μ_k||²
    k x_i∈C_k
```

где:
- C_k — k-й кластер
- μ_k — центр k-го кластера

**Алгоритм (Lloyd's algorithm):**

1. **Инициализация:** выбрать K начальных центров μ_k (случайно или K-means++)

2. **Assignment step:** присвоить каждую точку ближайшему центру
   ```
   C_k = {x_i : ||x_i - μ_k|| ≤ ||x_i - μ_j||, ∀j}
   ```

3. **Update step:** пересчитать центры как средние точек кластера
   ```
   μ_k = (1/|C_k|) Σ x_i
                   x_i∈C_k
   ```

4. **Repeat 2-3** до сходимости (центры перестают меняться или J не уменьшается)

**Сложность:**
- Одна итерация: O(n · K · d), где n — число точек, d — размерность
- Обычно сходится за 10-100 итераций

### 4) Middle-level нюансы

**Проблема 1: Выбор K (число кластеров)**

**Метод 1: Elbow method**

Постройте график J(K):
```
K = 1, 2, 3, ..., K_max
Для каждого K запустите K-means, посчитайте J
```

**"Локоть"** — точка, после которой J уменьшается медленно.

**Проблема:** субъективно, "локоть" не всегда явный.

**Метод 2: Silhouette Score**

Для каждой точки i:
- a_i = среднее расстояние до точек своего кластера
- b_i = среднее расстояние до точек ближайшего другого кластера

```
s_i = (b_i - a_i) / max(a_i, b_i)
```

**Silhouette ∈ [-1, 1]:**
- s ≈ 1: точка хорошо в своем кластере
- s ≈ 0: граница между кластерами
- s < 0: возможно, неправильный кластер

**Средний Silhouette Score** — метрика качества кластеризации.

**Метод 3: Domain knowledge**

Часто K известно из задачи (например, 3 типа клиентов).

**Проблема 2: Инициализация (K-means++)**

**Проблема случайной инициализации:**  
K-means сходится к **локальному минимуму**, зависящему от начальных центров.

**K-means++ (умная инициализация):**

1. Выбрать первый центр случайно
2. Для остальных центров:
   - Выбрать следующий центр с вероятностью ∝ D(x)², где D(x) — расстояние до ближайшего уже выбранного центра
3. Запустить обычный K-means

**Эффект:**  
Начальные центры **разнесены** → лучший локальный минимум.

**Проблема 3: Связь с EM (Expectation-Maximization)**

K-means — это **частный случай EM** для **Gaussian Mixture Model (GMM)** с:
- Изотропными ковариациями (σ² · I)
- σ² → 0 (жесткое присваивание вместо вероятностного)

**Generalization:**  
**GMM** — мягкая версия K-means, где каждая точка имеет **вероятности принадлежности** к кластерам.

**Проблема 4: Масштабирование признаков**

K-means использует **евклидово расстояние** → признаки с большим масштабом доминируют!

**Решение:**  
ВСЕГДА **стандартизировать** данные перед K-means.

### 5) Сравнение с альтернативами

| Метод | Форма кластеров | Выбросы | Необходимость указать K | Сложность |
|-------|----------------|---------|------------------------|-----------|
| **K-means** | Сферические | ❌ Чувствителен | ✅ Да | O(n·K·d·iterations) |
| **DBSCAN** | Произвольная | ✅ Устойчив | ❌ Нет (авто) | O(n log n) с индексами |
| **Hierarchical** | Произвольная | ⚠️ Средне | ❌ Нет (дендрограмма) | O(n²) или O(n² log n) |
| **GMM** | Эллиптические | ⚠️ Средне | ✅ Да | O(n·K·d·iterations) |

---

## 8.2 Иерархическая кластеризация (Hierarchical Clustering)

### 1) Интуиция и суть

**Простыми словами (ELI5):**  
Представь родословное дерево:
- **Agglomerative (снизу вверх)**: начинаем с каждого человека отдельно, объединяем в семьи, семьи в рода, рода в кланы...
- **Divisive (сверху вниз)**: начинаем со всего человечества, делим на расы, расы на народы, народы на семьи...

Результат — **дендрограмма** (дерево), где можно "отрезать" на любом уровне → любое число кластеров.

**Академически:**  
Hierarchical clustering строит **иерархию кластеров**, представленную деревом (дендрограммой). Не требует заранее указывать число кластеров.

### 2) Зачем и когда (Business Value)

**Какую проблему решает:**
- **Иерархическая структура** данных (например, таксономия)
- **Неизвестное K**: можно выбрать потом, анализируя дендрограмму
- **Интерпретация**: дерево наглядно показывает отношения

**Где используется:**
- **Биология**: филогенетические деревья, классификация видов
- **Маркетинг**: иерархия сегментов клиентов
- **NLP**: иерархическая тематическая модель документов

### 3) Математическое ядро

**Agglomerative Clustering (bottom-up):**

1. Начать с n кластеров (каждая точка = кластер)
2. Повторять:
   - Найти два ближайших кластера
   - Объединить их
   - Обновить матрицу расстояний
3. Продолжать до 1 кластера

**Расстояние между кластерами (linkage):**

**1. Single Linkage (minimum):**
```
d(C_i, C_j) = min{d(x, y) : x ∈ C_i, y ∈ C_j}
```
Минимальное расстояние между точками кластеров.

**Проблема:** chaining — кластеры вытягиваются в цепочки.

**2. Complete Linkage (maximum):**
```
d(C_i, C_j) = max{d(x, y) : x ∈ C_i, y ∈ C_j}
```
Максимальное расстояние.

**Эффект:** компактные, сферические кластеры.

**3. Average Linkage:**
```
d(C_i, C_j) = (1/(|C_i|·|C_j|)) Σ Σ d(x, y)
                                 x∈C_i y∈C_j
```
Среднее расстояние между всеми парами точек.

**Баланс** между single и complete.

**4. Ward's Linkage:**
```
Объединить кластеры, минимизирующие прирост inertia (как в K-means)
```

**Лучше всего** для K-means-подобных кластеров.

### 4) Middle-level нюансы

**Проблема 1: Computational Complexity**

**Naive алгоритм:** O(n³)
- n итераций
- Каждая итерация: поиск минимума в O(n²) матрице расстояний

**Оптимизации:**
- Single Linkage: O(n²) с MST (minimum spanning tree)
- Для остальных linkages: O(n² log n) с priority queue

**Следствие:**  
Не масштабируется на большие данные (n > 10,000).

**Проблема 2: Выбор числа кластеров из дендрограммы**

**Dendrogram (дендрограмма):**  
Дерево, где ось Y — расстояние объединения.

**"Отрезать" дерево** на высоте h → фиксировать число кластеров.

**Как выбрать h:**
- Визуально: большой скачок высоты → естественная граница
- Автоматически: максимизировать Silhouette Score для разных h

### 5) Сравнение linkages

| Linkage | Форма кластеров | Outliers | Когда использовать |
|---------|----------------|----------|-------------------|
| **Single** | Произвольная (может быть chaining) | ❌ Очень чувствителен | Кластеры вытянутой формы |
| **Complete** | Компактные, сферические | ✅ Устойчив | Шаровидные кластеры |
| **Average** | Средние | ⚠️ Средне | Сбалансированный вариант |
| **Ward** | Сферические (как K-means) | ⚠️ Средне | **Default choice** |

---

## 8.3 PCA (Principal Component Analysis)

### 1) Интуиция и суть

**Простыми словами (ELI5):**  
У тебя есть 3D-облако точек (например, рост, вес, размер обуви людей). Часть информации избыточна (рост и размер обуви коррелированны). PCA находит **главные направления** (оси), вдоль которых данные больше всего варьируются, и проецирует на них. Например, первое направление: "общий размер человека" (комбинация роста, веса, обуви).

**Академически:**  
PCA — это метод **снижения размерности** путем проецирования данных на **главные компоненты** (principal components) — ортогональные направления максимальной дисперсии. Использует **Singular Value Decomposition (SVD)** матрицы данных.

**Физический смысл:**  
Поиск **новой системы координат**, где оси упорядочены по убыванию дисперсии данных.

### 2) Зачем и когда (Business Value)

**Какую проблему решает:**
- **Curse of dimensionality**: слишком много признаков
- **Визуализация**: проекция на 2-3 компоненты
- **Noise reduction**: отбрасываем компоненты с малой дисперсией (шум)
- **Feature engineering**: главные компоненты как новые признаки

**Где это индустриальный стандарт:**
- **EDA (Exploratory Data Analysis)**: визуализация многомерных данных
- **Preprocessing**: перед кластеризацией, классификацией
- **Computer Vision**: eigenfaces для распознавания лиц
- **Геномика**: анализ генной экспрессии

**Когда применять нельзя:**
- ❌ Признаки **нелинейно связаны** (используйте Kernel PCA, t-SNE, UMAP)
- ❌ Нужна **интерпретация** (главные компоненты — линейные комбинации, сложно интерпретировать)

### 3) Математическое ядро

**Данные:** X (n × d), центрированные (среднее = 0 по каждому признаку)

**Цель:**  
Найти ортогональные направления (вектора) **w_1, w_2, ..., w_k**, максимизирующие дисперсию проекций.

**Первая главная компонента:**
```
w_1 = argmax Var(Xw) = argmax w^T (X^T X) w
      ||w||=1           ||w||=1
```

Это **собственный вектор** матрицы ковариации X^T X с **наибольшим собственным значением**.

**Ковариационная матрица:**
```
Cov(X) = (1/n) X^T X
```

**SVD подход (более эффективный):**
```
X = U Σ V^T
```

где:
- V — матрица правых сингулярных векторов (**главные компоненты**)
- Σ — диагональная матрица сингулярных значений (связаны с дисперсией)

**Главные компоненты:**  
Столбцы V: v_1, v_2, ..., v_d

**Проекция на k компонент:**
```
Z = X · V_k
```
где V_k — первые k столбцов V.

**Explained Variance Ratio:**
```
Дисперсия i-й компоненты / Полная дисперсия = σ_i² / Σ σ_j²
```

**Сколько компонент оставить:**  
Выбрать k так, чтобы **cumulative explained variance** ≥ 90-95%.

### 4) Middle-level нюансы

**Проблема 1: Масштабирование признаков (КРИТИЧНО!)**

**Проблема:**  
PCA ищет направления максимальной **дисперсии**. Признаки с большим масштабом (например, зарплата в рублях: 0-1,000,000) будут доминировать над признаками с малым масштабом (возраст: 0-100).

**Решение:**  
ВСЕГДА **стандартизировать** (или нормализовать) данные:
```python
from sklearn.preprocessing import StandardScaler
X_scaled = StandardScaler().fit_transform(X)
```

**Проблема 2: PCA и outliers**

PCA чувствительна к **выбросам** (они сильно влияют на дисперсию).

**Решение:**
- Удалить выбросы перед PCA
- **Robust PCA**: использует robust оценки ковариации

**Проблема 3: Интерпретация главных компонент**

**Проблема:**  
Главная компонента = линейная комбинация:
```
PC1 = 0.5·height + 0.3·weight + 0.7·shoe_size + ...
```

Сложно интерпретировать: "что это такое?"

**Частичное решение:**
- **Loadings** (веса признаков в компоненте) — показывают вклад
- Визуализация loadings: **biplot**

**Проблема 4: Сколько компонент оставить?**

**Метод 1: Explained Variance Threshold**
```
Оставить k компонент, объясняющих ≥ 90% дисперсии
```

**Метод 2: Scree Plot**
```
График собственных значений vs номер компоненты
Найти "локоть" (elbow)
```

**Метод 3: Cross-Validation**
```
Обучить модель с разным k на PCA-transformed data
Выбрать k, минимизирующий validation error
```

### 5) Сравнение методов снижения размерности

| Метод | Линейность | Сохраняет | Интерпретация | Когда использовать |
|-------|-----------|-----------|---------------|-------------------|
| **PCA** | ✅ Линейный | Глобальная структура (дисперсия) | ⚠️ Сложно | Многомерные данные, preprocessing |
| **t-SNE** | ❌ Нелинейный | Локальная структура (соседи) | ❌ Нет | **Визуализация** (2D/3D) |
| **UMAP** | ❌ Нелинейный | Глобальная + локальная | ❌ Нет | Визуализация, быстрее t-SNE |
| **Autoencoders** | ❌ Нелинейный | Зависит от архитектуры | ❌ Нет | Сложные данные (изображения) |

**Рекомендации:**
- **Preprocessing, noise reduction**: PCA
- **Visualization**: t-SNE или UMAP (для 2D/3D)
- **Feature extraction перед ML моделью**: PCA или Autoencoders

---

## Резюме модуля

Unsupervised Learning — **поиск структуры без меток**:
- **K-means**: partition-based кластеризация, inertia, K-means++, связь с EM/GMM
- **Hierarchical**: дендрограмма, linkages (Ward — default), O(n²-n³) сложность
- **PCA**: снижение размерности, SVD, explained variance, масштабирование критично

Эти методы — основа для EDA, preprocessing, и понимания данных!
